
=========================================================================
注意 => 版本更新日志 => E:\GitHub\build\rpm_bin\version.txt
=========================================================================

注意 => 以下是没有完成的目标：
=========================================================================
0.（已放弃）网站端：通过网站端可以查看各个部件的日志信息，通过 http 协议读取日志文件，在浏览器端查看，便于快速排查问题。
   X：（已放弃）实现起来太麻烦，而且当模块分离时，各个组件的读取有会产生不确定因素，造成开发难度，又得不偿失；
   A：（未完成）PHP   => /weike/php/logs/fastdfs.log
                         /weike/php/logs/php-fpm.log
                         /weike/php/logs/slow.log
   B：（未完成）MySQL => /weike/mysql/logs/mysql_error.log
   C：（未完成）Nginx => /weike/nginx/logs/access.log
   			 /weike/nginx/logs/error.log
   D：（未完成）transmit => /weike/transmit/transmit.log
   E：（未完成）tracker  => /fdfs/tracker/logs/trackerd.log
   F：（未完成）storage  => /fdfs/storage/logs/storaged.log
   G：（未完成）srs      => /weike/srs/logs/srs.log
5.（未完成）网站端：直播服务器的状态信息可以通过1985端口的API接口来获取呈现，便于快速定位服务器状态。streams | clients 接口最有用。https://github.com/ossrs/srs/wiki/v2_CN_HTTPApi
6.（未完成）网站端：存储服务器的状态信息可以通过php扩展调用接口获取。
9.（未完成）采集端：在GitHub上的EasyPlayer团队，好像能够支持多种摄像头协议，包括水滴摄像头，采集端需要支持更多的摄像头类型，需要参考；
14.（未完成）摄像头：需要提供一种摄像头直接推流的方案，支持摄像头直接推rtsp或rtmp流，目前的采集端只支持拉流方案；
15.（未完成）网站宣传中，需要突出按需上传的特点，有很好的实际应用场景，现在已经有了2个实际应用场景：
    A：16个校区，每个校区20个摄像头，数据汇聚到云端，总部通过云端观看；不可能将320路全部同时上传到云端，带宽费用高昂，按需上传可以大大节约带宽费用；
    B：20个风力发电厂，每个风厂20个风机，每个风机3个摄像头，数据汇聚到云端，总部通过云端观看；不可能将1200路全部同时上传到云端，按需上传可以大大节约带宽费用；
17.（未完成）网站后端：在点播管理页面，节目预览时，可以直接点击播放；
   A：（未完成）需要对videojs的界面与响应做进一步的深入分析，可以自由控制；
   B：（未完成）需要对videojs在移动端的表现形式做深入分析，可以在手机端自由呈现；
4.（未完成）网站后端：可以设置黑名单或白名单模式，设定哪些直播，或哪些视频可以对外观看；
   A：（未完成）可以将采集端与微信帐号绑定，微信用户登录网站自行设定观看授权方式的进行；
   B：（未完成）每个微信登录用户，可以管理自己的采集端，是否公开采集端或公开采集端下的某些通道；（后台可以设定采集端是否公开，一旦关闭，所有通道不可见）
   C：（未完成）其它未登录用户，可以观看已公开的采集端或已公开的通道；一旦通道公开，通道下录像也会公开；（后台可以设定通道是否公开，一旦关闭，所有录像不可见）
   D：（未完成）小程序有关通道的管理，同样遵循以上规则同时，还可以新增一条规则：对于非公开的通道，所有者可以通过分享的方式，半公开给其它用户，其它用户可以通过分享入口进去观看（后台也可以设定是否开启这个观看权限，也就是说可以随时关闭）
19.（未完成）采集端需要与一个微信账户绑定，普通用户也能登录网站后台，专门管理与自己相关的采集端；管理员就是目前的管理所有配置的页面；
   A：（未完成）普通用户：管理与自己绑定的采集端配置，可以控制通道的添加、删除、修改、启动、停止；
   B：（未完成）管理用户：可以管理所有的采集端，所有的网站配置信息；
   C：（未完成）这样，一下子解决了普通用户的管理权限问题；
   D：（未完成）微信小程序，为了简化，只能管理与自己绑定的采集端，不要添加复杂的网站配置；
   E：（未完成）普通用户，可以管理多个采集端，都是通过微信扫码绑定；
   F：（未完成）一个采集端只能绑定一个微信用户，采集端拥有者可以解除绑定；
   G：（未完成）云录播、云监控属性赋给这个用户，这个用户管理的所有的采集端显示时呈现对应的属性；
20.（未完成）网站后台，只有一种模式，云服务模式，不要把业务教学逻辑加入系统；
   A：（未完成）浩一云，是一个底层的视频云平台，只跟一些基础数据打交道，对外提供API，复杂数据由第三方去呈现；
   B：（未完成）网站端的呈现逻辑暂时不要再去更新迭代了，只是做好已删除通道的处理就行了；
   C：（未完成）第三方集成商的应用是千变万化的，这一点必须明确，只能根据第三方的需求，跟它磨合；
   D：（未完成）基于微信小程序的互联网的监控巡查需求是清晰明确的，可以完全自己进行反复迭代的，
22.（未完成）网站后端：通道删除时，录像记录的删除操作。
   A：（未完成）正确的做法是：做删除标记，用任务去慢慢删除，这样不影响主流程；
   B：（未完成）数据库中设置删除标记，让swoole再去慢慢删除，不影响主进程的使用；
23.（未完成）网站后端：对录像删除的进一步思考：
   A：（未完成）需要将每一个采集端与微信帐号绑定，采集端被认领之后，才能在网站和移动端显示出来；
   B：（未完成）每个微信帐号会被分配一定的授权存储空间，采集端在录像时会判断这个录像空间是否够用，不够用，会自动删除老的录像腾出空间；
   C：（未完成）中心网站不负责分配用户的授权存储空间配置，交给节点网站自己去配置，中心网站只授权采集端的授权使用时间；
   D：（未完成）ihaoyi.cn相当于一个节点网站，它上面可以针对每个微信用户进行存储空间授权，自动回滚删除等等操作；
   E：（未完成）删除通道，不要删除通道下的录像，应该让录像模块自己回滚，这样可以避免用户误删除录像数据，造成损失，也就是说通道没了，录像还在，让新录像数据根据已授权的存储空间，去覆盖老的录像数据；
27.（未完成）采集端可以新增一个功能：可以实时动态转码，以便降低分辨率和码流，达到流畅直播的目的；可以通过外接一个转码模块实现，调用ffmpeg的SDK；
28.（未完成）采集端可以配置成，在下班之后上传，而不是录像完毕之后就上传，避免上传拥堵；
29.（未完成）HLS观看端，汇报周期可以配置，默认12秒汇报一次，最低10秒汇报一次；10秒~60秒
32.（未完成）采集端需要提供32位和64位编译的版本；
33.（未完成）采集端可以配置同时推流的路数限定，避免采集端全部占用上行带宽；
   A：（未完成）采集端有录像功能，而且同时只有一条录像文件上传通道，相当于只占用一条上传通道；

34.（未完成）采集端，需要出一个树莓派的版本，这样可以解决没有PC终端推流的问题，便于快速部署，具有PC采集端80%的功能；

36.（未完成）需要在中心服务器上增加公司业务管理功能：
   A：（未完成）客户管理功能，简易的CRM系统；
   B：（未完成）在客户管理功能基础上，增加合同管理功能，将业务流程电子化；

37.（未完成）将服务器在CentOS7.0以上版本的系统中打包、测试、发布；

36.（未完成）需要新增 网站整体的统计报表功能，采集端数量、通道数量、存储容量、已用存储、剩余存储、用户数量；

39.（未完成）采集端：新增一个接口，可以实现指定通道的立即截图功能；

42.（未完成）SRS、中转服务器、播放器（PC+小程序+移动端），需要引入防盗链机制，避免非法访问占用带宽；
43.（未完成）录像回放需要做防盗链处理；

50.（未完成）为了小程序能够访问内网的直播流，需要在节点与中心之间建立长链接，使用swoole来实现，所有中心与内网的API交互都走这条线；
   A：（未完成）swoole还能完成大量文件的异步删除，还能解决后台异步升级功能；

51.（未完成）https://myhaoyi.com 的首页，新增 小程序相关介绍；

52.（未完成）采集端：解除绑定时，是否需要增加一个环节：让之前已经绑定的微信用户扫码确认一下？改动比较麻烦，后续再改；

53.（未完成）乐直播，帮助别人直播的工具，官网上的直播思路可以值得借鉴，加入到 云监控 或 云录播 当中；
   A：官网地址 => http://www.lezhibo.com/help，专业高清移动直播服务商；
   B：微信小程序 => 乐推流，选的类目就是 在线教育；
   C：在基础设置里面，可以设置和修改活动名称、活动简介、活动公告、封面图、直播背景图、直播顶图以及当前观看人数。
   D：目前乐直播系统提供了5种授权方式。 1. 免费观看 2. 密码观看 C 需要输入特定密码，才能观看直播。 3. 手机认证观看 C 需要输入手机号，然后再填写正确的手机验证码才能进入直播。 4. 手机白名单观看 C 特定手机号，才能收到验证码进入直播。 5. 付费观看 C 需要支付特定的费用，才能观看直播。

46.（未完成）采集端：需要新增一个开关，可以控制用户观看时才拉流，目前是启动后就拉流；
   A：（未完成）通道还是要启动、停止，但是，启动后不拉流，用户观看时才拉流；
   B：（未完成）文件类型启动后就读取数据，rtsp/rtmp流观看时才拉流，摄像头设备观看时才拉流；摄像头设备不预览画面还能节省一路数据流；
   C：（未完成）这个功能改动比较大，影响到的地方太多，需要做大量的测试才能发布；

54.（未完成）华工用户提出一个功能：录像暂停；
   A：（未完成）可以考虑在采集端，增加一个右键菜单或按钮图标，可以对正在录像的通道进行暂停处理；
   B：（未完成）暂停的执行放在LibMP4当中，WriteSample不写盘，而是直接保存为已经写入数据帧；
   C：（未完成）暂时不忙实现，等待用户确实需要时，再添加也不迟；

56.（未完成）华工用户提到的云台功能：
   A：（未完成）需要支持ONVIF探测，这样就能支持IPC不跟采集端在同一个网络内，同样能够进行云台控制；
   B：（未完成）在播放器端也要支持云台功能，只有管理员身份的用户能够操作云台；

57.（未完成）支持ONVIF云台之前，需要先支持大华的IPC：
   A：（未完成）需要购买一台大华的IPC，使用SDK，能够拉流、推流，云台控制；
   B：（未完成）需要明确，目前的组播探测功能是否就是ONVIF功能，需要将组播探测和ONVIF探测统一起来；
   C：（未完成）只要是 rtsp:// 流转发协议，都要进行自动的ONVIF协议探测，看看是否支持这个协议，可以进行很多相当于SDK的操作，比如云台操作；

59.（已完成）采集端：需要增加一个开关，是否开启自动探测IPC功能，自动探测到的IPC自动进行特殊处理；
   A：（未完成）这种设备探测，一旦成功，除了自动增加通道，还会立即自动进行ONVIF探测；
   B：（未完成）可以针对实现已经实现ONVIF协议的IPC，直接进行云台操作，摄像头配置；目前是通过海康的SDK进行的；
   C：（未完成）IPC：如果有多个支持 rtsp 的摄像头，可以通过 upnp 或 onvif 协议，自动获取，然后添加，代替组播自动搜索协议，应用更广泛；

60.（未完成）云录播、云监控的使用手册，不要用word文档或pdf，而是直接用gitbook去完成。
   A：（已完成）gitbook在线帮助 => http://www.chengweiyang.cn/gitbook/index.html

61.（未完成）2014年给哈尔滨阿城教育局安装的微课系统，他们想改进：
   A：（未完成）每个登录用户，需要设定自己所在的学段（小学、初中、高中）
   B：（未完成）用户在查看视频时，需要根据自己所在的学段自动筛选，比如：初中，只显示初中一年级、初中二年级、初中三年级的内容；
   C：（未完成）用户在上传视频时，同样需要根据自己所在的学段，自动筛选，在上传时，自动上传到对应的学段、学科当中。
   D：（未完成）这就要求在服务器端增加转码模块（监视网站端新上传的原始视频，下载到转码模块，开始转码，转码结束，上传到fastdfs当中）
   E：（未完成）前端页面需要新增“上传”模块，点击“上传”，需要先用微信登录，然后进入上传流程。
   F：（未完成）一听说要收费，立马就撤退了。

12.（未完成）云录播前端播放页面，新增按日期选择观看录像节目；
13.（未完成）腾讯推出了一个类似GoogleDoc的多终端同步文档，可以将日志或敏感信息记录在上面，保持多终端同步，顺便整理一下有用的资料，而不是像现在这样到处都是。
14.（未完成）测试发现，H5的flv模式，播放aac音频时，声音会一卡一卡，hls和flash播放正常。

15.（未完成）有关评论的修改意见：
   A：（未完成）后台需要新增铭感词设置；
   B：（未完成）后台需要新增评论开启或禁用开关；
   C：（未完成）后台需要新增flv/flash/hls播放优先顺序的配置；
   D：（已完成）播放页热点推荐的数量从20个减少到10个；
   E：（未完成）后台管理员可以对已有的评论进行管理；
16.（未完成）移动端也需要新增评论、点赞功能，通过公众号授权获取用户信息；

17.（未完成）需要新增互动聊天功能，在PC端和移动端都需要开通这个功能；
   A：（已完成）互动聊天功能，在播放页的右侧，需要实验是否可以关闭，专门放置聊天界面，模仿youtube的简洁模式；
   B：（已完成）实验成功，controlNavigation: 'none', $('#video-gallery').css('width', '850px'); 动态修改视频窗口宽度；

18.（未完成）互动聊天功能开通之后，需要给浩一云申请开通微信支付功能，可以让观看者给直播者打赏；打赏的分账由节点服务器进行标记；
19.（未完成）打赏功能分为移动端打赏（拉取公众号的支付接口）和PC端打赏（弹出支付二维码，用户扫码支付）
20.（未完成）在PC端完成了互动聊天之后，可以考虑改进移动端的页面，参考目睹直播的思路：播放器下面使用tab标签；主播介绍|课程回看|互动聊天（有一个打赏按钮浮动，随时点击打赏支付）

21.（未完成）srs的直播延时在关闭gop时大概在3秒左右，开启gop在7秒左右；而且播放时间越长，延时越大，需要排查 采集端、srs、播放器 的流堆积情况；

27.（未完成）网站后台配置 将录像保留最大值扩大至少730天（可最多保留2年的视频数据）
28.（未完成）网站后台配置 将录像切片由【0，30】扩大至【0，60】；
29.（未完成）中转服务器 doReturnPlayLogin()和doResponse() 新增一个参数(playerCount)用于反馈在线用户数，这样播放端就能获取当前通道在线用户数的情况；
30.（未完成）云教室里的直播间虽然不是按需推流，但是播放端也要向中转服务器汇报观看状态，以便播放器能够获取真正有效的在线观看用户数量；直播服务器本身的计数器不精确；
31.（未完成）微信支付服务商
    A：（未完成）微信支付服务商申请条件：微信认证的企业类型服务号，微信支付服务商目前只面对企业认证的服务号才能开发申请。
    B：（未完成）https://pay.weixin.qq.com/index.php/partner/public/home
    C：（未完成）只需要申请一个微信支付帐号，绑定到小程序就可以，只能通过微信认证的企业服务号，才能申请开通微信支付服务商；
    D：（未完成）在微信支付服务商下面申请的特约商户帐号是无需缴纳300元认证费用的，资金直接流入特约商户帐号，服务商可以看到流水；
    E：（未完成）我们没有可卖商品，我们是技术服务商，一次需要申请微信支付服务商，需要首先申请开通一个服务号“浩一云”，认证，申请微信支付服务商；
32.（未完成）在transmit当中，播放器在线状态通知当中，增加当前通道在线用户数的功能，这样可以实时显示在线观看用户数；

33.（未完成）深圳一个客户需求：必须内网访问，不能对外连接，采集端和节点服务器都需要做一个改造，通过license文件或授权码进行授权；
    A：（未完成）做为首先更新的功能，采集端和节点服务器都支持本地授权，一旦进入本地授权模式，登录需要用户名和密码登录，不链接中心服务器；
    B：（未完成）采集端通过菜单进行授权文件输入，网站端通过后台输入文件，需要在网上找找这种授权方式的方法；
    C：（未完成）两种授权方式可以自由切换，自动匹配；

2018.08.01 - 2018.08.31
===============================================================================================
1.（不处理）讲师端在win8系统下无法启动，报告缺少动态库，添加动态库之后，又报告pthread无法找到入口；
2.（不处理）老师端摄像机使用一段时间之后就会频繁的卡住，调整分辨率之后恢复一会儿有卡住，不知是机器原因还是摄像机原因；
3.（未完成）老师端登录云教室时，需要检测教室里是否已经有老师登录，同一个云教室只能有一个老师在线；需要通过网站端的PHP接口到中转服务器获取老师登录状态？
4.（未完成）老师端登录加载过程有一个加载进度条，避免等等很长时间，突然跳出主窗口界面；
5.（未完成）学生端的摄像头窗口新增右键菜单，新增两个菜单内容：预览画面|开启云台
   A：（未完成）学生端的每个正在运行的摄像头通道都可以通过右键菜单开启画面预览功能（SDL2.0播放）；
   B：（未完成）云台窗口只有一个，是浮动窗口，可以上、下、左、右、还原、光圈、焦距控制摄像头；
   C：（未完成）需要能够自动探测当前摄像头是否支持ONVIF协议，根据摄像头的IP地址进行探测；
   D：（未完成）右侧窗口的码流信息，网络状态信息的显示情况；
   E：（未完成）状态栏窗口的中转服务器|存储服务器|网站服务器的连接状态显示；
   F：（未完成）系统设置 => 配置框内容的完成；
   G：（未完成）关于 => 配置框内容的完成；
   H：（不处理）aboutToShow => 这个菜单按钮自动关联的信号槽，可以解决菜单状态的更新问题；
   I：（已完成）菜单状态更新，最简单的还是用Timer进行状态更新，但有延时；目前用的是触发机制，无延时机制；

6.（未完成）老师端可以对摄像机打开云台操作窗口（VISCA、PELCO-D、PELCO-P）
   A：（未完成）非压缩的摄像机，自动探测云台协议：VISCA、PELCO-D、PELCO-P，通过右键弹出云台控制窗口，选择USB端口，进行摄像机的云台控制；
   B：（未完成）互动教室的摄像头，也可以弹出云台控制窗口，对教室里的IPC摄像头进行远程云台控制，通过中转服务器来完成；
   C：（未完成）状态栏信息的完善：推流情况，网络状态，等等；
   D：（已完成）H.264压缩器，默认配制成veryfast|baseline|zerolatency
   E：（已完成）输出模式，默认设置成“高级”，比特率设置成：1024，（关键帧间隔，设置为2秒；=> 这个还是通过页面配置，强制配置在设置0时有问题）
   F：（不处理）右键新增“预览缩放”=> 缩放至窗口 => 缩放至背景；（经尝试之后，发现还是有问题，暂时放弃处理）
   G：（已完成）双击进行资源位置切换时，让资源尽量按照序号排列的方式显示，需要经过两次切换，避免资源位置来回变动，感觉混乱；
   H：（已完成）将http地址统一起来，避免多处引用造成的混乱；

7.（未完成）如何消除老师端、学生端音频的回声问题：
   A：（已完成）回音的本质，是麦克风与音箱靠的太近，音箱的声音进入麦克风造成的；老师端、学生端都要对麦克风输入的音频数据进行降噪、消除回声处理；
   B：（已完成）老师端 => 录音设备 => 麦克风 => 属性 => 增强 => 开启回音消除 和 开启降噪（比较新的声卡才有，老声卡没有）
   C：（已完成）老师端 => 录音设备 => 麦克风 => 属性 => 侦听 => 侦听此设备（测试时勾选，直播时千万别勾选，会造成回音）；
   D：（已完成）老师端 => 录音设备 => 麦克风 => 属性 => 级别 => 麦克风加强（0.0db，如果选择加强会造成强烈的回音问题）；
   E：（未完成）老师端 => 数据源 => 音频输入捕获 => 滤镜 => 噪声抑制|噪音阈值（创建音频输入捕获时，自动添加）=> 抑制噪音和回声处理；
   F：（未完成）老师端 => 需要开启obs-filters，可以对音频和视频进行特效处理；主要是音频的抑制噪音和回声处理；
   G：（未完成）学生端 => IPC => 配置 => 音视频 => 音频 => 环境噪声过滤 => 开启
   H：（未完成）学生端 => IPC => 配置 => 音视频 => 音频 => 音频输入 => LineIn => 使用外置拾音器 => 降低环境噪声和回音问题
   I：（未完成）老师端 => 互动教室 => 高级音频属性 => 音频监测 => 仅显示器（静音输出）=> 只在老师端回放，不对外输出，避免在学生端产生回音；
   J：（未完成）老师端 => 互动教室 => 滤镜 => 噪声抑制|噪音阈值（创建音频输入捕获时，自动添加）=> 抑制噪音和回声处理；
   K：（未完成）以上在老师端加入的音频滤镜功能，只能解决噪音的问题，并不能解决回音问题，回音问题需要单独的硬件或软件来解决；
   L：（未完成）如果用硬件解决，需要在学生端加入一个音频硬件回声处理器，还需要加入一个音频分频器，老师端也需要处理回声问题，因为是小蜜蜂，相对回声不严重，
   M：（未完成）如果用软件来解决，学生端和老师端什么设备都不用添加，可以大大节省成本；学生端的摄像头就用摄像头本身的Mic进行拾音，外置的拾音器效果差（价格高的或许好些），还要额外增加供电，增加了复杂度；
   N：（未完成）现在，最重要的问题是如何找到一个有效的回音消除开源的公共库，用在讲师端和学生端的音频采集、音频播放当中；
   O：（未完成）学生端 => 接收到的音频解码之后的声音数据与摄像头麦克风接收到的声音数据要进行比较，让麦克风的数据减去音频解码后的数据；
   P：（未完成）学生端 => IPC摄像头的音频是包含了扬声器的音频数据，我们的软件拉取的是压缩的AAC数据，需要先解码成PCM，与接收到的网络解码后的PCM比较处理之后，再用AAC压缩，发送给老师端回放，就能消除回音；
   Q：（未完成）老师端 => 麦克风采集的PCM音频数据，要与网络接收到的互动教室解码后的音频数据进行比较，去掉麦克风音频数据里面包含的互动教室的音频数据，然后再进行音频压缩处理，发送到网络上去；

8.（已完成）回声与混响的进一步研究：
   A：（已完成）https://blog.csdn.net/voice_dsw/article/details/52016846 => 技术文档
   B：（已完成）回声，是指喇叭播放声音，再次进入麦克风；混响是指声波遇到障碍物反弹来回进入人耳的延时声音；
   C：（已完成）回声消除拾音器，可以配合有源音箱，配合网络摄像机，实现远程双向语音对讲。
   D：（已完成）回声消除音频模块，可以接入有源音箱和普通拾音器，实现远程双向语音对讲。
   E：（已完成）回声消除有源音箱，可接入普通拾音器，实现跟远程的对讲。
   F：（已完成）跟淘宝上的商家交流，烽火拾音器，bm-k-5带ANC环境噪音消除的210左右，不带电源和线材；
                还有更高级的hd_32b数字拾音器，带电源和拾音器700左右，电源上可以输出mic，应该可以接电脑声卡，这样就可以在老师端使用，就不用购买领夹麦克风了；
                具体要看实验的结果而定，商家说可以免费试用，不满意可以退货，hd_32b是目前比较高级的数字拾音器了，配的电源除了供电还能提供4种输出音频信号；
                目前在淘宝上能找到hd-32k，大概在1000多一个；
   G：（已完成）http://www.peakfire.cn，直接致电烽火科技厂商，咨询：
       1、老师说话通过领夹麦克风捕获后，经网络传递给教室里的学生端通过音箱播放出来；
       2、教室里的拾音器，会收到音箱播放出来的老师声音，通过IPC采集之后，借助学生端软件经网络到达老师端，从老师端的音箱当中播放出来；
       3、这样就会造成，老师说完话之后，老师端的音箱会听到老师刚才自己的讲话声音，发生一次回声播放，会感到不舒服；
       4、如果老师端的音箱声音，再被老师的领夹麦克风采集，经网络到达学生端，再被播放出来，就会形成反复回声，不停循环下去；
       5、因此，要设法阻断音箱的声音进入拾音器，或者拾音器能够屏蔽掉音箱里的声音；
       6、如果，拾音器能够屏蔽音箱里的声音，再加上拾音器能够输出3.5mm的mic信号，就能用在电脑的声卡上，就能免去使用领夹麦克风的麻烦；
   H：（已完成）http://www.topyeah.cn/，直接致电青岛谱悦电声科技，基本把回声处理这个问题搞清楚了：
       1、需要一个专门的声音处理设备，电脑声卡的声音输出和拾音器采集声音都接入这个声音处理设备，进行声音的判断和消除处理；
       2、然后，这个声音处理设备，会输出老师声音给音箱，输出拾音器声音给网络摄像机的外置音频接入口；
       3、声音处理设备，支持两路拾音器输入，如果要支持多路采集音频输出估计要用到音频分频器，相同的声音输入给不同的摄像头；
       4、目前，他的报价是3500元左右，包括声音处理设备和两个拾音器，不包括音箱和线材；
       5、另外，全向麦克风本身就是做这个处理的，只不过，距离有限，几个人围坐在一起，教室里范围大的情况不太适合，效果达不到；
       6、还有，老师端也可以用全向麦克风，但没有领夹麦克风那种移动性和效果好，因此，老师端用领夹麦克风效果更好；
   I：（已完成）http://www.systemzone.cn/product/?1.html，这家更专业一些：
       1、很专业的回声消除设备，4进4出，8进8出，12进8出，各种型号都有；
       2、我们需要的是3进2出，1路声卡输入，2个拾音器输入；1路音箱输出，1路音频输出（接IPC）；
       3、我们现在遇到的问题是：1路音频输出需要分出多路音频，供给不同的IPC使用；
       4、http://www.xunwei.tm/splitter/ad0116.html，这是一个1进16出的音频分配放大器；报价 => 1进16出1800元；1进8出1100元，接口总类比较多；
       5、http://www.xunwei.tm/，成都迅维，专门做音视频矩阵、解码器；
       6、广州震憾力电子科技有限公司，13128287588，陈先生，报价 => 1进10出，380元；接口上好像有点问题，只有卡农口，需要购买时确认；
   J：（已完成）Speex开源系统，能够处理回音消除功能；但作者实测效果不好；
   K：（已完成）E:\GitHub\HaoYiYun\Document\云教室\回音消除SDK，找了一些SDK，但是很难用上；
   L：（已完成）目前，最简单的还是用硬件模式，用回声处理设备来解决这个问题；

9.（未完成）老师端与学生端的交互问题，主要是针对互动教室的操作：
   A：（已完成）老师端 => 互动教室 => 尽量不要切换到输出画面，而是要全屏投影到第二个显示屏幕当中，这样老师就能监视学生端的情况；
   B：（未完成）老师端 => 互动教室 => 高级音频属性 => 音频监测 => 仅显示器（静音输出）=> 只在老师端回放，不对外输出，避免在学生端产生回音；
   C：（未完成）老师端 => 互动教室 => 滤镜 => 噪声抑制|噪音阈值（创建音频输入捕获时，自动添加）=> 抑制噪音和回声处理；
   D：（未完成）老师端 => 互动教室 => 当互动教室发生全屏投影时，会自动开启一个在最上层的浮动窗口，显示可以进行切换的学生端在线通道列表，并能实时更新；

2018.07.06 - 2018.07.31 开始推流端、观看端分离工作
===============================================================================================
1.（已完成）将观看端更新到Teacher端，替换ffmpeg的文件播放部分，推流端一直推流，观看端临时接入；
2.（已完成）推流端被动按需推流，当观看端触发时才推流，而不是一开始就推流；
3.（未完成）不要用本地录像，用服务器端录像的方案，只录制老师端推流数据，就是全局数据，这样，课程一结束，马上就能看到录像，还不影响推流端网络；
4.（已完成）obs的推流过程分析如下：
   A：video_output_cur_frame() => 未压缩的视频数据帧；
   B：obs-encoder.c:receive_video() => 接收未压缩数据帧，进行压缩；
   C：send_first_video_packet | new_packet | interleave_packets | send_interleaved | rtmp_stream_data | add_video_packet | add_packet | circlebuf(stream->packets => struct encoder_packet) => 投递压缩后的数据包，存放到环形队列当中；
   D：send_thread | send_packet | 
   E：obs-outputs.dll => rtmp_output | flv_output | null_output
5.（已完成）媒体源obs_data内存泄漏分析 => 只要打开配置页面，点击确定就会发生obs_data没有删除的泄漏，通过bmem.c分析查找到原因；
   A：终于找到原因 => 在线通道检测时，读取资源配置结构，obs_source_get_settings()，里面自动进行引用计数增加；
   B：OBSApp::doLoginSuccess()，屏蔽掉在线时钟检测，这是rtmp模式的方式，新模式可能会有变化，等插件打通之后再细化；
   C：经过这么一通折腾，获得后续改进方案，还找到了查找内存泄露的终极办法bmem.c里面的方法，一旦有内存问题，打开 DEBUG_LEAK 宏；
6.（已完成）profile_start | profile_end 的作用分析：
   A：主要作用是记录每个函数的启动顺序，持续时间，逻辑关系；
   B：为了简化去掉了profile的存盘操作，只保留日志信息，逻辑更清晰；
   C：obs-studio\logs => 记录了每一次软件运行时的完整日志信息；
7.（已完成）有关整个Teacher端拉流、推流的思考：
   A：拉流 => 需要用C++实现一个resource插件 => 根据obs的插件规范 => rtp-recv.dll
   B：推流 => 需要用C++实现一个output插件 => 根据obs的插件规范 => rtp-send.dll
   C：通过观看obs-outputs.dll|rtmp-stream.c里面的线程，可以通过信号量的方式来改进收发包机制，而不是用一个固定的时间去等待，这样就能降低sleep误差；
   D：这种机制会造成收包和发包线程分离，调度复杂度加大，需要完全重新改变处理流程，得不偿失，放弃这种方式；
   E：尝试将obs-ffmpeg-source恢复成文件和网络模式，不要用自定义的模式，自定义过程将全部用新的C++插件去实现；
8.（已完成）拉流、推流的C++插件整合到一个插件当中 => win-rtp.dll => win-rtp-source | win-rtp-output
   A：（已完成）进行 win-rtp.dll 工程的搭建工作；
   B：（已完成）开始 win-rtp-source 的搭建工作；
   C：（已完成）拥塞判断，需要在有拉流用户接入之后进行，用户没有接入之前，只是简单的丢包，而不进行拥塞累加判定；
   E：（已完成）开始尝试解码抽取的数据帧，并尝试投递到obs的展示层，关键是时间戳的计算；
   F：（已完成）因为是交给obs上层去完成音视频的同步，只要解码之后，填充obs数据帧，相对比自己用SDL处理要简单很多；
   G：（已完成）需要进一步优化播放过程，适配obs的需要，与单独SDL处理不同，简化了很多操作；
   H：（已完成）由于音视频的处理大部分都是相同的，但是考虑到没必要再次重构已经验证的思路，做进一步优化就可以了；
9.（已完成）开始 win-rtp-output 的搭建工作；
   A：（已完成）Teacher-Pusher => 老师端推流开始搭建 => CUDPSendThread => 具体实现
   C：（已完成）obs-ffmpeg-audio-encoders.c:enc_properties()设置音频的比特率最小值和最大值；最小值调整为32kbps；新增16khz|32khz两个采样率；
   D：（已完成）由于老师推流端会被多个学生端观看，在缓存清理时，不能采用以前的靠探测反馈，只能靠定期清理的方案；
   E：（已完成）老师推流 => 学生观看的特殊性，需要调整数据包处理思路，服务器需要向老师端探测，确认网络质量来决定补包时刻点；
   F：（已完成）服务器端暂时不必向学生端探测，学生端只是向服务器端探测，用来确定补包时刻点；
   G：（已完成）需要先处理老师端推流到服务器的处理过程，完成数据缓存、补包、丢包的处理过程；
   H：（已完成）老师推流端的拥塞丢包处理过程，不是靠服务器的探测指令，目前采用缓存时间量的方式来丢包；
   I：（已完成）学生推流端的拥塞处理是靠丢视频数据帧来解决的，判断拥塞的缓存数据量来判断的；
   J：（已完成）学生推流 => 老师观看的过程是P2P过程，服务器只是做为中转服务器，没有缓存数据包；
   K：（已完成）老师推流 => 学生观看的过程是一对多过程，服务器不仅要中转，还要缓存数据包，进行多点分发过程；
   L：（已完成）服务器端补包，如果要补的包号，比最小包号还要小，直接丢弃，已经过期了；
   M：（已完成）Linux下面的 %lu 可能会是64位的，因此，需要用 %u 代替，否则，可能会出现信息打印错误；
   N：（已完成）服务器端，使用信号量解决补包延时问题，利用信号量的等待超时功能；
10.（已完成）Student-Looker => 学生端观看开始搭建 => CUDPRecvThread => 具体实现
   A：（已完成）然后，再完成学生观看端与服务器端的数据交互过程；
   B：（已完成）学生观看端，一开始就要被创建，每隔500毫秒发送创建命令包，请求老师推流端的数据内容；
   C：（已完成）老师推流 => 学生观看的过程是不需要学生发送准备就绪命令的，因为不需要进行P2P穿透操作，都是通过服务器中转，太多用户时推流端P2P模式根本无法应付；
   D：（已完成）学生观看端，目前只有一个服务器的发包路线，后期可以改进利用多个观看端进行P2P补包，但是效率不一定高，还是通过服务器靠谱；
   E：（已完成）学生观看端根本就不需要处理拥塞问题，每次解析完数据帧之后，就从环形队列当中删除了；
   F：（已完成）找到一个推流端补包的潜在问题 => 补包发送后没有设置m_bNeedSleep=false标志，可能会导致补包延时；
   G：（已完成）学生观看端需要在探测命令中收到服务器反馈的服务器目前最小的有效数据包号，这个包号之前的数据都无效了，需要清理观看端的补包队列，避免一直等待，造成延时（服务器会缓存3秒数据）
   H：（已完成）解决了学生观看端在服务器的丢包补包机制，解决了老师推流端在服务器的丢包补包机制，都使用信号量的方式解决；
   I：（已完成）老师推流端在服务器端发生的丢包，老师推流端补包后，服务器会原样转发给所有的学生观看端，必须转发，服务器丢包必然造成观看端丢包；
   J：（已完成）学生观看端发送的补包命令到达服务器端后，服务器会挨个查看丢包序号，如果这个丢包号正在服务器上要补的包号，不要加入补包队列当中；
   K：（已完成）服务器端在发送学生端的丢包时，也进行了丢包类型的判断，如果本身就是 PT_TAG_LOSE ，也不能补包，上一条就是避免出现 PT_TAG_LOSE的丢包；
   L：（已完成）观看端、推流端，创建命令发包周期都使用100毫秒（改大了对延时有影响？），只有，学生观看端不断尝试连接的周期改为500毫秒；
   M：（已完成）学生观看端，会发生严重丢包，造成无法持续观看的问题；是由于服务器中断造成的？
   N：（已完成）老师推流端，音视频缓存量，由原来的缓存3秒增加到缓存5秒，服务器端缓存的老师推流端数据量；
   O：（已完成）老师推流端，不要缓存已发送数据包，由服务器探测来进行缓冲区的删除操作，老师推流端的缓存不用来观看端补包，只用来服务器端补包；
   P：（已完成）学生推流端 => 老师观看端 => 延时0.4秒到0.5秒，老师推流端 => 学生观看端 => 延时0.4秒到0.5秒，学生推流端往返延时在1秒左右；网络状况良好情况下；
11.（已完成）开始用vs2015搭建全新的学生端界面，使用QT，完成登录，界面控制，云台控制界面等等；
   A：（已完成）完成了登录界面的搭建工作，能够登录验证房间号码的有效性；
   B：（已完成）学生端配置目录统一修改为obs-student，老师端配置目录统一修改为obs-teacher，这样便于区分和查找；
   C：（已完成）开始进行主界面的绘制工作，分左右两侧，左侧为本地摄像头列表，右侧为老师端拉流图像；
   D：（已完成）https://blog.csdn.net/LG1259156776/article/details/52469244 => 有关QSplitter的一些实用技巧；
   E：（已完成）窗口系统不要使用OBSDisplay的显示方式，它是用直接的D3D和OpenGL的方式；为了简化，第一期先用SDL2.0简化实现；
   F：（已完成）https://www.cnblogs.com/wiessharling/p/3750461.html => 有关子窗口全屏的文章
   G：（已完成）绘制老师窗口的标题栏和显示边框，参考以前的采集端窗口绘制；https://blog.csdn.net/liang19890820/article/details/51227894
   H：（已放弃）解决QString显示中文乱码问题 => #pragma execution_character_set("utf-8")，这种方式问题多；还是用QStringLiteral()思路更清晰；
   I：（已完成）绘制学生端通道窗口，通道标题，思路与老师窗口一致，背景色、字体、高度，都一致；
   J：（已完成）有关QString中文的相互转换：
       QStringLiteral("讲师端画面") => 静态中文字符串转换成本地格式，不一定是UTF8
       QString::fromLocal8Bit("讲师端画面") => 静态中文字符串转换成本地格式，不一定是UTF8
       QString::fromLocal8Bit(strLocal) => 字符串变量转换成本地格式，不一定是UTF8
   K：（已完成）学生端和讲师端的主进程都是Unicode字符集（WCHAR），UTF8和ANSI都是CHAR
      （已完成）与服务器通讯都用UTF8格式，我们自己的API都是转换成UTF8格式，因为使用string(char)；
      （已完成）在显示和主App当中存放，都用QString（WCHAR）格式，这样就能解决乱码问题；
      （已完成）只要不显示，都用UTF8保存与服务器通讯的字符串数据，只有显示时才进行转换；
      （已完成）qobject_cast => 强制转换更有效的方法，不要只是简单的用()进行转换；
   L：（已完成）开始进行学生端的登录改造过程：
      （已完成）学生端与讲师端都有一个初始的登录服务器房间号的过程，这个过程不做限制，后续再进行调整；
      （已完成）学生端登录房间号之后，再进行节点服务器、中心服务器的验证确认，与之前的采集端保持一致，进行判断；
      （已完成）学生端的libcurl要用到https，obs提供的libcurl不支持https，将student与teacher的编译结果分离，student使用自己的一套libcurl支持https；
   M：（已完成）完成了通道的添加、修改、删除、开始、停止等操作；
   N：（已完成）进行“断开重连”的具体操作实现，方便快速重连操作；
   O：（已完成）开始尝试将中转服务器合并到udp服务器当中，端口号确定为21002，学生端、老师端都链接这个端口，进行命令交互和数据交互；
      （未完成）学生端的动态截图也通过这个交互通道完成，学生端和讲师端都有一个登录过程，都通过房间号来进行相互关联；PHP网站端只是完成网站与服务器的数据内容交互；
      （已完成）需要改造edu.ihaoyi.cn的房间登录验证过程，网站端这时不用链接中转服务器；
      （已完成）修改讲师端登录流程，使用新的中转服务器的处理流程；同时，优化了讲师端变量和文字；简化讲师端与网站端的交互流程，获取中转服务器地址和端口；
      （已完成）只限制学生端使用时间，讲师端不限制使用时间，但都需要登录服务器，验证房间号，验证房间密码；
      （已完成）学生端和讲师端都需要从网站获取到系统配置的udp服务器的地址和端口，讲师端通过网站接口获取，学生端通过注册时获取；
      （已完成）需要在网站端的系统配置表wk_system当中，新增两个字段udp_addr和udp_port，用来记录udp服务器的地址和端口；
       QString::fromUtf8 => 将UTF8转换成Unicode的QString
       QString::toUtf8   => 将Unicode的QString转存成UTF8格式
   P：（已完成）学生端开始加入连接udp服务器，启动后就不断尝试接入；
      （已完成）学生端由于使用的SDL，必须用一个完整窗口进行显示，需要在CViewTeacher当中增加一个专门用来显示的窗口CViewRender
      （已完成）QT当中，目前使用的全屏方案有点问题：如果全屏窗口有子窗口，好像不能全屏，只有当窗口是单独子窗口时才能全屏；
      （已完成）SDL的QT窗口不能用QT的接口show进行显示，必须使用API接口ShowWindow()，可能是SDL通过API进行隐藏的原因；
      （已完成）SDL2在使用D3D进行画面绘制时，是锁定窗口状态，这时如果进行全屏操作，就会改变窗口属性，造成冲突崩溃；用变量标识：全屏过程中，不能重建SDL句柄；
      （已完成）当渲染窗口的矩形大小发生变化时，没有采用之前的直接重建的方式，而是在渲染窗口设置标志，让播放层自己检测标志进行重建，这样简化了操作，重建的次数也会减少；
      （已完成）优化渲染窗口的重绘问题，设置播放状态，在播放状态下，不要重绘背景，通过获取播放层状态进行必要的文字信息显示；
      （已完成）https://blog.csdn.net/u013255206/article/details/70312818 => 解决边框闪烁的问题；
      （未完成）还有一个闪烁问题 => 拖动Splitter造成 CViewRight|CViewTeacher|CViewRender 发生变化时的闪烁问题，可以参考obs里面的处理过程；
      （已完成）学生端、讲师端只要登录房间成功之后，就立即连接中转服务器，等待交互命令，而不是等待5秒之后；都有30秒汇报机制，避免服务器端超时删除；
      （已完成）学生端观看线程是否启动，要看讲师端是否在线，学生端可以通过中转服务器获得所在房间内是否有讲师登录，讲师端登录也会登录状态转发给在线的学生端，学生端再启动观看线程。
      （已完成）学生端登录中转服务器之后，服务器会反馈房间里是否已经有讲师端存在，学生端就会根据反馈启动接收线程CUDPRecvThread，进行UDP数据拉取；
      （已完成）讲师端登录中转服务器之后，会遍历所在房间里有效的在线学生端，向他们转发讲师端上线通知，学生端会启动接收线程CUDPRecvThread，进行UDP数据拉取；
      （已完成）讲师端退出或被中转服务器删除之后，会遍历所有在房间里有效的在线学生端，向他们转发讲师端下线通知，有两种状态：TCP连接对象|UDP连接对象；
      （未完成）尝试降低udp数据包的大小来测试目前上传对网络影响，目前是800字节，可以在100~1400字节之间尝试；
   Q：（已完成）需要去掉服务器端的Reload的命令，作用不大，但是会造成服务器和连接者都会发生混乱；
      （已完成）重建的发生是由于UDP讲师端或UDP学生端被服务器删除之后，终端并不知道造成的，现在有了TCP长连接，可以进行这个删除通知操作；
      （已完成）讲师端TCP连接和学生端TCP连接都必须先于UDP启动，每个UDP连接在创建命令当中都要带上与TCP的关联套接字编号，进行相互关联；
      （已完成）这样，当UDP连接在退出时，就能依靠TCP连接进行事件通知，达到精确控制的目的，UDP连接在服务器和终端当中都只保留套接字编号，每次使用时通过编号去查找对象，避免保存指针造成的问题；
      （已完成）rtp_create_t 结构中新增一个字段 => tcpSock，用来跟TCP连接进行关联；彻底删除与Reload相关的结构和代码；
      （已完成）学生端TCP连接和讲师端TCP连接登录成功之后，服务器都要返回TCP套接字给终端，以便建立UDP时在rtp_create_t当中使用；
      （已完成）讲师端在启动时rtp_source注定将无法启动，因为一定会参数不足，这个需要特别注意；（更改了服务器端的处理流程：触发学生端推流）
      （已完成）obs-teacher里面有两个输出类AdvancedOutput|SimpleOutput，需要在两个地方都需要进行输出配置；
      （已完成）讲师端的rtp_output插件当中，create|update, start|stop|destroy 过程都是有差异的，stop时只删除了推流线程，配置对象并没有删除，destroy是才删除配置对象；
      （已完成）讲师端的rtp_output插件当中，AdvancedOutput::StartStreaming|SimpleOutput::StartStreaming，每次启动都重新设置了配置信息；
      （已完成）讲师端的rtp_output和rtp_source插件当中，由于udp_addr和udp_port都是从网络配置当中获取，不是硬编码了，可以去掉 DEF_UDP_HOME|DEF_UDP_PORT 的定义了；
      （已完成）讲师端、学生端在收到 kCmd_UDP_Logout 命令时，需要注意重复删除的问题，同时，需要通过强烈的网络阻塞才能进行超时删除测试验证；
      （已完成）讲师端 on_streamButton_clicked 可以进行推流的开启或关闭；
      （不处理）学生推流端，在网络先拥塞只发音频，当网络恢复正常之后，需要再发送视频，不要永远都不发视频了；（采用了新的模式：严重拥塞，学生端主动中断推流）
   R：（已完成）讲师端通过创建rtp_source只拉取一路房间里的学生端摄像头，通过中转服务器告诉学生端可以推流了；
      （已完成）讲师端 => 服务器 => 学生端 的交互流程改进，通过中转服务器而不是通过curl网站服务器；
      （已完成）学生端 通道的启动和停止，不但要向网站服务器汇报，还要向中转服务器汇报，讲师端只从中转服务器获取在线通道的列表信息；
      （已完成）学生端、讲师端 登录命令中，需要加入学生端的名称字段UTF8格式，便于讲师端查看使用；
      （已完成）RemoteSession当中使用临时动态缓存，发送命令数据包，如果用固定的缓存可能在多线程发送时造成命令被冲掉的可能性；
      （已完成）讲师端 通过中转服务器获取所在房间里的所有在线的摄像头通道列表，并通过界面显示出来，供用户进行拉流通道选择；
      （已完成）学生端、讲师端、中转服务器，发送数据的代码做了统一优化，调用统一的通用接口，而不是每次都重复代码 => doSendCommonCmd
      （已完成）中转服务器端每一个摄像头通道的Create(启动)和Delete(关闭)，都是讲师端触发造成的，因此，都需要在中转服务器转发给房间里的老师端对象；
      （已完成）老师端成功发起Camera_LiveStart命令之后，不要启动rtp_source拉流线程，中转服务器会触发学生端开始通过UDP推流；学生端推流Create触发中转服务器转发给讲师端，学生端推流Delete触发中转服务器转发给讲师端；这样形成一个完整的命令闭环；
      （已完成）老师端登录成功之后，也跟学生端一样，会带着rtp_source里的摄像头通道编号，获取是否能够创建rtp_source的反馈，就能避免讲师端每次重启不能播放的问题；
      （已完成）学生端和老师端在中转服务器当中的特性现在正在趋同，过程都逐渐向相同特质靠拢，这样维护起来就会好很多，稳定性也会加强；
      （已完成）老师端发起的Camera_LiveStart命令，不仅需要发送摄像头通道编号，还要使用场景资源编号，这样才能在讲师端精确定位资源；
      （已完成）老师端动态改变拉取的摄像头通道的处理，需要进一步的完善，便于用户任意调整：反复选择相同的摄像头通道；两个通道来回切换；
      （已完成）老师端需要在rtp_source当中根据摄像头通道的在线标志进行特殊处理，学生端需要先停止正在推流的所有通道（不包括与新通道编号一致的通道），再启动新通道的推流线程；
      （已完成）老师端需要在界面层限制：只能有一个rtp_source数据源的存在，如果有多个，目前在逻辑上和服务器上会发生混乱；（一个可以克隆多个，目前是支持的，虽然支持，还是屏蔽了）
      （已完成）老师端需要在启动时，rtp_source资源的create过程，不要尝试创建拉流线程，在得到rtp_source资源的 场景资源编号 和 摄像头通道编号之后，通过登录命令发送给服务器；
      （已完成）老师端OBSBasic::OBSInit()虽然先执行，但是OBSBasic::DeferredLoad()信号槽是异步的，导致后执行的OBSApp::doCheckRemote()会先于资源装入前加载，CRemoteSession的创建必须等待OBSBasic::DeferredLoad()加载完毕之后才能进行；
      （已完成）老师端OBSBasic::DeferredLoad()加载完毕之后，立即调用App()->doCheckRemote()，启动远程连接，防止rtp_source的启动延时；
      （已完成）中转服务器在当UDP连接中断时发送的kCmd_UDP_Logout当中，需要带上nDBCameraID，便于学生端进行删除操作；
      （已完成）老师端在登录是，发送的kCmd_Teacher_Login登录命令，服务器端还是要立即发送反馈命令，目的是得到关联的TCPSockFD；
      （已完成）老师端在登录时，发送的kCmd_Teacher_Login登录命令，在服务器端会根据SceneItemID和DBCameraID触发学生端的kCmd_Camera_LiveStart命令，然后再有学生端通过中转服务器回应kCmd_Teacher_Login反馈给老师端；
      （已完成）老师端停止拉流或老师端退出时，需要通知学生端推流者，停止推流，或者，当学生推流者缓存的数据超过4秒，就直接停止推流，而不是现在只发音频的处理模式，长时间没有老师观看端回应，就需要自动被删除；
      （已完成）学生端在推流时，如果发现缓存超过4秒，就发起停止推流指令，迫使服务器端通知老师端中断拉流线程；这样就能简化学生端对网络拥塞的处理过程；
      （已完成）老师端相对应的，会用一个时钟检测rtp_source资源的健康情况，发现中断，可以主动发起kCmd_Camera_LiveStart命令，触发学生端再次推流，接着触发老师端的rtp_source主动连接服务器再次拉流；
      （已完成）学生端先于中转服务器启动，通道启动后，在中转服务器中没有形成CTCPCamera对象；讲师端无法获取在线通道列表；
      （已完成）学生端远程连接被超时删除之后，已经启动的通道没有重新更新到中转服务器，在中转服务器中没有形成CTCPCamera对象，讲师端无法获取在线通道列表；
      （不处理）针对以上两个问题：学生端在启动时，遍历通道状态，把在线通道编号附加到登录命令当中，在服务器端进行CTCPCamera对象的更新处理；
      （不处理）针对以上两个问题：学生端在启动后，发送kCmd_Student_OnLine在线汇报命令时，需要附带上当前在线的通道编号，在服务器端进行CTCPCamera对象的更新处理；
      （已完成）以上的思路还是有问题：太过复杂，需要简化；学生端远程对象登录成功之后，会遍历所有摄像头通道，重新发送kCmd_Camera_PullStart命令，避免服务器端没有摄像头通道对象的问题；
      （已完成）学生端正在推流的在线状态信息显示，正在推流的图标信息显示；
      （已完成）通道编辑对话框的QLineText在进行粘贴复制时，报告剪贴板不可用的错误；CoInitializeEx()初始化的位置不要放在main()，应该放在doLoginSuccess()登录成功之后；
      （已完成）SDL2.0的初始化不能放在CoInitializeEx()之后，而是应该放在CPlayThread()当中，也不用重复调用COM的初始化；
      （不处理）发现：IPC动态改变码率之后，不用断开连接，就能动态根据变化的码流拉取数据，跟之前的测试有点不同，这个需要后期进一步确认；
   S：（已完成）讲师端的数据源窗口布局进行调整，按照固定的第一行一个大窗口，第二行多个小窗口排列；
      （已完成）只有一个窗口时，默认按比例填充屏幕；有第二个窗口时，自动排列两行：第一行一个大窗口，第二行多个小窗口；
      （已完成）双击第二行的小窗口，自动跟第一行的大窗口进行显示位置切换，需要记录第一个大窗口的窗口对象指针或代号，便于查找定位；
      （已完成）双击进行窗口位置交换时，无需记录位置，而是根据scene_item的特征进行计算判断，避免记录带来的还原问题，pos坐标为(0,0)的就是第一行大窗口；
      （不处理）讲师端在推流时，只编码压缩第一个大窗口的图像，不要对整个整个显示画布进行编码；在压缩前就进行了数据的变换处理，学生端就可以不做处理了；
      （不处理）讲师端在进行YUV原始数据输出时，可以预先进行图像的缩放工作，整个画布1680*1050，整体输出1120*700，我们实际需要的是1680*840，因此，变换区间是1680*840 => 1120*700；
      （不处理）video-io.c:video_thread() => 处理已经变换后的YUV数据，再次处理后放入压缩器；
      （不处理）video-io:obs_graphics_thread() => output_frame() => output_video_data() => video_output_lock_frame() => 产生YUV数据的过程；
      （已完成）尝试在讲师端对原始画面进行裁剪压缩的工作失败了；尝试在学生端在回放时对解码后的数据进行裁剪显示，成功了，具体参见 => CVideoThread::doDisplaySDL()
      （已完成）https://obsproject.com/docs/ => 开发文档 => 作用并不大；
      （已完成）obs_load_sources => scene_load => scene_load_item => obs_scene_add => 创建 obs_sceneitem_t 对象的地方；
      （已完成）obs_scene->id_counter是在obs_scene_item读取完毕之后从配置当中更改的，obs_scene_item->id是在obs_scene_add之后从配置当中更改的；
      （已完成）所有的obs_scene_item都是以链表的形式，存放在obs_scene当中，id序号是根据链表递增排列，id会不断累加；永不重复；
      （已完成）obs_sceneitem_remove => obs_sceneitem_release => obs_sceneitem_destroy
      （已完成）OBSBasic::DeferredLoad() => Load() => 成功之后，需要调用场景资源位置重排接口OBSBasic::doSceneItemLayout()；两行（1行1列，1行5列）
      （已完成）window-basic-source-select.cpp:AddSource() => 成功之后，需要调用场景资源位置重排接口OBSBasic::doSceneItemLayout()；两行（1行1列，1行5列）
      （不处理）直接对预览窗口进行全屏，而不是新建一个投影窗口进行全屏预览，增加一个菜单和快捷键盘，比如：F5和F键；（尝试之后，效果不好，暂时不处理）
      （已完成）行与行之间增减显示间距，列与列之间增加显示间距，不要让显示窗口看起来那么拥挤；
      （已完成）进行scene_item的删除操作时不进行显示位置的窗口重排；
      （已完成）在对场景资源进行重排显示时，只输出第一行大窗口资源的音频+其它没有界面的全局音频资源；屏蔽其它资源的音频输出；
      （已完成）默认都采用“关闭监视”的方式：讲师端不回放声音，只保留最大窗口的音频输出，其它带视频窗口的音频都屏蔽掉，不输出；
   T：（已完成）启迪未来远程控制帐号和密码；
      （已完成）1025266224:admin123 => i7主机的TeamViewer的登录帐号和密码 => 可以通过设备管理器 网络适配器 => 属性 > 高级 > Network Address => 更改网络地址，来改变ID号
      （已完成）1030711791:admin123 => 赛扬主机TeamViewer的登录帐号和密码 => 可以通过设备管理器 网络适配器 => 属性 > 高级 > Network Address => 更改网络地址，来改变ID号
      （已完成）修改 TeamViewer ID 的方法 => https://blog.csdn.net/dongqing27/article/details/80646510
      （已完成）Teacher与Student，全新整体打包输出，安装、测试、发布；
      （已完成）Teacher、Student，32位Release编译、打包、输出；
      （已完成）讲师端、学生端在新系统中主窗口图标没有显示出来；this->setWindowIcon()使用png图片可以解决这个问题，某些机器不认ico；
      （已完成）SDL_OpenAudio()启动报错，是由于一个进程只能打开一个SDL2.0音频，因此，讲师端、学生端都需要禁止启动多个进程，使用互斥的方式，参考“讲师端”的实现；讲师端已经有专门的处理重复加载的代码；
      （已完成）SDL_OpenAudio()失败的问题，是由于某些硬件设备必须插入耳机才会启用，因此，代码需要修正，避免没有音频对象时，还在一直灌音频数据；
      （已完成）某些主机的声卡显示耳机未插入，可以设置：禁用前面板检测来解决，这样就不用非要插入耳机才能使用扬声器，SDL_OpenAudio()加载就能成功；

12.（已完成）发现obs在显示层有1秒左右缓存延时，这个要想办法去掉 => obs_source_set_async_unbuffered
   A：（已完成）win-rtp已经把解码好的数据丢给了obs显示层，网络层没有任何的缓存数据包，缓存应该累积在了obs的显示层；
   B：（已完成）win-rtp当中针对数据帧的处理有点问题，跟obs的机制有关，obs是会严格安装音视频同步，win-rtp在寻找关键帧的过程中，没有丢掉音频，造成同步延时；
   C：（已完成）win-rtp改进方案 => 始终用视频做为同步机制，使用视频的第一个关键帧的时间戳做为播放0点时钟，第一个关键帧之前的音频都扔掉，这样就不会造成obs的同步延时；
   D：（已完成）win-dshow数据采集层和obs显示层，看看是否有缓存延时，需要尽量少的缓存，否则有延时，目前obs采集到学生端播放，互联网延时在0.5秒左右；
   E：（已完成）win-dshow里面设置了obs_source_set_async_unbuffered，播放层补缓存，直接播放；win-rtp当中也设置补缓存，但是有点卡顿；
   F：（已完成）发现目前移动的家庭网络，只要有推有拉，网络就不会有大的波动，只会偶尔波动；
   G：（已完成）需要进一步优化win-rtp当中rtp-source的延迟问题，现在确定就是播放层的问题，如何达到低延时不卡顿是个问题；
   H：（未完成）win-rtp的rtp-output也有延时，obs在压缩层是否也有缓存问题，需要进一步的分析；

13.（已完成）还需要新增个win-rtp-service用来配置链接服务器的地址信息，目前是硬编码写死，obs有一套service机制来解决；
  A：（已完成）目前采用的是在服务器网站端进行udp地址和端口配置，来解决地址问题，暂时不用写obs的service机制；

14.（已完成）云教室 需要将中转服务器与UDP转发服务器整合到一起去，修改中转端口；这种方式可以简化很多复杂的交互操作；
   A：（已完成）观看端尝试连接通道，需要向中转服务器汇报，否则，一直尝试连接失败；
   B：（已完成）观看端主动退出之后，推流端没有收到通知，继续推流，需要改进；

15.（已完成）电子白板 => 就是把投影仪和幕布融为一体，HDMI或VGA连接电子白板(投影仪)和电脑，USB连接电脑和电子白板，完成交互识别；电子白板已经产品化，价格便宜；
16.（已完成）网站参考页面 => https://www.yuanfudao.com/
17.（已完成）AWS据说带宽流量免费？直播服务器自己搭建最好用aws，抖音都是用的aws => https://aws.amazon.com/cn/

2018.06.26 - 低延时学生端推流，服务器转发，讲师端接收，讲师端音视频同步播放，开发总结：
===============================================================================================
1.（已完成）交互命令与数据结构定义：
   A：每一个命令的第一个字节都由三部分组成：TM(Terminate Type，7-8位)，ID(Identify Type，5-6位)，PT(Payload Type，1-4位)，详见rtp.h命令结构体定义；
   B：TM(终端类型)，目前有 3种：0x01（学生端），0x02（讲师端），0x03（服务器）
   C：ID(终端身份)，目前有 3种：0x01（推流者），0x02（观看者），0x03（服务器）
   D：形成四种交互终端：学生推流者、学生观看者、讲师推流者、讲师观看者；
   E：PT(载荷命令)，目前有10种：0x01（探测），0x02（创建），0x03（删除），0x04（补包），0x05（序列头），0x06（准备就绪），0x07（重建），0x08（音频包），0x09（视频包），0x0A（丢失包）
   F：UDP数据载荷固定为800字节，UDP命令结构体按4字节整数倍定义。音视频数据包结构体rtp_hdr_t长度为12字节；
   G：推流端和观看端都使用环形队列(circlebuf.h)来管理音视频数据包，为了便于快速定位，环形队列每一个数据包长度800+12，音视频载荷不足800字节的，用0填充；
2.（已完成）UDP服务器架构与房间定义：
   A：UDP服务器监听端口5252，使用UDP阻塞套接字模型，收到网络命令数据之后，解析第一个字节，获得TM(终端类型)，ID(身份类型)，PT(载荷命令)，进行分发处理；
   B：所有向5252发送数据的终端，都会自动创建一个CNetwork对象，通过nHostPort来定位CNetwork，最终只有两种终端CStudent和CTeacher，都从CNetwork继承；
   C：在edu.ihaoyi.cn会预先建立虚拟房间，产生房间号，学生端软件和讲师端软件需要预先通过房间号登录虚拟房间；
   D：学生端软件和讲师端软件通过Create命令告诉UDP服务器已登录的房间信息，并在UDP服务器创建CRoom对象；
   E：CRoom对象包含一个推流CStudent（会变化），一个推流CTeacher（不变），一个观看Teacher（不变），多个观看CStudent（变化）；
3.（已完成）学生推流端基本流程：
   A：初始化视频格式头 H264（SPS+PPS+Width+Height+FPS），如果推流线程未启动，启动之；
   B：初始化音频格式头  AAC（采样率索引号+声道数），如果推流线程未启动，启动之；
   C：初始化线程当中设置初始的命令为发送创建命令，创建UDP异步套接字；
   D：如果状态是发送创建命令，每隔100毫秒向服务器的5252端口发送创建命令包，当收到服务器确认收到创建命令之后，设置状态为发送序列头命令；
   E：如果状态是发送序列头命令，每隔100毫秒向服务器的5252端口发送序列头命令包，当收到服务器确认收到序列头命令之后，设置状态为等待状态（等待观看端发送准备就绪命令）
   F：推流线程每隔1秒钟，向服务器的5252端口发送网络探测命令包，服务器转发给房间里的讲师观看者，讲师观看者收到之后，再返回给服务器，服务器再返回给学生推流端；
   G：讲师观看端登录服务器之后，服务器会转发学生推流端的播放序列头给讲师观看端，讲师观看端收到播放序列头之后，每隔100毫秒发送准备就绪命令包；
   H：学生推流端收到服务器转发的来自讲师观看端发送的准备就绪命令之后，设置状态为发送音视频数据包命令，并回复讲师观看者一个准备就绪包，告诉它不要再发送准备就绪包了；
   I：音视频数据线程，发现推流线程是发送数据包状态后，开始将获取到音视频数据帧投递到推流线程，并安装800字节进行封装，依次放入推流环形队列，形成打包序号；
   J：推流线程会检测环形队列当中有没有打包好的音视频数据包，有包立即发送出去，按照序号发送，形成发送序号；
   K：推流线程会收到讲师观看端发送的补包命令，将需要补包的序号放入一个map队列；
   L：推流线程会检测丢包队列，取出一个丢包号，找到丢包数据立即发送出去；
   M：推流线程收到讲师观看端发送的网络探测命令，会附带一个观看端已经成功接收到的连续最大播放包序号，用这个序号删除发送环形队列当中的过期数据包；
   N：推流线程只要有任何的发送数据或接收数据，就不能休息，既没有发送也没有接收时，才进行sleep休息，最大休息15毫秒，如果休息太短，CPU会上升，后期可以考虑用QT异步优化；
   O：推流线程退出时，会主动发送删除命令，删除服务器端的学生端推流对象，服务器端也会定期（每隔10秒）检测并删除长时间（15秒）没有数据交互的终端；
4.（已完成）讲师接收端基本流程：
   A：初始化接收线程，创建UDP异步套接字，设置初始的发送创建命令状态；
   B：如果状态是发送创建命令，每隔100毫秒向服务器发送创建命令包，服务器收到之后，会回复服务器收到的来自学生推流端上传的播放序列头，接收线程收到播放序列头之后，设置为准备就绪状态；
   C：如果状态是准备就绪命令，每隔100毫秒向服务器发送准备就绪命令包，服务器收到之后，会回复服务器收到的来自学生推流端的映射端口和地址，接收线程收到服务器回复之后，设置为接收音视频数据包状态；
   D：接收线程每隔1秒钟，向服务器的5252端口发送网络探测命令包，服务器转发给房间里的学生推流者，学生推流者收到之后，再返回给服务器，服务器再返回给讲师观看端；
   E：接收线程收到音视频数据包之后，会按照序号放入接收环形队列，丢失包做标记，数据区用0填充，同时，将丢失包放入补包队列；
   F：接收线程会检测补包队列，将需要补包的序号组合在一起发送给学生推流端；
   G：接收线程收到学生推流端发送的补包之后，才将补包队列里的丢包号删除掉；
   H：接收线程会从环形队列抽取完整一帧数据，放入播放器；
   I：接收线程只要有任何的发送数据或接收数据，就不能休息，没有任何操作时，踩进行sleep休息，最大休息15毫秒；
   J：接收线程退出时，会主动发送删除命令，删除服务器端的讲师观看者对象，服务器端也会定期（每隔10秒）检测并删除长时间（15秒）没有数据交互的终端；
5.（已完成）遇到的问题与解决方法汇总：
   A：服务器端正常运转的前提：学生端和讲师端连接服务器的UDP映射端口保持不变，如果是那种每次发送UDP数据包都会造成端口变化的复杂网络就无法进行数据的中转操作；
   B：目前采用的异步socket方式，sleep时间会造成探测包、收发数据包 延时误差，如果设置过小的sleep值，会造成CPU上升，目前设置为15毫秒；后期可以考虑用QT异步优化；
   C：H.264的视频帧前4字节要改成00 00 00 01，第一帧必须带上SPS和PPS，直接使用avcodec_decode_video2解码，got_picture为0时，设置has_b_frames = 0，非常重要，避免错误帧缓存造成延时；
   D：目前从海康IPC获取到的H.264数据帧只有I帧和P帧，没有B帧，这样延时相对较小；
   E：avcodec_decode_video2得到的AVFrame一定要用av_frame_clone拷贝走，H264的解码帧是相互关联的，解码器会保留AVFrame地址，后续帧会一直影响前面帧的数据，造成画面抖动；
   F：解决了AVFrame解码后数据相互影响，以及解码失败的延时问题，就能根据时间戳任意控制音视频的播放，采用系统0点计时方式，音视频播放帧的时间戳只需要跟系统时间比对，无需相互比对，就能自动同步，同步是由音视频时间戳决定的。
   G：环形队列的使用非常重要，千万不能用指针操作，只能通过接口方式操作，因为是环形队列循环使用，会有start_pos > end_pos的情况，这是用指针就会发生错误；每次操作都用一个临时缓冲去拷贝数据，这样就最安全。
   H：环形队列在抽帧时，别全部抽取之后，新到达的序号包要进行丢包处理，有可能691先到达，690后到达，就需要给690预留位置，设置丢包标记；
   I：https://www.cnblogs.com/lidabo/p/6857616.html => 非常重要的低延时理论实践文章；
   J：http://bbs.chinaunix.net/thread-4096315-3-1.html => 不要加锁. 不要弄复杂, 程序先写出来, 你就会发现UDP处理高并发实在太简单了.
   K：https://blog.csdn.net/wuxinyanzi/article/details/52300960 => MTU探测思路 => 设置socket为不分片，发送一系列不同大小的包到server，超过MTU的包自然会被丢弃。这样可以得到路径MTU值。
   L：https://www.cnblogs.com/fsw-blog/p/4788036.html => tc 详细用法 (ln -s /usr/lib64/tc /usr/lib/tc)
      https://blog.csdn.net/duanbeibei/article/details/41250029/ => tc 详细用法
      tc qdisc add dev eth0 root netem delay 200ms 70ms 30% loss 10% corrupt 0.1% => 延时200ms，抖动70ms，丢包10%，0.1包损坏；
      tc qdisc del dev eth0 root netem delay 100ms 30ms 30% loss 1% corrupt 0.1%
      tc qdisc replace dev eth0 root netem delay 200ms 70ms 30% loss 10% corrupt 0.1%

6.（已完成）发现一个更为重要的问题：每一帧都有发包延时，这个需要精确的计算出每一帧的发包延时，这个非常重要，目前只做了0点偏移，并没有计算发包延时；
   A：首先，要解决系统0点时刻问题：系统认为的第一帧应该已经准备好的系统时刻点，取系统时间的纳秒时间值。
   B：在观看端收到第一个数据包 或 收到服务器反馈的播放序列头信息时，都是设置系统0点时刻的位置；谁先到就用谁设置；
   C：播放器的第一帧0点时间戳：观看端收集好完整第一帧音视频数据的PTS时间戳，越快设置第一帧时间戳，播放延时越小，至于能否播放，不用管，这里只管设定启动时间戳...
   D：千万不能想当然的在收到完整第一帧时间戳的时候才设置系统0点时刻，因为，第一帧从推流端被传输到观看端，一定存在网络延时，还有切片组帧过程，如果第一帧来的太慢，这个延时永远无法消除；
   E：系统0点时刻与第一帧0点时间戳，是两个完全不相干的东西，各自有自己的时间轴，帧的时间戳在帧被推流端创建时就已经确定了，是一个相对时间；
   F：系统0点时刻相当于推流端第一帧时间戳在观看端的系统绝对时间映射点，也就是把推流端的播放时间点映射到观看端的播放时间轴的计时起点；
   G：我们可以通过操纵系统0点时刻来灵活的控制延时，因为，音视频播放器都是使用系统时间轴来控制播放的，我们可以人为的添加一个系统时间Delta来控制延时；
   H：通过人为改变系统0点时刻来控制播放延时，而不是通过控制缓冲区，这样相对最简单；实验已经通过，可以明显的看到解码缓冲[0,30]之间匀速波动；
   I：在观看端进行第一个视频关键帧查找时，不要扔掉音频，也不要在找到关键帧之后才设置第一帧0点时间戳，必须在收到第一帧就设置0点时间戳；
   J：注意：观看端收到的所有音视频数据帧都是落后于推流端的，都是相对推流端延时的，只是延时大或延时小的问题；只要控制在0.3秒以下就不会有感觉；
   K：发现：在没有补包的情况下，观看端会收到重复的UDP数据包，UDP会被路由器重复发包，出现 Supply Unknown 事件...

7.（已完成）推流端在准备好视频头之后，就立即完成了创建命令和序列头命令，然后，才到达音频头信息，造成音频头信息无法汇报给观看端，观看端无法处理音频；
   A：需要在音视频都准备好之后，才进行推流线程的启动工作，就能避免这个问题；
   B：音频播放缓存优化 => 使用环形队列处理解码后的音频数据 => 每个音频解码后的数据包长度是固定的 => 忽略时间戳乱序问题，认为后面的时间戳一定比前面的大；
   C：视频播放缓存优化 => 使用环形队列处理解码后的视频数据 => 每个视频解码后的数据包长度是固定的 => 忽略时间戳乱序问题，认为后面的时间戳一定比前面的大；
   D：使用环形队列优化 => 有一个潜在风险 => 数据帧根据PTS排序问题，始终假设后面数据的PTS一定比前面数据的PTS大 => 因为始终全力追赶播放，缓冲个数相对较小；
   E：由于视频播放使用环形队列太过复杂，已经放弃，只在音频播放里面使用环形队列；

8.（已完成）视频在没有丢包的情况还是会出现解码失败，需要尝试对解码后的AVFrame进行队列管理，P帧也需要关联数据；也可能与网络抖动有关系；
   A：尝试给解码后的AVFrame设置一个缓存列表，2倍gop_size，仍然会出现解码失败的问题，有可能跟时间戳的混乱有关系，即时间戳大的先到，导致无法解码；需要增加一个解码前缓存；
   B：尝试发送和接收端进行存盘验证音视频数据帧的解码正确性，目前即使完整数据帧也会发生解码失败问题，这个需要进一步验证。在保证网络畅通的情况下；
   C：发现，网络流畅的情况下视频解码出错的可能性很小，这个就很奇怪了。
   D：这个问题可能是之前修改的解码方式有关，后来，还原后，即使网络差的情况下解码也没有出错。
   E：只要网络上有人进行微信视频通话，就会造成本地无线网络卡死，造成获取无线IPC数据失败，无法进行数据交互。
   F：只要网络数据不丢失的情况下，是不会造成解码失败的，解码失败一定是数据错误造成的。
   G：看阿里云的帮助文档：购买的带宽都是下行带宽，用户上行带宽没有限制；
   H：通过海康的后台页面动态设置码率信息，并不能及时反馈到压缩器当中，接收端还是要停止重连之后才能更新码率；相当于IPC保留了原始链接，等没有用户链接了才关闭原来压缩器；
   I：后来，在使用海康的rtsp输出连接时，通过海康网页后台进行动态码率调整，rtsp码率立即就进行了调整；之前可能是用的SDK模式？

9.（放弃）音频是按采样率匀速播放的，一旦出现卡顿，一定会出现数据堆积，因为数据并没有丢失只是慢了，音频播放并不知道，只会仍然按采样率速度匀速播放下去，这样必然发生堆积，延时增大；
   A：（放弃）也就是说，网络抖动的累计延时都会体现在音频上面，因此，必须要保证音频数据的匀速到达，所以，必须在发送端建立两个环形队列，音视频分开处理，优先发声音，优先补声音；
   B：（放弃）尽量让音频有一个匀速的播放空间，不要像弹簧一样来回波动，任何的波动，音频延时都会被累加，不清理就会延时不断增大，清理就会造成音频卡顿频繁，体验变差；
   C：（放弃）丢包进入之后立即解码，音频线程去掉，使用回调模式；
   D：（放弃）视频线程去掉，使用接收线程定期显示画面；
   E：（放弃）网络接收过程不要一次只接收一次，接收多次，直到发生错误为止，这样可以快速获取网络数据；
   F：（放弃）https://blog.csdn.net/abcSunl/article/details/77196788 => 有关音频变速不变调的思路
   G：（已完成）为什么音频播放总是会跟不上，总是会发生数据堆积？（有可能是声卡硬件的问题？）
   H：（已完成）经测试发现，网络抖动之后，后面数据一下子就被灌入，但是，音频播放并不会因此加速播放，卡顿时间被累积，音频被拉长，
       后续数据就发生堆积，累积延时增大，引发音频清理，造成咔哒声音，因此，需要找到能够精确控制音频播放的工具，不能只是扔给硬件了事。
       可以考虑自己使用DirectSound，直接细粒度的控制音频播放缓冲区；
       https://blog.csdn.net/disadministrator/article/details/43966017 => 有关音频解码后格式
       https://blog.csdn.net/leixiaohua1020/article/details/40540147 => 雷神出品，有关DSound
       https://blog.csdn.net/williamaiden/article/details/72799761 => 有DSound的详细说明
   I： http://www.yunliaoim.com/im/2516.html => 有关延时的详细说明 => 没啥作用
   J：目前看来，对于音频的处理，只能采用增大缓存的方法进行处理，即：音频出现清理之后，就需要增大延时；一旦增大延迟之后，音频抗抖动就很强了，延迟设定在500毫秒；
   K：最终，采用计算出来的 缓冲评估时间 做为 播放延时时间，来控制播放层的延时问题，网络好时延时低，网络差时延时大，自动动态调节；
   L：也不能来的那么陡，也要用一种遗忘衰减算法，进行播放延时控制；new_delay = (7 * delay + keep_delay) / 8

10.（已完成）需要处理网络拥塞造成的发送端数据拥塞，形成发送端延时，这个需要定期清理，利用观看端的探测机制进行处理；
   A：（已放弃）要解决发包太快造成的网络拥塞问题；前几秒可以用设定码流的2倍发包，后面逐渐降低；按音视频总体码流计算，这样处理后，才能保证音频优先到达；
   B：（已完成）因为是直播，根本没有那么多数据被发送，数据的产生是由压缩器决定的，想快速发包都不可能；
   B：（已完成）码流平滑的前提是获取精确有效的音视频码流数据信息，即：音视频每秒设定的码流信息；
   C：（已完成）如何动态判断网络拥塞，然后动态调整 m_zero_delay_ms，调整音视频播放延时，也要采用平滑调整的方式进行；
   D：（已放弃）目前可能方法有：通过 rtt 探测结果来判断，通过动态视频帧率计算（每秒实际播放帧数）与系统初始化的帧率比较，小于初始帧率，说明出现卡顿，增大延时；
   E：（已完成）动态延时控制，通过缓存评估时间来解决，通过每次探测结果进行播放延时修正；
   F：（已完成）之前就完成了观看端通过探测命令向推流端汇报观看端已播放的音视频序列号，便于推流端删除过期数据；
   G：（已完成）现在需要反过来，让推流端通过探测命令向观看端汇报推流端当前音视频最小序号包的值，便于观看端及时清理缓存，删除已过期的补包序号；
   H：（已完成）这个过程是推流端的拥塞丢帧过程，推流端丢弃的不是一个包而是整个GOP视频数据，音频也要做丢包处理，以视频的丢包时间戳为基准；
   I：（已完成）推流端在每次进行探测发包时，需要进行视频环形队列检测：只要有两个GOP视频视频数据就扔掉最前面那个，并同步到观看端，进行播放控制和丢包清理；
   J：（已完成）发现目前的拥塞处理有问题，每次通过探测命令进行拥塞同步，但是，观看端在收到拥塞同步之前，就发出了一个观看端的最大播放包探测命令，推流端收到后对缓存清理，
       这时，观看端又发现丢包了，请求推流端重传丢包，推流端发现丢包已经被清理了，造成无法补包，观看端图像静止不动；这种已经发生拥塞的时候进行同步操作是不可能的，网络已经出问题了。
   K：（已完成）需要尝试新的办法，不要进行拥塞同步，也不要进行丢包处理，采用在打包之前丢视频帧的方案，超过3秒缓存就只播关键帧，如果连续超过5次都只能播关键帧，就只播放音频；
   L：（已完成）这种方案很好的解决了网络拥塞问题，没有丢包造成混乱的问题，效果很好，后续再继续优化；
   M：（已完成）拥塞判断，需要在有拉流用户接入之后进行，用户没有接入之前，只是简单的丢包，而不进行拥塞累加判定；

11.（已完成）视频也有拥塞延时问题，即网络拥塞之后，再恢复，就会有些数据包滞留在已解码队列当中，这个延时是否是sleep造成的呢？
   A：如果视频播放时，发现有已解码AVFrame拥塞，说明AVFrame的时间戳落后了，这个落后的时间差是怎么来的？不是数据帧时间戳有偏差了，就是系统时间戳有偏差了。
   B：有时候的测试又是精确的，网络拥塞产生延时，网络畅通，延时消失；
   C：这个问题最终是通过系统0点时刻来解决的，观看端收到的第一个数据包就认为是第一帧数据，设定为0点时刻，其实还是有延迟落差，但相对较小；
   D：通过系统0点时刻与第一帧0点时间戳共同配合解决，同时，还找到了控制卡顿的方法（使用缓存评估时间）
   E：修正系统0点时刻，尝试在观看端在发出每一个Create命令的时候进行更新系统0点时刻，因为，如果观看端接收服务器反馈的Create命令有延时，就会累加到播放层，永远无法清除；
   F：目前有三个地方在设置系统0点时刻：发生Create命令、收到Create命令、收到第一个数据包，经过一定实验之后，只保留一个；
   G：最终采用发送Create命令更新系统0点时刻的方法 => 发出创建命令就认为是第一帧数据已经准备好可以播放的时刻点，这样受网络波动延时的影响最小；
   H：这种方案是根据实际情况 => 观看端必然落后推流端，观看端接入的瞬间一定认为推流端已经准备好了，这样就会做到延时最低；（实际推流端是否准备好不太重要）

12.（已完成）新增P2P探测，比对P2P的rtt与服务器端的rtt，谁小向谁发数据；
   A：（已完成）学生推流端 => 音视频数据包和补包都可以进行线路选择，选最快路线；
   B：（已完成）讲师观看端 => 只有补包需要进行线路选择，选最快路线；
   C：（已完成）在同一个内网里的推流端和观看端无法进行P2P探测，只能通过服务器中转模式；

13.（放弃）新增服务器推流缓存，便于给观看端快速补包，而不是只能到推流端补包，加快补包过程，降低延时；
   A：经测试发现：效果非常差，带来了额外的复杂度，因为有乱序存在，在服务器补包就会引发混乱，效果非常差；
   B：使用P2P模型直接进行补包操作，效果非常好，应该尝试这种最直接简单的方式，还能减轻服务器的带宽压力；

14.（已完成）跟阿里云客服电话沟通之后，总结如下：
   A：经典网络相对没有专有网络安全，但是自由度更高，没有那么多限制，目前最好不要升级，阿里云后期会全部升级为专有网络；
   B：1Mbps抖动特别剧烈，目前没有找到好的解释和原因，尝试临时升级到5Mbps试试情况，如果还是抖动很大的话，就需要提交工单排查；
   C：顺便询问了有关CDN的问题，南北方需要CDN专线做桥接，如果用CDN的话只需要购买一台服务器就可以了，我们这种私有的UDP协议需要看CDN厂商是否支持自定义私有协议；
   D：否则，只能在南北方购买服务器，再通过专线链接通讯；
   E：临时升级到5Mbps之后，依然网络抖动非常大；从他们那里进行网络探测的结果是否依然抖动很大？
   F：将软件放到另外的地方，同样进行推流、观看，1Mbps带宽，网络稳定，没有问题；
   G：这么看来，应该是家里移动宽带的问题，不能同时上行下行传递UDP数据包，现在需要把上行和下行进行分离，看看这样分开是否同样会有影响；

15.（已完成）联系了移动宽带运营商，上门看看为什么在码流很小的UDP上传都会发生网络拥堵的问题？
   A：到家查看网络状态，截图之后，上报后台，过几天给结果；

16.（已完成）将推流和观看分离之后，遇到的新问题：
   A：（已完成）视频解码之后获取的高宽，与SPS序列头里的高宽不一样，需要修改视频播放代码，在图像转换时，使用预设的高宽进行格式转换，用一个固定的转换空间，而不是动态分配；
   B：（已完成）推流端已启动，一直推流，观看端后接入，没有确定第一个包的包号，造成观看端从1开始期待收取数据包，造成图像无法观看；将第一个包号减1设置成最大播放包号；
   C：（已完成）播放窗口的任何变化，都要通知播放器，更新播放窗口，否则，会出现画面无法显示的问题 => 重建SDL窗口、渲染、纹理，来解决D3D页面丢失问题；
   D：（已完成）人为的将启动延时缩小了100毫秒，如果有问题，后期再调整；
   E：（已完成）修正了重复设定第一个数据包序号的问题；
   F：（已完成）推流线程、观看线程退出缓慢的问题 => 先停止数据产生的源，再停止数据处理线程；

2018.06.01 - 2018.06.30
===============================================================================================
1.（已完成）要完成双向低延时的交互回放，需要完成下面四项低延时功能：
   A：（已完成）06.11 确定时间戳状态下，使用SDL2.0完成音视频播放的低延时功能，音频缓存超过300毫秒主动清理音频缓存。
   B：（已完成）06.26 学生端将音视频数据通过单独线程用UDP发送给Linux服务器，用最快的速度，尽量少留缓存，保留一秒缓存，供丢包回传使用；
       https://www.cnblogs.com/lidabo/p/6857616.html => 有关低延时的技术案例
       https://www.cnblogs.com/li0803/archive/2010/11/20/1882792.html => RTP/RTCP协议详解
       https://blog.csdn.net/bytxl/article/details/50400987 => RTP/RTCP协议详解
       https://www.cnblogs.com/foxmin/archive/2012/03/13/2393349.html => RTP/RTCP协议详解
       https://blog.csdn.net/machh/article/details/51868569 => RTP/RTCP协议详解
       https://blog.csdn.net/fishmai/article/details/53676194 => RTP 打包实例
       为了简化操作，所有的 Payload 都统一成固定大小，放入环形队列当中，这样可以直接通过序号就能定位到；
       需要解决有空隙的环形队列问题，网络包到达的先后顺序不一致，会有空隙，circlebuf要解决这个问题；
       接收端，需要对circlebuf进行封装，可以根据网络包序号，进行缓冲的自由伸缩，判断，插入，这样可以避免网络层获取到的缓冲干扰；
       综合比较之后，选择单进程多线程进行UDP服务器的开发，用主线程阻塞等待接收数据，收到之后，放到另一个线程进行数据处理；
       针对学生端推流时 => 发送端和接收端都要各自进行延时探测，各自使用自己的探测结果进行计算；
       针对老师端推流时 => 很多处理过程都要调整，需要单独处理；
       进入最复杂的阶段 => 丢包检测，丢包补充，缓冲区控制；
       已收到最大连续包号 = 最小丢包号 - 1
       解决了UDPSendThread当中数据帧打包问题：帧总长度刚好是800整数倍（分片大小），打包错误会导致psize为0；
       现在重点解决视频帧解码无法得到got_picture的问题，参考文档如下 => 这些文档的方法都不能解决最终问题 => 最终发现是由于AVFrame的缓冲区受后面解码帧的影响；
       实验发现：只要不释放AVFrame空间，即使缓存上百帧，后面解码的数据帧也会影响前面的上百帧数据，因此，解码完毕，需要立刻将解码后的数据拷走，原来AVFrame不要释放，为后续待解码AVPacket提供解码参考源，这也能解释为什么不丢包也会解码失败的问题；
       终于解决了延时播放，自由控制时间戳，解码后的数据混乱问题，解码后无法得到picture等等一系列头疼问题，主要是对ffmpeg的解码机制不了解；
       RTSP的时间戳计算目前看没有问题，网络解析出来的压缩数据帧也没有问题，就是目前的方法：构造AVPacket，投递给直接解码就可以了；
       https://bbs.csdn.net/topics/392059669 => 只有I帧和P帧 => 没有解决
       https://blog.csdn.net/u013898698/article/details/61433612 => 编码延时 => superfast => zerolatency
       https://blog.csdn.net/leixiaohua1020/article/details/14215833 => CODEC_CAP_DELAY
       https://blog.csdn.net/h514434485/article/details/52164087
       https://blog.csdn.net/huyinguo/article/details/4725326 => CODEC_FLAG_LOW_DELAY
       https://bbs.csdn.net/topics/390692774

       解码成功，got_picture为0，说明需要后续的数据帧，会造成解码器内部的数据帧堆积，需要让解码器把堆积数据吐出来 has_b_frames
       https://blog.csdn.net/u013506600/article/details/22329403 => 有关AVFrame当中picture解码问题；
       需要解决解码失败之后，造成解码器内部会缓存数据帧，后续再解码时，永远有延时，需要将解码器内部缓存的数据帧快速取出来，避免延时；
       需要配合best_effort_timestamp时间戳，而不能用AVPacket里面的时间戳，只有解码器知道已缓存的数据帧的真正时间戳是什么；
       然而，以上的实验并不成功，还会造成反复卡顿现象，需要另外寻找新的方法；
       同时，增大了AVFrame解码缓冲的块数，仍然无法解决解码失败后的延时问题；
       https://blog.csdn.net/leixiaohua1020/article/details/19016109
       http://blog.jianchihu.net/avcodec_decode_video2-no-pic.html

       H.264在只有I帧和P帧的时候，解码失败造成视频数据帧堆积的问题，这个文档有说明 => 吐出所有的坏帧...
       在没有B帧的情况下，误码可能会导致视频帧在这里堆积，所以要在这个判断之前将s->avctx->has_b_frames强制设为0。
       https://blog.csdn.net/quanben/article/details/4400336

       （已完成）CUDPSendThread => CPU占用过高 => 需要优化 => 可以考虑使用QT的套接字模型来解决
       （已完成）CUDPRecvThread => CPU占用过高 => 需要优化 => 可以考虑使用QT的套接字模型来解决
       （已完成）丢包问题得到解决，在 doParseFrame() 当中不能将环形队列中的数据包抽干，抽干会导致新包无法写入环形队列？
       （已完成）支持环形队列被抽干，这样可以快速将有效帧投递，降低延时；但是对环形队列为空时，有新包到达后需要提前预判是否有丢包，并做丢包处理；
       （已完成）千万不能在环形队列当中进行指针操作，当start_pos > end_pos时，可能会有越界情况，必须用接口，不能用指针偏移，环形队列可能会回还...
       （已完成）终于解决了发送端和接收端数据包内容不一样的问题，其实是对环形队列的使用错误造成的问题，环形队列千万不能用指针，只能用接口，防止队列回还造成的错误；

   C：（已完成）学生端使用单独线程用UDP从Linux服务器，用最快的速度，拉取音视频数据，组成数据帧，使用SDL2.0播放出来；
   D：（已完成）将学生端数据拉取、组包、解码过程封装起来，移植到讲师端；整个过程控制在1秒以内；
   E：（已完成）改造讲师端推流机制，使用UDP推流到Linux服务器；不使用rtmp推流；
   F：（已完成）学生端从Linux服务器，拉取讲师端推流数据，组成数据帧，使用SDL2.0播放出来，整个过程控制在1秒以内；
   G：（已完成）对错误日志进行了优化，统一存放在用户临时工作目录下面；
2.（已完成）需要解决Teacher端交互时的高延时问题，需要解决的问题如下：
   A：（已完成）采集端推流到服务器，讲师端从服务器获取流，讲师端组包还原音视频数据帧，讲师端解码H264+AAC，讲师端播放；
   B：（已完成）讲师端推流到服务器，采集端从服务器获取流，采集端组包还原音视频数据帧，采集端解码H264+AAC，采集端播放；
   C：（已完成）需要在网上找找有关低延时视频会议系统的解决方案；
   D：（已完成）需要重新编写采集端，专门处理多路RTSP输入，一路讲师端网络输入，选择一路RTSP网络输出；
   E：（已完成）需要重新编写讲师端，专门处理多路摄像头输入+多路窗口输入+1路麦克风输入+1路采集端网络流输入，输出讲师端网络流；
   F：（已完成）有关RTMP延时的分析文章收集：
                http://blog.chinaunix.net/uid-26000296-id-4932817.html
                https://blog.csdn.net/qq_38810947/article/details/74199967
                https://blog.csdn.net/zhiboshequ/article/details/79955433
                https://blog.csdn.net/king457757706/article/details/51004253
                https://www.cnblogs.com/lidabo/p/6857616.html => 更全的低延时文章
                https://blog.csdn.net/mandagod/article/details/52559053 => 比较详细
                https://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&mid=2653547697&idx=1&sn=acc748b7fcf0058b58e244970e51eabc => 微信原始地址
   G：（已完成）简单UDP服务器（Linux）：
                https://blog.csdn.net/u011922698/article/details/55218313
                https://blog.csdn.net/yueguanghaidao/article/details/7055985
                https://blog.csdn.net/aa120515692/article/details/47294335 => UDP并发服务器
                https://blog.csdn.net/aa120515692/article/details/47299529 => UDP并发服务器Select机制
                https://www.cnblogs.com/skyfsm/p/6287787.html?utm_source=itdadao&utm_medium=referral => 比较好的Linux下的UDP基础文章
                https://blog.csdn.net/yang_rong_yong/article/details/49818249
                https://blog.csdn.net/u010643777/article/details/72190891 => UDP下的epoll并发框架 => 有人测试epoll效率低，越简单越好
                https://blog.csdn.net/dog250/article/details/50557570 => UDP的epoll并发框架 => 有人测试epoll效率低，越简单越好
                http://bbs.chinaunix.net/thread-4175621-1-3.html => UDP服务器性能测试 => 很重要的一篇模型实验总结文章，越简单越好
                http://bbs.chinaunix.net/thread-4096315-3-1.html => 不要加锁. 不要弄复杂, 程序先写出来, 你就会发现UDP处理高并发实在太简单了.
                https://blog.csdn.net/libinbin_1014/article/details/50096211
3.（已完成）先用ffmpeg解析H.264视频帧，再用SDL2.0绘制到窗口上；
   A：（已完成）尝试使用SDL2.0解码音频，并回放出来；
   B：（已完成）https://blog.csdn.net/jay100500/article/details/52955232 => AAC帧头格式 => ADTS
   C：（已完成）两路音频同时播放时，只有一路有声音，关闭时还会卡死在界面；
   D：（已完成）需要解决音视频自主播放时的同步问题；
   E：（已完成）进一步研究发现：SDL2.0不支持多路音频播放，支持起来麻烦，还要解决音视频同步问题；
   F：（已完成）需要将目前的实验代码，转移到NO_DELAY分支，研究obs里面针对网络流的数据播放，它是音视频同步的，而且支持多路；
   G：（已完成）将需要的模块单独整理出来，进行符合自己要求的封装；
   H：（已完成）这样可以将所有的精力都投入到obs的研究当中，利用obs的输出库，来完成绝大部分工作，除了服务器之外；
   I：（已完成）目前，单向的学生端推老师端的延时在3.5秒~4.0秒，根本没发进行实际应用；
   J：（已完成）还是要使用SDL2.0播放音频，这种方式最快速也最容易，用obs的库非常麻烦，它是专门服务obs使用的，属于整个obs体系当中；
   K：（已完成）解决了音频单声道的问题，现在需要对音频进行优化处理，不要产生硬编码或哒哒声；（采用解码后的缓冲，而不是固定缓冲）
       https://blog.csdn.net/beiliufangdegezhe/article/details/41623579
       https://blog.csdn.net/u012853439/article/details/53218662
   L：（已完成）进行音视频同步处理过程：
       https://www.cnblogs.com/wangguchangqing/p/5900426.html => 比较好的基础文章 => 通常来说只有在流中含有B帧的时候，PTS和DTS才会不同。
       https://www.cnblogs.com/jiayayao/p/6890882.html
       https://blog.csdn.net/i_scream_/article/details/52760033 => SDL2.0的音视频同步文章
       https://blog.csdn.net/leixiaohua1020/article/details/42181571 => 裸流解码，
       https://blog.csdn.net/nine_locks/article/details/48007055
       https://blog.csdn.net/u010289908/article/details/46507643 => 非常重要的PTS计算方法；
       https://blog.csdn.net/leixiaohua1020/article/details/11800877
       https://blog.csdn.net/leixiaohua1020/article/details/14215821 => AVStream
       https://blog.csdn.net/chenchong_219/article/details/13161509 => 时间戳详解
       https://blog.csdn.net/leixiaohua1020/article/details/12678577 => av_read_frame 详解
   M：（已完成）mp_media_next_video 很关键，计算解码后的时间戳，投递到obs显示系统；
   N：（已完成）我们处理的数据已经是数据帧了，可以直接构造AVPacket来进行解码，而不需要调用av_parser_parse2，AVPacket的结构如下：
       https://www.cnblogs.com/xiacaojun/p/6638800.html => AVPacket详解
       https://blog.csdn.net/qq_27727131/article/details/51799663 => sps计算帧率
       https://blog.csdn.net/caoshangpa/article/details/53083410 => sps计算帧率 => 替换原有的计算方式，这种更精确。
       https://blog.csdn.net/leixiaohua1020/article/details/44084321 => 读取数据给音视频流对象赋值 => AVStream
       pts => 显示时间，就是帧时间戳
       dts => 解码时间，与显示时间相同
       pos => 在缓冲区的位置，可忽略
       data  => 数据区指针
       size  => 数据区大小
       flags => 0（普通帧）1（关键帧）2（坏帧）
       duration => 视频 => 1000/帧率 => 毫秒 => 根据time_base(1,1000)和帧率的计算值，不能用（下一帧pts减去当前帧pts）
       duration => 音频 => 采样点/采样率*1000 => AAC => 1024/采样率*1000 => 毫秒
       stream_index => 流编号，区分音视频标记
   O：（已完成）AVStream->start_time是怎么计算出来的？没有找到相关代码，就默认为0；
   P：（已完成）AVPacket的字段研究完毕，开始研究音视频解码后的时间戳的计算问题；
       AVPacket的时间戳是可以通过数据帧直接赋值，单位是毫秒，这是解码前的时间戳；
       AVStream的时间戳是解码后的时间戳，是跟系统本身是相互关联的；
       mp_media_next_video => decode_packet => 解码数据帧 => AVStream的时间戳是以纳秒为单位的 => time_base(1,1000000000)
       将解码后的pts和duration都需要转换成纳秒形式保存起来 => os_gettime_ns => 取得当前时间的纳秒值
       interrupt_callback => 是为了防止avformat_open_input在解析网络数据卡死时做的检测机制；
       mp_media_reset => 在这里进行时间戳的重置 => obs里面的ffmpeg网络播放机制，延时非常大，需要改进；
       base_sys_ts => 全局，只要source不删除，只初始化一次；
       m->play_sys_ts => 播放系统开始时间戳 => 在mp_media_reset中被重置 => os_gettime_ns()
       m->start_ts => 第一帧开始时间戳（纳秒），取音视频第一帧最小的时间戳 => mp_media_reset 中初始化一次
       m->next_pts_ns => 下一帧的PTS（纳秒），先初始化为音视频第一帧最小时间戳 => 每次检查音视频队里中最小时间戳赋值给next_pts_ns => mp_media_calc_next_ns
       m->next_ns => 下一帧播放时的时刻点（纳秒），mp_media_calc_next_ns 和 mp_media_sleepto 用到，每次最多sleep 20毫秒，等待下一帧播放的时间不要超过3秒；
   Q：（已完成）当视频中有B帧时，需要处理 composition time => RenderOffset
       http://www.cocoachina.com/bbs/read.php?tid=71131
       http://www.likecode.com/2014/11/21/111
       将obs设置成文件模式，对比AVPacket时间戳，发现一些规律，但无法应用在libmp4v2当中，后来即使强制将时间戳调整一致，还是无法在498样本大小位置解码出第一个关键帧
       后续，需要编译ffmpeg，与obs对比跟踪调试avcodec_decode_video2，看看到底哪里不一样，目前这种直接调用解码器的方式可能存在一定的问题，需要编译ffmpeg才能调试清楚；
   R：（已完成）时间戳的计算可能还是存在问题：
       时间戳的计算没有问题，avcodec_decode_video2 也没有问题，是由于模拟延时造成的问题；
       没有B帧的情况下，目前的处理方式没有问题，通常 BaseLine 压缩模式就没有B帧，只有I和P帧；
       模拟丢帧的情况也没有问题，就是出现花屏；
       模拟延时问题一直没有成功，总是造成丢帧，混乱；
       视频延时帧不能丢弃，有可能是过期的落后帧，会造成花屏，视频缓冲很快就会消耗光，难点在音频；
       视频超前帧，不能立即播放，需要等待最大20毫秒时间；
       音频延时200毫秒，清空缓存，等待新数据帧，超前100毫秒以上，等待下次填充；
       音频为什么会出现来不及播放的问题？这个需要深入研究，研究SDL2.0例子，有关音频的播放问题；(估计还是要将音视频单独用线程处理）
       SDL2.0升级到2.0.8，需要调用CoInitializeEx()，http://wiki.libsdl.org/SDL_AudioSpec，2.0.4开始，可以不用callback，用SDL_QueueAudio()代替；
       https://blog.csdn.net/kiazhu/article/details/54744486 => 比较全面的SDL音频API详解；
       音视频单独使用播放线程，因为是以本地时间做为基准时间，音视频的时间戳可以相互不干扰，为了最大限定的降低延时，音视频相互干扰的概率越小越好。

4.（已完成）重新设计采集端的音视频播放流程：
   A：（已完成）音视频的解码、播放，都放在一个线程当中；
   B：（已完成）第二版：发现放在同一个线程，造成播放时有延时干扰，在使用SDL2.0的音频时，采用主动投递方式；

5.（已完成）本地拉取RTSP数据流，进行最快速度的解码显示，也有900毫秒左右的延时，因此，学生端不能用IPC模式，需要对方案进行调整：
   A：（已完成）海康IPC的SDK回显模式，延时在200毫秒左右，比RTSP拉流的延时要低很多；
   B：（已完成）ffmpeg解码器配置参数之后，RTSP延时又变成了300毫秒左右，很奇怪，屏蔽参数之后延时也没有变大；
   C：（已完成）学生端，使用vs2015，利用libobs的函数库，读取USB-Camera的数据，用x264进行压缩编码，去掉B帧模式；最大限度的降低延时；
   D：（已完成）学生端，使用vs2015，利用UDP获取讲师端推流数据，组合成音视频帧，调用ffmpeg解码，SDL2.0回显；
   E：（已完成）学生端，还是用vs2010，改造采集端代码，放到B_NO_DELAY分支，完成了播放层面的低延时回放功能；

6.（已完成）obs里面直接拉取本地rtsp流的延时在2.5秒左右，音视频同步，将obs里面播放网络音视频的部分抽取出来，进行直接的音视频数据帧的播放；
   A：（已完成）由于用SDL2.0直接播放音视频数据帧，存在音频问题，存在音视频不同步问题，这是第一个目标，使用采集端的NO_DELAY分支进行测试；
   B：（已完成）直接拉流延时控制在300毫秒以下，才能在网络上达到1000毫秒以下的目标；重点研究obs-ffmpeg如何播放和音视频同步的方法；
   C：（已完成）ffmpeg相关的代码其实没有感觉中的那么费劲，需要静下心来慢慢体会，某种程度上比c++还要简单一些，思维方式不同。
       av_find_best_stream
       avcodec_find_decoder => 查找 codec
       avcodec_alloc_context3 => 分配 codecContext
       avcodec_parameters_to_context => 设置参数
       avcodec_open2 => 打开解码器
       av_frame_alloc => 分配帧缓冲区

2018.05.01 - 2018.05.31
=========================================================================
1.（已完成）海康人脸系统分为三种层次：
    A：人脸检测 => IPC检测到人脸会通过报警回调接口反馈信息；http://www.hikvision.com/cn/prlb_841.html
    B：人脸抓拍 => IPC检测到人脸不仅报警还会抓拍图像一起反馈；http://www.hikvision.com/cn/prlb_1608.html
    C：人脸比对 => IPC检测到人脸不仅抓拍还要跟已有的人脸库进行比对，然后一并进行回调反馈通知；http://www.hikvision.com/cn/prlb_1609.html
    D：因此，三种形式的IPC是不同的类型，处理的复杂度是不一样的；人脸比对的IPC淘宝价在4500元左右；
2.（未完成）DS-2CD2432F-I/W，300万像素，无线卡片式IPC，音频格式需要转换成AAC格式；12V移动电源可用；英文版，不带电源适配器；
    A：（未完成）由于是WIFI模式，开启预览再拉流，感觉有时网络会不稳定，需要降低视频大小，降低至1280*720；或直接关闭预览；
    B：（未完成）某些自动配置无法执行，后续需要进一步的优化，详情查看 CCamera::onDeviceLoginSuccess() 函数；
    C：（已完成）不要使用这款设备，音频和配置都有问题，可以用“小企鹅”完全代替；
3.（已完成）DS-2CV3Q21FD-IW，200万像素，无线小企鹅IPC，目前测试结果如下，有线地址 => 192.168.1.63
    A：（已完成）可以用组播探测软件访问配置，IP地址 => 192.168.1.24，登录密码 => adminBBORSE，在摄像头底部的六位验证码；
    B：（已完成）自动链接萤石云，不能用网页访问，iVMS-4200可以远程配置；http://www.hikvision.com/cn/download_more_390.html
    C：（已完成）支持 H264+AAC 方式，开启声音的降噪功能，效果很好；
    D：（已完成）时间戳不能使用PC的时间戳，必须使用硬件本身的时间戳，否则，音视频不同，画面卡顿严重；在Chrome控制台能看到黄色警告信息；
    E：（已完成）目前看来最大的问题是延时比较大，采集端、服务器、播放终端都有可能造成较大的延时问题，这个需要后续专门进行优化处理；
    F：（已完成）音频开启环境噪声过滤、关闭移动侦测、固定无线IP地址(192.168.1.24)；小企鹅完全能满足需求；
    G：（已完成）主码流 => STD_H264 | 1280*720 | 变码率 | 1024kbps | 复合流 | I帧间隔20 | 视频帧率15fps | AAC | 音频32kbps | 采样率16kHz
    H：（已完成）子码流 => STD_H264 | 704*576 | 定码率 | 512kbps | 复合流 | I帧间隔20 | 视频帧率15fps | AAC | 音频32kbps | 采样率16kHz
    I：（已完成）先用“设备网络搜索”修改IP地址，再用iVMS-4200配置无线、音视频参数；后期直接通过SDK配置想要的音视频参数；工具只是配网络；
    J：（已完成）无线模式，本地回放延时在200毫秒左右，低码流下清晰度比较高；开启降噪模式声音效果好。

4.（已完成）DS-2DE2402IW-D3/W，400万像素，无线IPC，目前测试结果如下，有线地址 => 192.168.1.64
    A：（已完成）组播探测方式改动了，使用标准的ONVIF模式，不是通过组播回应的方式，需要在采集端加入标准的ONVIF协议；使用第三方SDK来完成ONVIF；
       （新情况）在无线模式下无法完成自动探测，在有线模式下，能够探测到无线地址，这样一个IPC就会有两个通道64（有线），29（无线），有一个多播搜索开关。
    B：（已完成）可以用网页进行配置管理，登录IP地址 => 192.168.1.29，登录帐号 => admin:admin123
    C：（已完成）主码流 => STD_H264 | 1280*720 | 变码率 | 1024kbps | 复合流 | I帧间隔20 | 视频帧率15fps | AAC | 音频32kbps | 采样率16kHz
    D：（已完成）子码流 => STD_H264 | 704*576 | 定码率 | 512kbps | 复合流 | I帧间隔20 | 视频帧率15fps | AAC | 音频32kbps | 采样率16kHz
    E：（已完成）音频开启环境噪音过滤功能，可以有效解决环境杂音；
    F：（已完成）1M码流的效果，感觉和“小企鹅”的效果差不多；
    G：（已完成）先用“设备网络搜索”修改IP地址，再用浏览器修改无线、音视频参数；后期直接通过SDK配置想要的音视频参数；工具只是配网络；
    H：（已完成）无线模式，本地回放延时在150毫秒左右，比之前的设备要好一些；开启降噪模式声音效果好。
    J：（已完成）300k低码流模式下效果差，可以考虑开启smart264试试。经测试320k低码流开启smart264效果好很多。
    K：（已完成）无路那是smart264还是普通264，都只有I帧和P帧，没有B帧，目的是降低延时。
    L：（已完成）smart264模式下，延时在300毫秒，如果是普通264，延时在900毫秒；
    M：（已完成）smart264模式下，关键帧间隔不能设置，实测的结果是大概10秒以上；关键帧少，码流就会低，同等码流下质量就会好很多。

5.（已完成）采集端 IPC 时间戳不能使用PC端时间，必须使用RTSP传递时间，否则，会造成图像卡顿，Chrome的控制台提示音视频的时间戳有问题；上次Tim提到的卡顿也是这个原因；
    A：（未完成）如果这样修改之后，需要考虑RTSP时间戳发生跳变的问题，现有的摄像头就能测试；

6.（已完成）赞|踩|评，有关“删除”界面的显示有问题，任何已登录都可以删除评论，只有创建者才能删除评论；
    A：（已完成）修正 云录播|云监控的评论删除页面鼠标滑过时是否显示删除按钮的问题；
    B：（已完成）修正 云录播|云监控在IE8下添加|删除 评论时的等待样式错位问题；

7.（已完成）云教室的播放器，需要处理三种类型的视频：
   A：wk_camera => 采集端按需推流的直播视频，编号为 1~200000   => live/live1 - live/live200000
   B：wk_live   => 教室端直接推流的直播视频，编号为 200001~... => live/live200001 - live/liveXXXXXXXX
   C：wk_record => 录像记录视频；

8.（已完成）有关在线教育的学习平台升级改造，可以参考 http://study.163.com/，整个用户体验、使用流程、交互界面 都可以参考；

9.（已完成）直播教室的呈现页面进行调整，加入房间号的功能，在醒目的地方加入直播间号码，并对开播时间显示位置进行调整；
   A：（已完成）将课程名称放在最上面，将时间与讲师并列，新增 教室（显示直播房间号）；
   B：（已完成）这个教室号（直播房间号）就是与采集端|教师端关联的唯一关键号码；

10.（已完成）采集端|教师端 可以考虑采用输入房间号的形式，来完成直播间的加入，参考目前主流的直播网站的方式，需要进一步的分析与思考，看看怎样才更好一些；
    A：（已完成）研究了一下斗鱼直播，斗鱼直播伴侣和obs是一样的功能，界面不同，内核有些差异，主播大多用obs，因为换平台不用换软件，虽然伴侣用起来更简单，连直播地址都不用输入；
    B：（已完成）每一个主播都需要申请一个房间，分配一个房间号，http://www.douyu.com/xxxx，直接可以进入主播的直播间，观看直播；
    C：（已完成）ios手机直播可以直接通过投屏到PC端，PC端再通过捕屏直播，也可以使用app通过获取镜像模式，直接直播；安卓也可以通过手机投屏的方式进行直播；
    D：（已完成）因此，房间号很重要，是链接 采集端+老师端+网页端 的核心纽带；采集端和教师端都需要加入同一个房间才能进行交互；

11：（已完成）通过云教室后台，将 采集端 加入到 直播间，而不是通过 采集端 自己加入，这样就能简化采集端软件的操作流程，将复杂度留给网站处理；
    A：（已完成）采集端 也要新增一个操作按钮 => 绑定直播间 => 将采集端挂在某个wk_live上面；这样两边都可以自主操作；
    B：（已完成）采集端 点击“绑定直播间”，弹出已有的直播间列表，选择一个加入其中；
    C：（已完成）采集端 在“绑定小程序”右侧，新增“绑定直播间”按钮；
    D：（已完成）采集端 新增 CDlgRoom 对话框，可以列出所有的直播间，并选择一个直播间加入其中；

12.（已完成）改造 媒体源 为 学生互动 => 枚举所有当前教室里的采集端上的有效IPC列表，
    A：（已完成）改造媒体源的界面，只留下一个ListView控件，专门显示讲师端登录的云教室上，已经挂载的采集端上的IPC列表；
    B：（已完成）网站端只查询采集端上正在运行的通道列表，采集端上没有运行的通道不显示；
    C：（已完成）显示在线摄像头列表，选择一个，获取rtmp地址，在主窗口中显示出来；
    D：（已完成）需要让“互动教室”保持在线状态；
    E：（不处理）需要对ffmpeg_source访问失败时的处理 => 将camera_id和player_id复位 => media_stopped()
    F：（已完成）ffmpeg_source属性框双击关闭属性框，属性窗打开后，自动定位到当前正在运行的记录行；

13.（已完成）新增 LoginWindow 窗口，处理 loginLiveRoom 和 logoutLiveRoom 事件。
    A：（已完成）Teacher端必须先加入一个直播间Room之后，才能开始直播，没有加入房间之前不能开始直播，一个房间只能有一个Teacher端在线；
    B：（已完成）Teacher端必须在主界面启动前就要选择房间；使用curl机制，跟采集端机制保持一致；
    C：（已完成）Teacher端尝试了QT搭建漂亮界面的功能，确实比MFC创建界面方便多了，而且效果比MFC强100倍；
    D：（已完成）绘制圆角窗口，需要在paintEvent()中创建圆角矩形addRoundedRect()，再用QBrush填充；使用样式表设置圆角矩形不起作用；
    E：（已完成）还有一种办法是：用png的透明作用，来设置圆角矩形；设置窗口或控件的字体、颜色，使用样式表比较方便；
    F：（已完成）obs-studio/global.ini中新增配置 General:LiveRoomID|LiveRoomKey|LiveRoomServer，保存已登录的云教室号码与直播地址，做为中转；
    G：（已完成）obs-studio/basic/profiles/default/service.json中更新云教室直播地址配置，在主窗口创建过程中；
    H：（已完成）wk_live当中新增讲师端在线标志status，讲师端登录或退出都需要更新这个标志；
    I：（已完成）登录成功，自动获取直播地址，自动更新配置文件service.json

14.（已完成）云教室老师端动态截图功能有点问题，NV12不能存盘，需要转换成YUV420，但是图片清晰度失真，I420输出，系统提示有转码问题，后续还要调整；(格式参数输入问题)
    A：（已完成）使用ffmpeg的sws_getContext()|sws_scale()，输入源需要格式AVPixelFormat，不能写成video_format，否则会造成的格式错位问题...
    B：（已完成）改成 libyuv 转换后，也能正确转换，但是需要自己判断不同像素调用不同接口，还是太麻烦，最终还是使用sws来转换；
    C：（已完成）修改 jpg 的保存质量 => qcompress => https://blog.csdn.net/zhoubotong2012/article/details/79342116
    D：（已完成）总算彻底、完美的解决了NV12格式直接生成jpg文件的问题，详见 => libobs\media-io\video-io.c

15.（已完成）开始推流之后，直播通道每隔2分钟自动截图并自动上传；
    A：（已完成）将截图自动存放在 obs-studio 目录下面 => os_get_config_path()
    B：（已完成）自动每隔2分钟覆盖截图，命名为 live_2000XX.jpg
    C：（已完成）创建自动上传线程，自动连接存储服务器；

16.（已完成）进行编译、发行、打包 等外围工作；
    A：（已完成）完成 老师端 的打包工作；
    B：（已完成）完成 采集端 的打包工作；
    C：（已完成）修正 采集端 配置存放位置，与 Teacher 端一致；
    D：（未完成）后续 采集端 还有一些本地写入的文件Logger.txt，mplayer启动运行时需要写入的文件；

17.（已完成）Teacher端在64位独立主机上运行的问题；
    A：（已完成）jpg背景图无法显示的问题，估计是缺少组件 => imageformats/qjpeg.dll
    B：（已完成）gif背景图无法显示的问题，估计是缺少组件 => imageformats/qgif.dll
    C：（已完成）为什么没有默认的电脑输出声音这个数据源，是由于电脑的默认音频输出有关；
    D：（已完成）为什么窗口捕获永远是针对桌面，不能针对单独的窗口进行 => load_graphics_offsets: Failed to start 'get-graphics-offsets64.exe'
    E：（已完成）64位系统要调用64位的钩子函数，否则，无法捕捉窗口；尝试编译一个64bit的版本；_WIN64是在系统默认的配置，不是在工程设置当中；
    F：（已完成）64位系统安装32位的Teacher没有问题，是由于跟TeamViewer混用之后，截屏递归造成的 => init_hooks 调用get-graphics-offsets32.exe和get-graphics-offsets64.exe，两个进程；
    F：（未完成）如何解决Teacher端的内存泄漏问题？

18.（已完成）开始尝试在win32系统下，编译obs的64bit版本，使用msvs2015，在 192.168.1.19 I3主机上安装
    A：msvs2015 => F:\迅雷下载\VisualStudio，安装 QT VS Tools 
    B：qt5.8.0  => E:\Qt\qt-opensource-windows-x86-msvc2015_64-5.8.0.exe
    C：cmake    => E:\cmake\cmake-3.11.2-win64-x64.zip
    D：obs-deps => E:\obs-deps\dependencies2015.zip

2018.04.01 - 2018.04.30
=========================================================================
1.（已完成）有关“采集端”与“节点网站”的注册说明：
   A：（已完成）“采集端”由于有唯一的MAC地址做标记，无论怎么删除重装，在myhaoyi.com当中都只有一条记录；
   B：（已完成）“节点网站”由于没有硬件标记，而是自己生成的标记，因此，只要数据库删除重装，又会生成一条新的节点记录。
2.（已完成）在寻找演示视频的时候发现jwplayer的演示栏有很多值得参考的东西：
   A：（已完成）要做一些全方位的展示功能，参考jwplayer的操作方式去实现；
   B：（已完成）全部演示 => https://developer.jwplayer.com/jw-player/demos/
   C：（已完成）电视墙   => https://developer.jwplayer.com/jw-player/demos/innovation/click-to-play/
3.（已完成）云录播、云监控的“实时栏”修改成“在线巡查”或“在线巡课”，可以多分屏显示；并能控制每个分屏的显示；
   A：（已完成）在“直播”栏后面，新增“巡课”栏；
   B：（已完成）“巡课”栏界面完全参考jwplayer的电视墙模式，按照交错规律排列直播通道墙；
   C：（已完成）“巡课”排列顺序按照直播通道创建时间倒序排列，不要改变顺序，否则，就要记录顺序；
   D：（已完成）“巡课”页面，鼠标放上去显示通道详细信息，点击播放之后，右上角显示关闭按钮；再次点击其它任意通道，本通道关闭，播放当前点击通道；
   E：（已完成）云录播当中的名称是“巡课”，云监控当中的名称是“巡查”；
   F：（已完成）云录播当中的更改应用到云监控当中；
   G：（已完成）由于IE8不支持flex模式，电视墙模式不兼容IE8（已经兼容IE8）；
   H：（已完成）在图片懒加载过程中，可以支持background-image模式，具体参见页面；
   I：（已完成）在图片懒加载过程中，可以支持加载前等待，加载后关闭等待，具体参见页面；
4.（已完成）chrome浏览器被劫持，在某些网站上会注入脚本，总是会点击广告，后来将chrome整个卸载，并将AppData\Local\Google整个删除，重新安装才解决了。
   A：（已完成）后来又发生了一次，这次发现DNS被串改，应该是木马修改的，木马可能是漏洞未修复造成的，用360修补了144个漏洞。
5.（已完成）采集端的通道控制是通过网站端来控制的，目前是默认256路，估计要改成默认9路；
6.（已完成）后台可以设定录像回滚周期，即：只保留多少天的视频录像；
   A：（已完成）后台统一配置，自动应用到每个通道的录像当中；
   B：（已完成）事件触发设置在采集端上传视频之后，自动检测设定的间隔周期，删除该通道下所有超过周期的录像。
   C：（已完成）数据库 haoyi|monitor 的 wk_system 表中新增 web_save_days 字段；
   D：（已完成）系统设置，新增“录像保留”天数，默认0永久保留；
7.（已完成）“巡课”、“巡查”在IE8当中的显示兼容性问题。
   A：（已完成）不采用flex，使用layui的栅格模式，IE8兼容background-size
8.（已完成）直播管理 新增 查看播放地址功能；
9.（已完成）后台导航栏新增 跳转前台 功能；
10.（已完成）播放页面video-js，双击全屏；
   A：（已完成）https://github.com/ctd1500/videojs-hotkeys，这是videojs的一个插件，支持双击全屏；
   B：（已完成）利用这个插件的源码进行单独的双击全屏修改；player.on('dblclick', function(event){});
   C：（已完成）flash模式下，无法响应dblclick只能响应mousedown，详见 => http://stackoverflow.com/questions/1444562/javascript-onclick-event-over-flash-object
   D：（已完成）flash模式下，IE8的全屏功能无效，目前没有好的解决办法；
11.（已完成）云录播的系统设置 新增 “教学管理”、“学校”、“科目”、“年级”、“教师”、“职称”可修改名称；
   A.（已完成）教学管理 新增"职称"栏；新增"职称"数据表；
   B.（已完成）新增直播墙名称自定义字段name_tour；
   C.（已完成）优化wk_system内容，只留一份代码；
12.（已完成）完成了《直播教室-设计.docx》，目录 => E:\GitHub\HaoYiYun\Document\方案
   A：（已完成）就是一种直播课堂方案，老师在北京上课，学生通过网络投影观看学习；
   B：（已完成）开源的obs能够完全满足要求，需要改进，涉及到 VS2015|QT|cmake；在itellyou上下载vs2015安装；professional_2015_x86_x64 => F:\迅雷下载\VisualStudio
   C：（已完成）有关obs的详细编译过程，https://blog.csdn.net/gengxt2003/article/details/79070741
   D：（已完成）清华大学开源软件镜像站，https://mirrors.tuna.tsinghua.edu.cn/
13.（已完成）云录播 网站升级注释写成了 云监控 的升级注释；
14.（已完成）采集端 新增了世纪葵花的400电话；
15.（已完成）前端播放页面，新增 评论、分享、点赞功能，参考YouTube的页面设计；
   A：（已完成）评论、录像、通道，三种内容会有 赞、踩 次数；
   B：（已完成）新增 wk_zan|wk_comment 表，评论记录有父节点；
   C：（已完成）添加平路、评论回复、评论删除、回复删除；
   D：（已完成）直播没启动，评论区无法触发刷新事件，在error_page当中做特殊处理；
   E：（已完成）云监控 播放页面的录像按日期选择功能还没有完成；
16.（已完成）升级版本为1.3.3
17.（已完成）有关USB高清摄像机的信息记录：
   A：优点 => 免驱、免采集卡、模拟COM口（免云台控制线），给的软件据说是为了模拟COM口用的；
   B：缺点 => 对机器的配置要求较高，CPU占用高；
   C：云台控制标准协议 => VISCA、PELCO-D、PELCO-P，都是通过RS232串口通讯；USB可以模拟串口，也能实现云台控制，可以免去串口线；会议摄像机里面用的多；
   D：还有一种云台控制协议 PTZ，通常通过 ONVIF 协议控制，目前在海康监控摄像头里面用的多；PTZ应该是网络协议，其它的是串口通讯协议；
   E：这样，就能通过自己的软件实现全部的云台操作，所以，使用USB模式的摄像机非常有必要，完全没有必要采用HDMI+高清采集卡的方式；
   F：实物展台可以采用定焦摄像头，没有必要采用20倍变焦的摄像头；
18.（已完成）obs的源码分析文章：
   A：（已完成）第一篇 => https://cloud.tencent.com/developer/article/1004548
   B：（已完成）第二篇 => https://segmentfault.com/p/1210000009106399
   C：（已完成）第三篇 => https://blog.csdn.net/laohuangniu/article/details/70313303
19.（已完成）obs调试过程中的问题总结：
   A：需要将vsbuild\rundir\Debug\bin\32bit的内容全部拷贝到vsbuild\UI\Debug，才能调试，修改编译目录的方式无法设置调试断点；（这种方式调试起来麻烦，后面改进了）
   B：https://blog.csdn.net/laohuangniu/article/details/70313303，这篇文章讲解了test工程运行中的问题，这个工程很重要，通过它来不断改进，将obs的功能体现出来，但不用obs的界面；
   C：现在的关键是完成界面和数据采集功能；
   D：调试win-test成功，现在需要建立一个vs2015的工程，对话框的形式，建立场景、显示窗口等等，搭建雏形；
   E：调试模式需要设定特殊的模块目录，obs-windows.c:module_data:"../../data/obs-plugins/%module%"
   F：需要利用obs实验出虚拟画布功能，不要显示黑色的画布，然后，隐藏屏幕流，就能实现最初的设想；
   G：需要改变思路，不要违背obs的核心思想，顺着obs的思想走，对界面进行调整；固定数据源；不断优化使用体验；
20.（已完成）根据对obs的全新认识，顺着obs的思路，重新调整方案：
   A：（不完成）使用左侧设定一个基于桌面的画布窗口，右侧仍然是对话框形式的浮动界面，所有的操作都针对左侧画布窗口；（对obs改动太多，得不偿失）
   B：（不执行）需要解决显示器隐藏的情况下，仍然捕获桌面内容，进行压缩、上传、录像；现在的obs只要预览不显示，就不会处理数据；（这种思路不现实）
   C：（已完成）使用QT搭建界面，有利于直接将obs的界面使用到自己的软件当中；（直接在obs里面进行界面代码和内核代码调整）
   D：（已完成）画布大小就是显示器桌面大小，锁定显示器捕获，画布窗口里的元素：最底层是一个显示器捕获+老师摄像头+展台摄像头+互动学生摄像头；
   E：（已完成）第一步是需要解决当前obs编译调试的问题，而不是每次编译相关dll都需要拷贝到UI/Debug目录下；
   F：（已完成）直接在rundir/Debug/bin/32bit下运行，应该是编译目录设定有问题，造成无法设置调试断点；
       C:\Users\Jackey\AppData\Roaming\obs-studio
       Output Directory => E:\obs-studio\vsbuild\rundir\Debug\bin\32bit
       Intermediate Directory => E:\obs-studio\vsbuild\UI\obs.dir\Debug
       Debug Command => E:\obs-studio\vsbuild\rundir\Debug\bin\32bit\obs32.exe
       Working Directory => E:\obs-studio\vsbuild\rundir\Debug\bin\32bit\
       注意：调试目录与工作目录保持一致，否则dll无法加载；
       注意：obs本身在编译完成之后会执行post脚本，会把obs32bit.exe发送到vsbuild\rundir\Debug\bin\32bit，因此Output Directory可以不动，Teacher工程才需要修改；
   G：（已完成）重点研究obs-ui，使用QT重组界面，完全不用以前的代码，就是将obs界面代码进行简单重组，现在的obs编译版本全部达到了预期的需要。就是对界面进行重组。
   H：（已完成）obs的目录结构：
      bin -> 32bit -> obs32.exe     => 界面和模块编译后的动态库
      data -> libos|obs-plugins     => 模块和插件用到的数据
      obs-plugins -> 32bit -> *.dll => 插件编译后的动态库
   I：（已完成）VS2015需要安装扩展组件，新建一个QT界面的工程，obs是利用cmake外挂编译了QT模块，可以直接调试，新建的工程，必须与QT绑定，重新绑定编译才行；
      Tools => Extensions and Updates => Online => QT VS Tools
   J：（已完成）.ui是QT生成的页面配置文件，可直接编辑，编译时会自动生成ui_xxx.h文件，.qrc是资源配置文件；QT工程会新增一些配置项，会提前预编译一些文件，然后供vs2015使用；
   K：（已完成）成功将Teacher用obs的界面编译完成，现在需要进一步对界面进行调整；
   L：（已完成）重新在E:/obs-studio创建目录，从https://github.com/HaoYiTech/obs-studio上拉取数据；
   M：（已完成）需要对QT的界面设计器进行系统学习，要不然后续针对obs界面改造会非常费劲；
      https://blog.csdn.net/a10929/article/details/78114261
      https://blog.csdn.net/GoForwardToStep/article/details/53792702 => QQ风格窗口
      https://blog.csdn.net/goforwardtostep/article/details/53494800 => 自定义窗口
      https://blog.csdn.net/goforwardtostep/article/details/77825598 => 有关布局的
      https://blog.csdn.net/goforwardtostep/article/details/52085030 => 有关信号槽
      https://blog.csdn.net/zhuyingqingfen/article/details/44019915  => QT小技巧
      https://blog.csdn.net/iaccepted/article/details/24426875
      https://www.cnblogs.com/sfy5848/p/4835458.html
   N：（已完成）可以参考的obs界面，陆续增加中：
      https://github.com/Bilibili/biliobs
      https://github.com/alibaba/tblive
21.（已完成）对obs界面的修改操作：
   A：（已完成）隐藏scenesDock|transitionsDock，设置sourcesDock|mixerDock最小尺寸为(300,200)
   B：（已完成）修复预览开启与关闭菜单项，修改图标，修改标题名称；setStyleSheet("QListView::item:selected {background: #4FC3F7;}")，设置选中条背景色；
   C：（已完成）修改 默认语言环境从 en-US => zh-CN；优化右键菜单，删除一些无用的，去掉“滤镜”“交互”，保留“属性”；
   D：（已完成）修改 来源 => 数据源 - 将多种图层混合输出，混音器 - 将所有音频混合输出，通过修改UI\data\locale\zh-CN.ini => data\obs-studio\locale\zh-CN.ini 完成；
   E：（已完成）修改 添加 => 添加(源)，重命名 => 重命名(源)，移除 => 移除(源)；
   F：（已完成）修正 右键菜单 重命名 对来源名称的修改错误；SourceItemNameEdited()当中currentItem有可能不是selectedItems
   G：（已完成）去掉 自动更新升级功能；EnableAutoUpdates，去掉自动日志上传功能；UploadLog()；profile_data没有去掉，可以跟踪运行状态；
   H：（已完成）开启 状态栏，显示在窗口底部；预览窗口的缩放方式有问题，没有安装配置的方式显示，造成操作问题；fixedScaling强制锁定；
   I：（已完成）解决停靠栏右键菜单显示问题；选择CustomContextMenu项就行，构造void on_xxx_customContextMenuRequested(const QPoint &pos);会自动响应右键菜单事件；
   J：（已完成）点击关闭之前，弹框询问，不要直接关闭程序；OBSBasic::closeEvent(QCloseEvent *event) => event->ignore();
   K：（已完成）进程关闭之后，窗口界面的配置信息没有存盘 => 需要打开OBSBasic::Save()之前屏蔽的代码；SceneItem就是SourceItem；
   L：（已完成）启动两个进程时的报错处理过程，有问题，需要修正；
   M：（已完成）第一次启动，修改默认的配置目录和名称为 default ，以前是中文的“未命名”，界面主题采用 Default，不要采用 Dark；
       MakeUserDirs() => 创建大量默认的配置目录信息；
       InitGlobalConfig() => 创建 obs-studio/global.ini
       MakeUserProfileDirs() => 创建用户信息配置目录
       去掉自动向导配置功能，全部采用手动配置...
   N：（已完成）需要解决obs-studio/plugin_config里面隐藏访问的问题，跟加载模块相关，已经去掉了不常用的模块；
   O：（已完成）有四种资源类型 => obs->source_types => OBS_SOURCE_TYPE_INPUT | OBS_SOURCE_TYPE_FILTER | OBS_SOURCE_TYPE_TRANSITION | OBS_SOURCE_TYPE_SCENE
       enc-amf | obs-browser | obs-vst | libdshowcapture => 这几个扩展模块是子工程，需要单独下载，否则，不会生成工程文件；
   P：（已完成）开启数据源的 添加 | 修改 | 删除 功能；
       “添加”功能，CreateAddSourcePopupMenu()，有两个地方弹出菜单 => 预览右键 | 停靠栏点击加号；一个模块dll中可能包含一种资源类型，但可包含多种资源；
       “添加”功能，数据源数量就是扩展模块数量 => obs->input_types => obs_register_source_s(OBS_SOURCE_TYPE_INPUT) => obs_load_all_modules => obs-plugins\32bit
       “添加”功能，只保留有限的几种数据输入源 => 视频捕获设备 | 显示器捕获 | 窗口捕获 | 音频输入捕获 | 媒体源，禁用game_capture|wasapi_output_capture => obs_enable_source_type()
        删除image_source.dll(有3个数据源)，删除test_input.dll(有5个数据源)，删除text_freetype2.dll(有1个数据源)，删除obs_text.dll(有1个数据源)
   Q：（已完成）新增 网络流 数据源 => 直接使用“媒体源”的网络流就可以，内核是ffmpeg，本身就是支持rtmp|flv|hls|rtsp数据流；就可以不用费劲去加载vlc-video.dll（牵涉大量工作）
   R：（已完成）注意：工程重新编译时，会对locale资源进行重新拷贝到data目录下，需要提前做好备份工作或直接修改UI\data\locale下的zh-CN.ini文件；
   S：（已完成）新增 双击数据源预览框 全屏投影 功能 => OBSQTDisplay::mouseDoubleClickEvent() => OpenSourceMonitor()
      （已完成）修改 双击数据源预览框 移至顶部 并 拉伸至全屏功能，而不是全屏投影，因为，全屏投影并不会反应在实时的直播画面当中，只是全屏在投影预览当中 => OBSBasic::DoDisplayDbClicked()
   T：（已完成）修改 数据源预览框 的缩放方式，目前只能按等比例缩放，看看能否修改成任意比例的缩放；每一个数据源都可以配置transform变换参数，开启变换对话框界面 => OBSBasicTransform.ui
   U：（已完成）新增 按钮控制 停靠栏 => 开始直播（自动录像mp4）| 常规配置（输出窗口大小等等基本配置信息，还是在原有界面上进行简单的调整）
   V：（已完成）新增 统计信息 按钮 => statsButton 与 statsAction关联；
   W：（已完成）修改 开始推流 背景色 和 按钮高度 => setStyleSheet("QPushButton{background-color: #FFA500; height: 35px; font-size: 20px; color: black;}");
   X：（已完成）修改 ffmpeg_source 时的资源界面，OBSPropertiesView，通过资源属性来动态创建窗口对象；
       现在，需要隐藏文件模式，只留下一个QListView框，写入在线采集端摄像头列表，每一个Item就是一个rtmp链接地址，就是一个动态获取流地址的界面；
       OBSBasicProperties当中，隐藏preview对象，隐藏文件模式，始终用非文件模式；ffmpeg_source_defaults() => is_local_file => false;
       需要新建一个ListView，选择Item之后，直接找到对应的obs_property_t，赋值即可 => obs_data_set_string() => RefreshProperties()
22.（已完成）开始 edu.ihaoyi.cn 的开发设计当中：
   A：（已完成）创建新的目录 => E:\GitHub\HaoYiYun\educate，对应 monitor|recorder，创建新数据库 educate；
   B：（已完成）搭建网站雏形 => 云教室 | 手机 | 登录
   D：（已完成）云教室(直播)元素 => 房间(wk_room)，课程(wk_lesson)，讲师(wk_teacher)，直播(wk_live)
       新建 wk_room 表，用来记录云教室(直播教室|招评标室)的内容，是跟物理房间或教室对应的；
       新建 wk_grade 表，用来年级相关记录信息；放在lesson当中；
       新建 wk_subject 表，用来记录科目(项目)的总体信息；放在lesson当中；
       新建 wk_lesson 表，用来记录具体科目生成的课程的内容；
       新建 wk_teacher 表，用来记录开讲老师的信息；
       新建 wk_live 表，用来记录实际直播内容表，推流地址固定，创建直播之后会随机生成一个，实际展示的云教室记录内容；
   E：（已完成）{{$vo.start_time|strtotime|date='m-d H:i',###}}，可以连续用两个函数参数；
   F：（已完成）开始搭建播放页面，新增针对直播教室的 赞|踩|评 功能，0(camera_id)，1(record_id)，2(comment_id)，3(live_id)
   G：（已完成）为了保持与原有机制的兼容，需要对中转服务器进行改造，200000以前的都是camera，200001以后的都是live(20万做为分界线)
   H：（已完成）赞|踩|评，有关“删除”界面的显示有问题，任何已登录都可以删除评论，只有创建者才能删除评论；
   I：（已完成）开始 show 页面的核对与完善工作；后续，还要将show里的错误处理机制进行优化处理；

2018.03.01 - 2018.03.31
=========================================================================
1.（已完成）中心服务器支持“浩一云”和“浩一云服务”两个小程序的接入处理；
   A：（已完成）“浩一云”恢复成可接收回看直播数据的功能；
   B：（已完成）“浩一云服务”只提供采集端设备管理的功能；
2.（已完成）中心服务器的 MiniAction.class.php 解除绑定接口有两个调用方向：
   A：（已完成）小程序 => MiniAction.class.php => unbindGather => 需要调用中转服务器接口 => 到达采集端...
   B：（已完成）采集端 => MiniAction.class.php => unbindGather => 只进行数据库处理，不调用中转服务器，否则会引发混乱；重复开启线程引起崩溃；
   C：（已完成）采集端 => doWebUnBindMini => /wxapi.php/Mini/unbindGather/gather/1
3.（已完成）有用的矢量图标库 => http://www.iconfont.cn/
3.（已完成）css/js/php资料好 => http://www.php.cn/
4.（已完成）口袋动画PA插件 => http://www.papocket.com/
5.（已完成）解决了采集端是无线网卡无法获取MAC地址的问题；
6.（已完成）mysql修复数据表的方法：
/weike/mysql/bin/mysql -h 127.0.0.1 -u root -p haoyi
repair table wk_camera;
repair table wk_camera USE_FRM;
7.（已完成）MacBook上的“拖移锁定”通过“辅助功能”打开，就能单指双击锁定。
8.（已完成）用新的虚拟机安装一遍云录播，看看是否出现wk_camera损坏的问题。
   A：（已完成）确实存在wk_camera损坏的问题，需要更新打包软件。
   B：（已完成）数据库haoyi->wk_camera与monitor->wk_camera都损坏了。
9.（已完成）安装脚本中新增 libgomp 服务器CentOS6.8最小化安装时需要。

2018.01.31 - 2018.03.09 => 有关小程序开发、审核的记录
=========================================================================
1. 2月2日，小程序第一次审核失败，在线教育，里面放了文娱视频，需要去掉，再放上一个在线教室；
2. 这两天仔细阅读了开发社区的帖子，发现小程序的审核非常严格，目前有很多禁区，基本总结如下：
   A：类目问题：很容易撞红线，具体类目的细节根本无法解释，只能凭感觉；很多类目都需要特殊的认证材料；
   B：不能有UGC内容：指的是不能通过小程序产生UGC内容，比如：评论、通过小程序上传等等，但可以在网站后台添加，小程序刷新内容；
   C：不能有诱导分享内容：小程序展示页面不能有任何二维码，不能引导用户下载其它APP或依赖其它程序（这一点需要验证）；小程序页面涉嫌及诱导：关注，分享，星标，下载等功能
   D：与审核人员交流渠道：建议将测试信息存放在云盘，并将云盘链接及账户密码提供到小程序【版本描述】处以供审核同学参考。
   E：每次审核提交后，在一段随机时间内，会触发小程序的真机测试程序，会让一台真机（通常是安卓）联机测试提交的小程序，基本上会覆盖所有的页面和按钮，截图保存，审核人员通常都是靠这些截图来进行人工审核和判断；
   F：这种自动测试机制，跟在开发环境中申请测试的效果基本一致，只不过申请测试时会随机安排5到6台真机测试，并提供每台机器的覆盖页面截图等详细信息；审核自动测试可能会加重测试力度，截图估计会更细一些；
   G：想想也是，每天都有大量的小程序提交审核，如果每个审核员都打开手机运行一下进行测试判断，效率太低，根本就不可能，只能是看一些现成的截图，来进行判断；更不会去审核代码，那更是一个不可能的工作；
   H：所以，一定要重视每次提交前的自动测试，再结合审核禁忌，有意避开，确认无误之后再提交；必要时，可以在网站后台做一些操作，不让审核人员看到一些敏感词，等审核通过之后再将服务器代码修改回来；
   I：目前，就是为了避免让审核看到“我的通道”里面的下载采集端字样，让所有人的采集端都一样，都有通道显示，等审核通过之后再改回去；
   J：不能有“分享到朋友圈”按钮，否则，会被投诉“诱导分享”；
   K：即使“浩一云”小程序上线了，将来也极有可能被竞争对手或恶意用户投诉：与服务类目不符，造成下架风险；
   L：根据目前来看，小程序 也只能做为一个宣传的噱头，运营的风险太大，要特别注意“共享通道”的问题；
   M：小程序不能“虚拟支付”，什么是虚拟支付：这个概念随时在变，以前没有开放游戏时，游戏币就是虚拟支付，开通会员包月观看不知道算不算虚拟支付？付费购买虚拟商品属于虚拟支付。付费观看图片，涉及虚拟支付，属未开放类目内容。
   N：一个微信官方审核人员的微信号：nomorehu
3. 如果最新提交的小程序审核通过，新版的小程序需要增加的功能如下：
   A：（未完成）完善通过扫描采集端二维码绑定小程序的流程；
   B：（未完成）在live播放窗口下方，增加“弹幕”和“回放”两个标签，并可以根据服务器对通道的配置，决定是否显示，即“弹幕”和“回放”是受服务器配置控制；这是服务器产生的UGC，而不是小程序用户产生的UGC，不提供评论入口和录像入口；
   C：（未完成）为了避免“共享通道”被用户滥用，需要在wk_track当中加入一个审核字段，只有通过审核的通道才能对外共享，只能共享互联网通道，用户在“我的通道”里点击“共享”后，会有等待审核字样；
   D：（未完成）小程序管理员会收到一个模版消息，打开之后，决定是否运行共享，同时，也能在模版消息中随时关闭共享；就是将审核字段设置成0或1；在这个等待过程中，用户可以停止共享，共享记录会被删除；
   E：（未完成）“共享通道”页面，可以直接在任意通道上点击分享按钮，转发给微信好友；也可以点击右上角分享整个页面；
   F：（未完成）“我的通道”页面，可以直接在任意通道上点击分享按钮，转发给微信好友；不可以点击右上角分享整个页面，这是个人的通道管理页面；分享个人通道时，即使是局域网的通道也可以分享出去；
   G：（未完成）“关于我们”页面，增加转发功能；
   H：（未完成）“直播播放”页面，参考之前记录的 IT大咖说 截图，显示正在观看的用户数，也可以参考 微信电竞 的模式；
   I：（未完成）“直播播放”页面，视频下方的视图选用scroll-view，可以局部滚动，而不是跟随整个视频窗口滚动；
   J：（未完成）“我的通道”页面，共享按钮受服务器端采集端配置限制，wk_gather新增字段，是否开启共享功能，不开启，显示拥有者头像和名字；
   K：（未完成）“共享通道”、“我的通道”，不要显示流类型的名称，只显示图标和通道名称，不要给用户莫名的感觉；
4. 2月6日，小程序第二次审核失败，工具/图片/音频/视频 涉及文娱视频，请登记类目；
   A：因此，去掉了工具/图片/音频/视频 类目，只留下在线教育；
   B：同时，将小程序简介也进行了修改，只留下跟在线教育相关的信息：在线课堂分享，远程网络教学。
   C：经过这两次，觉得小程序这种严格审核机制，估计是难以通过审核的，因为，涉及到文娱视频；
   D：如果第三次，还是无法通过审核，那就去社区提交申诉（估计要到年后了）；
   E：如果申诉后，仍然无反应，还是给出涉及文娱视频，那估计就没戏了，小程序这条路无法走通；
   F：现在就要早做打算，对小程序无法走通的应对处理，采集端的应对（屏蔽绑定），网站的应对处理（屏蔽小程序入口）；
   G：03.14，由于“浩一云服务”审核通过，重新打开网站前端的小程序入口二维码；
5. 社交-直播类目需要的资质：《信息网络传播视听节目许可证》或《网络文化经营许可证》(经营范围含网络表演)
6. 文娱-视频类目需要的资质：《信息网络传播视听节目许可证》或《广播电视节目制作经营许可证》
7. 2月8日，如果在线教育的直播播放无法审核通过，可以考虑不带播放视频的小程序，用于管理采集端；做最小的改动；
8. 2月9日，跟预期一致，再次审核失败，始终判断目前提供的内容含直播，涉及视频，要求补充选择文娱-视频类目；
   A：因此，必须采用新的思路，去掉所有涉及直播、视频的代码，就是改造live页面，重新提交审核；
   B：赶在10日之前提交，争取在春节前再审核一次；
9. 2月11日，第4次审核失败，还是提示：目前提供的内容含直播，涉及视频，请补充选择文娱-视频类目；
   A：删除了“共享通道”、“我的通道”、“播放页面”，将“我的采集端”放到首页；
   B：变更类目为：工具 => 信息查询，再次提交审核，估计要到年后2月23日之后才会有审核结果；
   C：如果还是无法审核通过，就需要换一个小程序，继续提交，还是以公司名义提交小程序，在制作之前多看一些类型的小程序作为参考；
   D：鉴于目前对直播类小程序的严格审核，将小程序的定位从播放终端，改变成采集终端管理工具，绑定采集端、线上支付，是工具，而不是播放器；
   E：尝试申请两个小程序：云监控(无法申请)、云录播(正在名字审核)，思路都一样，把小程序当成采集终端管理工具，而不是播放终端；类目：IT科技 => 硬件设备
10. 2月23日，第5次审核失败，还是提示：目前提供的内容含直播，涉及视频，请补充选择文娱-视频类目；
   A：可见，只要被打上了烙印，怎么修改都无济于事，只能废掉，另开一个新的小程序；
   B：因此，第一次提交的小程序非常重要；
11. 3月9日，重新申请了“浩一云服务”的小程序，只提供采集端绑定业务功能，并提交了审核申请；
   A：重新创建小程序的策略成功，3月10日一早就通过审核，神速；
   B：下一步可以考虑在这个基础上逐渐新增功能，始终围绕采集端设备来思考，不要进行直播播放，可以围绕通道管理进行；

2018.02.01 - 2018.02.28
=========================================================================
1.（已完成）采集端：需要防止一台机器启动多个实例的问题，否则，用户在请求播放时，无法定位到对应的采集端；
2.（调整思路）需要在网站后台，新增“升级服务”栏目，比对当前版本与服务器版本的差异，显示升级报告，单个文件升级或整个升级；主要针对网站和数据库升级；
   A：（换思路）升级服务当中，可以新增一个查看日志的功能；
   B：（换思路）升级服务当中，可以新增一个删除缓存的功能；因为数据库或php更新后，可能会造成字段或页面显示问题；
   C：（换思路）中心服务器 新增 swoole伺服服务器，用于节点服务器交互，节点服务器也用swoole建立连接；
   D：（换思路）中心服务器 新增 /weike/htdocs/swoole.php 监听端口 20002 ，处理异步升级、小程序异步反向通知；
   E：（换思路）mysql数据库文件 => *.frm是描述了表的结构，*.MYD保存了表的数据记录，*.MYI则是表的索引
   F：（换思路）在节点网站，新增 系统升级 异步列举所有模块本地版本与服务器版本，表格形式，最下面新增一个“立即升级”的按钮；
      PHP_VERSION => php 查看版本
      /weike/mysql/bin/mysql -V => mysql 查看版本
      /weike/nginx/sbin/nginx -v => nginx 查看版本
   G：（换思路）这种方式，根本无法实现：php无法重启mysql，无法覆盖文件；
   H：（换思路）系统升级 给出操作步骤和提示，让用户通过脚本去升级数据库和网站；
   I：（换思路）这种方式，最终测试还是不行，新增字段时，表索引文件和数据记录文件都会改变，只覆盖文件会造成数据表无法打开；
   J：（换思路）swoole只适合微信小程序内网通道的观看，对于数据库升级没有任何帮助；
3.（数据库升级最终思路）采用sql语句动态升级：
   A：（已完成）先从服务器中获取表字段的详细信息，Field/Type/Default/Extra/Null/Comment
   B：（已完成）在本地数据库中查找字段是否存在，选择使用 ALTER TABLE `MODIFY` 还是 `ADD`
   C：（已完成）根据需要计算出 Field/Type/Default/Extra/Null/Comment，详见 upDbTable
   D：（已完成）如果本地没有该数据表，选择使用 Create Table 命令
4.（已完成）目前核心的三大块重要工作：
   A：（已完成）完成小程序的第一版本工作，建立起微信账户与采集端的关联关系结构；
   B：（已完成）完成API的接口设计，加入到网站后台当中，用户登录就能测试使用；
   C：（已完成）完成所有重要功能的文档说明，编制文章，有结构有调理的文章，相当于在线使用帮助；
   D：（已完成）完成网站通过后台，动态升级数据库，通过脚本升级网站代码；
   E：（已完成）现在需要根据目前的小程序界面设计，找到需要的界面组件库，完成第一步工作；
5.（已完成）获取小程序码的方法：
   A：（已完成）参考测试代码 monitor/wxapi/Lib/Action/GatherAction.class.php 里的 qrcode() 函数
   B：（已完成）先获取access_token，再获取小程序码，直接返回jpg的图片数据
6.（已完成）华工用户提出的完善功能：对于切片录像，需要一个批量下载界面；
   A：（已完成）可以在播放页面进行处理，点击下载时，弹出下载列表，可以单独下载也可以批量自动下载；
   B：（已完成）采用layui页面，参考数据库升级方式，单独或批量下载；
7.（已完成）网站后台，需要新增查看用户在线观看统计列表，用户历史观看统计的列表；
   A：（已完成）直播服务器 => 列举当前挂载的直播服务器列表，点击查看挂载通道列表；
   B：（已完成）观看用户列表 => 列举当前通道下正在观看的用户详情；
   C：（待完善）已挂载通道列表、观看用户列表的自动分页显示功能，目前是全部显示；
   D：（待完善）观看用户的微信详细信息查看；
8.（已完成）组件管理 => 中转客户端，存在没有列举完所有终端的问题，需要改进，改进成表格模式；
9.（已完成）采集端：需要增加对多屏窗口的操作按钮，例如：每隔16个窗口就自动形成分页功能；可以避免路数太多造成显示窗口太小的问题；
   A：（已完成）可以通过配置文件设置每页窗口个数，最少4个窗口，最大36个窗口，默认9个窗口；
   B：（已完成）翻页之后的焦点处理；
   C：（已完成）跳转页 功能完善；
   D：（已完成）添加/删除 通道时的处理；注意：通过网站的添加/删除操作；
   E：（已完成）左侧树形控件点击之后，需要自动进行翻页定位；
10.（已完成）网站端：采集端表新增两个字段 auto_ipc page_size，可以进行配置；
11.（已完成）网站端：屏蔽小程序扫码入口功能，等待小程序具有播放功能之后再开启；
12.（已完成）需要完成最后的工作：采集端注册中心网站后，显示小程序二维码，等待微信扫码绑定；
   A：（已完成）采集端：改进小程序码的显示流程：不要强迫用户绑定，专门有个菜单按钮弹窗，所有的小程序码都在这个窗口里面处理；
   B：（已完成）采集端：这样原来的流程都不变，还能通过中转服务器实时响应，不用破坏原来的流程；
   C：（已完成）小程序码：分解成两个步骤：通过中心站获取token，通过采集端自己获取小程序码，保存到内存，随时显示；
   D：（已完成）小程序码：在获取token时，参数是gather_id，返回：绑定用户编号、用户户头像、用户昵称、小程序响应页面、access_token、expires_in
   E：（已完成）采集端：每次点击“绑定小程序”时，才会获取token，然后获取小程序码，部分保存到GMConfig当中，最终，根据获取到的数据决定如何显示；
   F：（已完成）采集端：小程序码窗口标题高度23+3=26px，左右留白共3+3=6px；
   G：（已完成）小程序：测试扫码结果页面，可以使用开发工具的条件编译自定义参数 scene=xxxx 进行模拟；https://mp.weixin.qq.com/debug/wxadoc/dev/api/qrcode.html
   H：（已完成）小程序：单行多余字符显示省略号 => overflow: hidden; text-overflow: ellipsis; white-space: nowrap;
   I：（已完成）小程序：多行多余字符显示省略号 => https://www.cnblogs.com/hellman/p/5755376.html，有赞的小程序示例当中好像有多行的示范；
   J：（已完成）采集端：重新梳理了显示流程，都在绘制背景函数里面处理；
   K：（已完成）采集端：新增了“解除绑定”操作按钮；
   M：（已完成）小程序：用户点击“解除绑定”之后的处理 => 不需要调用接口通知采集端，因为采集端自己也会调用，可能引起混乱，需要做区分才行；
   N：（已完成）小程序：用户点击“确认绑定”之后的处理 => 跳转到“个人中心”页面；
   O：（已完成）小程序：用户点击“取消”之后的处理 => 跳转到“个人中心”页面；
   P：（已完成）小程序：wx.redirectTo 关闭当前页面，不会有回退按钮...
   Q：（已完成）小程序：wx.scanCode 在扫描带参数的小程序码时，返回具体数据还不清楚，必须等待小程序发布之后再查看；在“个人中心”的“我的采集端”也有扫码过程；
   R：（已完成）小程序：目前对 wx.scanCode 的处理：判断 path 是否有效，有效就跳转页面 => wx.redirectTo({ url: res.path })；
   S：（已完成）小程序：存在的疑问是 wx.scanCode 扫描带 scene 的结果是放到res.path里面？现在还不确定，需要等小程序发布之后测试；（path里面会带scene参数）
   T：（已完成）小程序：由于路径是 pages/bind/bind，在使用 redirectTo 跳转时需要使用 ../../

2018.01.01 - 2018.01.31
=========================================================================
1.（已完成）http://ihaoyi.cn => 未来的单独运营的节点网站，企业、机关、学校，可以租用云服务的方式，获得浩一云服务，针对它会有单独的运营小程序；小程序名：浩一云
   A：（已完成）http://ihaoyi.cn => 做为“云监控”的对外演示网站；
   B：（已完成）http://demo.myhaoyi.com => 做为“云录播”的对外演示网站；
   C：（已完成）https://myhaoyi.com => 做为中心网站和小程序的交互网站，采集端、节点服务器都需要通过中心网站进行授权；
2.（已完成）采集端的所有配置都放到myhaoyi.com当中，默认都是浩一科技，在OEM授权之后，每次启动都从myhaoyi.com读取OEM配置；
   A：（已完成）这是非核心功能，暂缓执行；如果发生总是无法写入Config.xml的情况，就需要提前实现这个功能；
   B：（已完成）仔细分析这个功能，实现起来还有点麻烦，牵涉到中心网站的授权问题；实现之后的意义也不是很大，暂时搁置；
   C：（已完成）采集端配置全部放到节点网站，对采集端、节点网站，都需要在中心服务器进行授权验证；
3.（已放弃）需要在网站后台，新增“API接口”栏目，用于API的直接测试使用，可以代替帮助文档；
   A：（已完成）直接采用的是“API接口文档”的方式来解决，最简单最方便，放在产品里，容易引起系统不稳定；
4.（已完成）在公网上会发生很多异常的情况，特别是transmit里面会发生假死的情况：
   A：（已完成）transmit目前的处理方式是 gettcpstate ，但还是会发生假死的情况；gettcpstate 不太靠谱，无法有效检测状态，超时检测最靠谱也简单；
   B：（已完成）transmit加入所有链接超时检测机制，目前主要是gather端是长链接，全部采用超时检测机制，即使不是采集端，1分钟还存在的http也应该断开；去掉对 gettcpstate 的检测，没有太大意义；
   C：（已完成）后来发现，不是超时干掉假死连接，而是ForRead错误干掉的，在过了一段时间之后，应该是系统底层发出的删除指令；
   D：（已放弃）因此，最好的办法是在网站后台，组件管理 => 中转客户端，可以手动删除列出的终端列表；
   E：（已完成）使用汇报超时机制来避免假死的问题，没有使用人工干预，手动删除的方式，有点别扭，不够自动化；
5.（已完成）微信小程序音视频解决方案 => https://cloud.tencent.com/act/event/wx-video.html
   A：播放详解 => https://cloud.tencent.com/document/product/454/12519
   B：推流详解 => https://cloud.tencent.com/document/product/454/12518
6.（已完成）2017.12.26，小程序音视频组件升级，可以直接推送和拉取rtmp协议流，大大降低手机端直播延时；
   A：（已完成）前期开放门槛较高，只开放给特定的行业；
   B：（已完成）目前微信小程序可以支持HLS、rtmp、flv，还需要进一步的确认和实验；手机上保留一篇内部采访提到支持flv直播；
   C：（已完成）常青：如果使用 live-player 标签，可以使用RTMP协议和http-flv协议进行接入，也可以使用HLS协议接入，但HLS协议需要使用微信小程序早就开放的<video>标签。
   D：（已完成）亲测，live-player标签，确实可以直接播放rtmp和flv，HLS需要用到video标签；这样使用微信小程序播放，比PC端播放兼容性还要好。
   E：（已完成）在后台新增了 教育->在线教育 类目，顺利开通了实时播放音视频流<live-player>和实时录制音视频流<live-pusher>；在添加类目时，会自动列出需要的资质，“在线教育”没有资质限制。
   F：（已完成）这下好了，手机端可以一步到位的实现低延时观看监控实时直播视频，而且还是两种选择rtmp和flv，这是可以大做文章，加紧宣传的好机会；
7.（已完成）在我的通道当中，需要新增共享操作，以便将拥有的采集端通道共享出来，具体操作是新建一条记录到wk_track当中；只有拥有者才能共享通道；
   A：（已完成）简化读取共享通道的过程 => 每个通道单独从节点调用接口，不要在中心进行优化，太过复杂；
   B：（已完成）共享通道上拉分页显示，累加当前页面，继续请求；
   C：（已完成）共享通道下拉重新获取分页数据，从第一页开始重新刷新；
   D：（已完成）开始多分页的测试当中，上拉、下拉的多分页测试；
   E：（已完成）统一快照截图的显示大小，目前有大有小的显示，通过设定参数搞定：<image mode="widthFix top"></image>
   F：（已完成）image组件的缩放、裁剪混用，样式已经设定210px，裁剪的 top center 没有差别；
   G：（已完成）image组件的lazy-load没有起作用，设定后没有看到效果；
   H：（已完成）下拉刷新时，只把数据清除，不应用到视图里，这样可以防止刷新闪烁；
   I：（已完成）快照截图地址为空时，image并不会触发binderror事件，需要在模版中预先指定snap.png的地址；在模版里加入三元操作符号 => {{item.image_fdfs?item.image_fdfs:'../../images/snap.png'}}
   J：（已完成）共享直播通道点击导航到播放页面，通过连接传递参数，不能传递对象；因此，需要将通道数据转换成json，再通过链接参数传递到直播播放页面，这样就能避免重复的数据库调用；
   K：（已完成）微信接口返回errcode|errmsg，我们的接口返回err_code|err_msg，这样以便区分；
   L：（已完成）对象合并(Object.assign)，数组合并(concat)；
   M：（已完成）直播通道播放页面，直播地址与相关录像是两个API分别请求，不要混在一起，便于管理；
   N：（已完成）直播通道播放页面，将直播切换按钮放在信息栏，避免使用button占用空间；
   O：（已完成）直播通道播放页面，上拉刷新（加载更多录像），下拉刷新（刷新直播通道，只在直播通道没有播放时才刷新，正在播放录像、正在播放直播时不用刷新）；
   P：（已完成）将error和more写入同一个模版文件，并使用了统一的模版样式，在app.wxss里面统一加载，后续会增加更多的模版；
   Q：（已完成）小程序Page扩展功能的方法 => Page(Object.assign({}, Zan, {}))
   R：（已完成）开始直播页面当中，直播回放、录像回放、直播与录像切换的实现；
8.（已完成）开始直播回放时，加入<live-player>组件，低延时直播解决方案；
   A：（已完成）mode="RTC" => 画面抖动，效果差；
   B：（已完成）<live-player>组件，全屏、开始、暂停，需要自己画出来自行处理；
   C：（已完成）<live-player>组件，播放画面在最上层，怎么加入操作界面层？腾讯视频云里面的按钮可以在视频之上；<cover-view><cover-image>能够解决这个问题；
   D：（已完成）需要加入一个时钟，每隔15秒汇报直播播放在线状态，避免推流终止问题；
   E：（已完成）<live-player>组件，没有poster功能，画面黑屏；<cover-image>
   F：（已完成）<live-player>的使用组件化，封装到模版当中；
   G：（已完成）zan-toast的优化，增加<cover-view>属性；
   H：（已完成）<cover-view>在开发工具里面支持FontAwesome，在真机当中无法显示，只能使用<cover-image>，将按钮制作成图片来使用；目前需要四张(play|pause|fullscreen|restore)
   I：（已完成）<cover-image>需要设定width和height才能显示图片，否则无法显示，<cover-view>当中无法支持FontAwesome图标字体；
   J：（已完成）<live-player>在安卓手机上无法播放，提示access denied，重新扫码下载小程序就OK了，估计是之前的小程序版本问题；
   K：（已完成）<live-player>在点击停止之后，由于是rtmp连接，srs会立即断开连接，当用户数为0时，又会触发采集端停止上传，造成小程序再也无法播放；
   L：（已完成）同时，中转服务器，已经对rtmp和hls都要进行超时检测，因此，为了更好的小程序体验，srs不要汇报rtmp用户数为0的情况，全部由中转服务器来决定是否推流；
   M：（已完成）在用wx.createLivePlayerContext创建对象时，始终只创建一个对象，详见 doAPIGetLiveAddr()
9.（已完成）<live-player>的 bindstatechange 事件会出现无法收到通知的情况，造成界面永远卡死，因为，状态都是通过这个事件来调整的。
   A：（已完成）目前的做法只是去掉了bindfullscreenchange这个全屏事件绑定，只留下 bindstatechange 事件；
   B：（已完成）这种做法没有道理，只能进一步观察会不会丢失bindstatechange事件；需要改进；
   C：（已完成）在每次重新进入live页面时会偶尔发生丢失bindstatechange事件；调试模式下发生频繁；
   D：（已完成）改进：在创建live-player成功之后，立即启动一个5秒的超时时钟，用来检测 bindstatechange 是否有效，无效直接停止播放；新增doCheckLiveState；
   E：（已完成）改进：doCheckLiveState 检测的是播放器是否收到2004状态事件，没有收到就直接关闭，这样就能完全解决这个问题；
   F：（已完成）改进：新增m_live_state_timer，记录2004状态检测时钟，点击开始重启启动时钟；点击停止关闭时钟；
   G：（已完成）改进：点击开始，获取rtmp地址，都需要重置 m_live_is_bindstate 状态，启动5秒超时检测；
   H：（已完成）改进：连接中断，重来机制，不要立即重连，等待2秒之后再整个重连，成功率高，效果不错；
10.（已完成）采集端：启动推流时，发现会阻塞主界面，有时会阻塞很久，导致推流中断，在向外网推流时更加频繁出问题；
   A：（已完成）CPushThread::IsRecording() 当中，去掉互斥；这里是主界面调用，使用互斥会阻塞主界面；
   B：（已完成）CPushThread::IsFrameTimeout() 当中，去掉互斥；这里是主界面调用，使用互斥会阻塞主界面；
11.（已完成）小程序开发工具暂不支持rtmp播放，https://developers.weixin.qq.com
12.（已完成）采集端：在win10下面无法获取MAC地址，造成无法启动；
   A：（已完成）在itellyou上下载win10，版本太多，下载了一个最老的版本，导致无法安装；又下载了一个1709多集合版本(win10的版本也是五花八门，特别多)；
   B：（已完成）DVD光盘用完了(在京东上买了50片DVD刻录盘)，只好下载了“大白菜U盘制作工具”放到U盘上做启动盘，幸好还有2个8G的U盘，删除了“酷爸”亲子课的一些资料和视频；
   C：（已完成）http://www.uqidong.asia/win10/，下载了一个专门为win10定制的U盘制作工具，“大白菜”不好使，需要将iso放到U盘里面；
   D：（已完成）这种U盘制作工具都有大量的后门，安装后会自动安装大量垃圾软件，网站上公开招募装机人员；因此，千万别用，还是用光盘安装；
   E：（已完成）win10的IE11下面，myhaoyi.com的首页两个导航按钮无法跳转；
   F：（已完成）win10下面的采集端运行时，缺少MSVCR100.dll；
   G：（已完成）win10下运行采集端完全正常，也没有激发ERROR_BUFFER_OVERFLOW错误，或许是广州用户的win10版本太老的缘故吧，将修改后的采集端发给他之后，运行正常；说明修改的方式正确，问题解决；
   H：（已完成）折腾了一圈，问题得到解决，终于有了win10的测试环境，也发现了一些问题，等DVD光盘到了，还要重新安装一遍；
   I：（已完成）win10的最新版本1709非VL版本超过4.7G，不能刻录到一张DVD上，只好下载1709的VL版本，4.66G；
   J：（已完成）rgba兼容IE8 => filter:progid:DXImageTransform.Microsoft.gradient(startColorstr=#B2FF5722,endColorstr=#B2FF5722);
   K：（已完成）始终觉得装的垃圾软件影响了win10机器的速度，因此，将itellyou上干净的win10拷贝到硬盘上，在win10上再次安装win10，重装后系统明显变快了；
   L：（已完成）打开win10上的远程，在win7上就可以远程操作win10，调试采集端在win10上的问题；
13.（已完成）采集端：在win10上无法推流，没有任何的提示，就是没进行推流，需要进一步的调试测试；
   A：（已完成）srs_librtmp 在设置接收、发送超时的问题，linux(秒)和windows(毫秒)的时间单位不同；win10下无法推流；
   B：（已完成）winxp/win7都没有严格按照设定的超时时间处理，win10是严格按设定时间判定超时，linux是30秒，windows下就是30毫秒，造成win10下很快超时，无法推流；
   C：（已完成）srs_librtmp:srs_hijack_io_set_send_timeout中加入 #ifdef _WINDOWS 开关；
   D：（已完成）将180主机上srs测试环境还原，以便将来打包使用；
14.（已完成）服务器：通常的服务器会缺少一些模块，常见的缺少组件：
   A：yum -y install gd
   B：yum -y install perl-DBI
   C：将安装更新放到了 install_monitor.sh 和 install_recorder.sh 当中。
15.（已完成）新增 通道配置可以设置rtsp数据流的连接模式，TCP模式或UDP模式，默认UDP模式；
   A：（已完成）网站后台新增rtsp数据流的TCP模式开关，摄像头和流转发分开配置；
   B：（已完成）网站数据库（云录播、云监控）通道表新增use_tcp字段；
   C：（已完成）采集端注册时获取通道配置里的use_tcp内容；
   D：（已完成）采集端获取网站远程配置的use_tcp内容；
   E：（已完成）采集端修改通道时，新增有关rtsp的use_tcp配置，摄像头修改和流转发修改；
   F：（已完成）采集端添加通道时，新增有关rtsp的use_tcp配置，流转发添加；
16.（已完成）小程序：开始“我的通道”的搭建
   A：（已完成）最上面的导航栏，当有两个采集端时才显示；
   B：（已完成）调用API获取用户拥有的采集端列表，只获取指定用户的WAN节点的采集端；
   C：（已完成）采集端下面的通道列表显示与“共享通道”的显示方式保持一致；
   D：（已完成）节点网站的wk_camera当中已经添加了shared共享标志字段；
   E：（已完成）“共享通道”和“我的通道”为空时的显示提示优化；
   F：（已完成）完善“我的通道”的显示细节；
   G：（已完成）“我的通道”上拉翻页 => 加载更多分页内容；s
   H：（已完成）“我的通道”下拉刷新 => 重新加载第一页内容；
   I：（已完成）“我的通道”切换采集端 => 重新加载新的采集端通道列表；
   J：（已完成）“我的通道”修正了没有通道时的样式错位问题；
   K：（已完成）“我的通道”点击通道时的事件响应处理完成；
   L：（已完成）“共享通道”修正了没有通道时的样式错位问题；
   M：（已完成）“共享通道”点击通道时加入了验证机制，避免访问已经停止共享的通道；
   N：（已完成）改进“下拉刷新”，是刷新整个采集端列表，而不只是刷新当前采集端的通道，因为，如果新增了绑定采集端，下拉刷新无法察觉；
17.（已完成）小程序：开始“个人中心”的搭建
   A：（已完成）完成基础界面和功能的布置；
   B：（已完成）微信登录返回的headimgurl，有可能是/0或/132，不是固定的；
   C：（已完成）“我的采集端”、“推荐给朋友”、“意见反馈”、“关于我们”全部完成；
18.（已完成）云录播、云监控的播放页面，左上角需要加上一个下载按钮，直接将MP4文件下载到本地；可以增加一个配置开关（不用登录也能下载）
19.（不执行）云录播、云监控的API需要增加一个功能：输入 rtmp/flvjs/hls 的地址，返回播放连接的功能；
   A：这么做完全没有意义，因为，并不能直到这个连接是可以播放的，完全是为了满足没有意义的需要；
20.（不执行）采集端：改进 Camera_List 的处理方式 => 如果发现通道上的用户数大于0，但是通道已经停止推流了，需要自动再次推流；
   A：（不执行）这种方式，增加了太多不可控的自动化操作，会带来更多的不确定性；用户会通过重新刷新去解决重连的问题；
21.（已完成）采集端：根据Config.xml的配置，可以打印详细的调试日志到Logger.txt当中，便于调试release版本的问题；
   A：直接在需要打印错误的地方用MsgLog处理，代替代码里面的 TRACE 调试信息；
22.（已完成）小程序获取用户登录时的unionid的方法：
   A：（已完成）http://blog.csdn.net/qq_38316918/article/details/78343128 和 http://www.jianshu.com/p/bb1ed9512dd1
   B：（已完成）具体应用过程写在了MiniAction.class.php当中；
24.（已完成）采集端绑定小程序过程中，中转服务器需要新增一个交互命令 kCmd_Gather_Bind_Mini，这个命令有三个子命令：
   A：（已完成）扫码成功 => 1 => Scan
   B：（已完成）确认绑定 => 2 => Save
   C：（已完成）取消绑定 => 3 => Cancel
   D：（已完成）采集端：还是要通过状态来控制显示细节，分为3种状态：kMiniToken | kMiniCode | kMiniHead；
25.（已完成）为提交发布小程序做准备：
   A：（已完成）整理一下本地的代码，提交到代码库；
   B：（已完成）在整理小程序代码时，发现：直播通道的相关录像有问题，逻辑出现混乱，估计是修改通道编号有关；
   C：（已完成）发现在摄像头录制的视频无法在IOS小程序当中播放；摄像头直播没有问题；安卓端也没有问题；
   D：（已完成）虚惊一场，将IOS机器重启之后，就能播放录像了；
   E：（已完成）在小程序的设置中新增在线教育的信息；
   F：（已完成）解决了“我的通道”页面，点击“共享”造成点击穿透的问题，需要改成catchtap事件；
   G：（需注意）目前的wk_track里面共享者是“浩一”，编号为2，采集端绑定者是1，可能会有问题，需要注意；
   H：（需注意）目前的“我的通道”，在共享时，会在节点里面的wk_camera做shared标记，在中心wk_track当中建立记录；
   I：（需注意）目前的“我的通道”，在读取时，直接读取节点的wk_camera，只根据shared标志来判断是否共享，不会读取中心服务器的wk_track记录；
   J：（需注意）目前的“共享通道”，在读取时，直接读取中心的wk_track记录，不会读取节点里面的shared信息；
   K：（已完成）微信小程序开发工具提供的测试功能，相当不错，提交报告后很快就得到了反馈，非常清晰；
   L：（已完成）有机会可以根据小程序的开发经验，投入到开发小游戏的工作当中，非常适合个人或团队开发；
26.（已完成）为了让小程序的共享通道有在线的直播数据，需要加上两个永远在线的直播通道：
   A：（不处理）在中心服务器，新增2个通道，一个监控视频（养猫），一个文件视频，利用ffmpeg循环直播，占用通道1和通道2，
   B：（已完成）如果要实现上面的功能，需要做大量的特殊处理；有可能会破坏整个逻辑，特别麻烦；直接在本地开启采集端就可以了；
   C：（已完成）新的思路：在小程序接口端，专门针对通道1和2做特殊处理，不向中转服务器要地址，而是直接给地址，就能解决在线问题；
   D：（已完成）新的思路：在公网上专门使用ffmpeg一直推流，启动2个进程；需要写两个.sh脚本
   E：（已完成）新的思路：有三种方法可以将ffmpeg在后台运行 nohup/setsid/&，最终选用setsid
   F：（已完成）新的思路：通道1和通道2的脚本如下，可以保持不间断永久推流；https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/
for((;;)); do \
  ./ffmpeg -re -i ./Silicon.Valley.S04E10.360x200x50.mp4 \
  -vcodec copy -acodec copy \
  -f flv -y rtmp://127.0.0.1/live/live1; \
  sleep 1;
done
for((;;)); do \
  ./ffmpeg -re -i ./sample.360x200x50.mp4 \
  -vcodec copy -acodec copy \
  -f flv -y rtmp://127.0.0.1/live/live2; \
  sleep 1;
done
27.（已完成）云录播、云监控的网站端需要新增一个小程序入口二维码，便于用户快速到达；
28.（已完成）直播服务器srs，开启默认配置，rtmp+flv+hls，开启gop，OBS推流，vlc观看，延时达到7~8秒左右；
   A：（已完成）将srs配置成低延时模式，延时为：2~3秒左右，根据文档，开启全部的低延时配置；
   B：（已完成）只是将gop关闭，延时为：2~3秒左右，说明是快显模式造成的延时；
29.（已完成）编写一个flvjs页面，可以多次加载同一个通道；
   A：（已完成）修正API里面play_camera和play_record里面的地址错误，并上传到了云监控和云录播；
   B：（已完成）使用API的方式去编写这个测试页面，使用一个静态页面，添加多个iframe的方式；
   C：（已完成）E:\GitHub\HaoYiYun\Document\小程序\浩一云\more.html，存放在这里；
   D：（已完成）云录播、云监控模版页面当中，有一个play_frame.htm没有用；
   E：（已完成）修改show.htm，可以关闭flvjs的支持；delete arrTech[0]; delete arrSource[0];
   F：（已完成）目前测试的结果如下：
       flvjs => chrome(能开6个不同窗口，相同窗口无法播放) => ie10(能开12个不同窗口，可以相同也可以不同) 
       flash => 没有开窗口限制，但是cpu占用率太高，画面卡顿厉害；
       hls   => 没有开窗口限制，只要内存够，画面流畅，cpu占用率也不高；
30.（已完成）小程序当中还有一些小问题，需要优化：
   A：（已完成）直播播放页面，有关2004通知的处理，看看能否用其它方法证明已经开始正常播放了，而不用一定要等到2004通知的到达，容易造成误判；
   B：（已完成）经过反复测试，发现：无论怎么调整，都有可能发生直播事件没有的情况下，就开始播放了，因此，状态检测机制没有问题，2003|2004；
   C：（已完成）做了细微调整：将检测时间修正为2秒，到期之后，不是停止通道，而只是关闭快照显示，认为正常播放；
   D：（已完成）2003|2004状态没来，不代表播放失败，播放失败是由 onLiveStateChange 专门处理，而不要主动判定；
   E：（已完成）同时，这样做，相当于延时2秒才显示画面，避免直接显示画面的黑屏问题，在停止之后，再启动过程比较明显；重新刷新页面效果不大，还是有黑屏问题；
31.（已完成）小程序：微信用户登录注册时，需要获取wx.getSystemInfo信息，保存用户对应的手机信息，以便将来统计使用；中心数据库的wk_user表，新增字段如下：
   A：wx_brand	      => 手机品牌
   B：wx_model        => 手机型号
   C：wx_version      => 微信版本号
   D：wx_system       => 操作系统版本
   E：wx_platform     => 客户端平台
   F：wx_SDKVersion   => 客户端基础库版本
   G：wx_pixelRatio   => 设备像素比
   H：wx_screenWidth  => 屏幕宽度
   I：wx_screenHeight => 屏幕高度
   J：wx_fontSizeSetting => 用户字体大小设置
32.（不处理）小程序：用户登录注册成功之后，将用户编号存入本地，7天之后过期，才需要重新登录，调用default页面；
   A：（不处理）将用户编号写入本地存储；
   B：（不处理）app.js当中读取本地存储，是否有用户编号，以及过期时间，没有，则跳转default页面；
   C：（已完成）目前采用的每次登录都需要通知中心服务器，这种方式比较保险，如果存放数据在本地，会带来很多其它意外情况，造成不稳定，目前暂时用这种看上去比较笨的办法；
33.（已完成）小程序：解决了反复刷新有可能造成等待框无法退出的问题；在尽可能多的地方加入 wx.hideLoading() 隐藏等待框；不要使用mask功能，可能会造成其它问题；
34.（已完成）发现 ihaoyi.cn 有人利用工具恶意登录，需要解决这个问题：
   A：（已完成）是微信提供的测试工具，登录之后的记录痕迹；
   B：（已完成）目前暂时不要从代码中做特殊处理，这种特殊处理，因为不直到微信测试工具的规则，有可能发生误判，造成其它意料之外的问题；
   C：（已完成）目前的处理办法是，直接从数据库当中删除即可；
35.（已完成）华为手机观看直播时，当通道不在线，显示提示信息时，安卓是一个错误的红叉；
   A：（已完成）安卓版微信小程序内部缓存的问题，重新下载之后就正常了；
36.（不处理）小程序：播放视频页面，自动根据手机旋转状态，进行全屏或恢复显示：
   A：（已完成）直播页面：onLoad当中自动开始监听加速度数据，在这个事件里面处理旋转；
   B：（已完成）直播页面：设定四种状态：竖屏、左旋转、倒置、右选择；
   C：（已完成）最终，测试效果不佳，容易与已有的点击全屏发生冲突，还是将代码屏蔽掉了；
37.（已完成）小程序只能跟 myhaoyi.com 通讯；小程序的所有功能都是围绕管理和统计进行；
38.（已完成）小程序：可以管理采集端、摄像头、网站等等信息，简洁大方，快速上线，需要在 myhaoyi.com 上编写中转服务器 wxsmit（使用swoole），进行命令中转；
39.（已完成）小程序：小程序的支持路径需要想清楚，所有的数据都必须通过 https://myhaoyi.com 中转，需要在 myhaoyi.com 上安装 wxsmit（使用swoole），用于小程序命令中转；
40.（不处理）SRS服务器没有超时检测机制，需要在socket上加入 SO_KEEPALIVE，一旦发生异常，Linux系统会自动清理断开连接，从而通知 SRS，使连接中断；
   A：（不处理）需要找到建立 socket 的代码，然后加入 SO_KEEPALIVE 选项；
   B：（已完成）在 transmit 也要可以考虑加入这个选项，但是，transmit 当中已经加入了超时检测汇报机制，可以暂时不要加；

2017.11.28 - 2017.12.31
=========================================================================
1.（已完成）微信小程序的设计思路准备：
   A：（已完成）小程序是一个管理工具，管理与微信用户绑定的采集端和通道；
   B：（已完成）小程序分为三个导航栏：共享通道 | 我的通道 | 个人中心，个人中心，有采集端管理，管理与该微信用户绑定的采集端；
   C：（已完成）我的通道列举所有的采集端挂接的通道，可以控制通道的启动、停止、预览、共享|关闭，开启共享，会在myhaoyi.com当中新增一条track记录，并修改通道本地数据库状态，关闭则删除记录，恢复状态；
   D：（已完成）在myhaoyi.com的haoyi数据库中新增wk_track表，记录已共享通道；在节点数据库里的wk_camera表新增shared字段，记录通道共享状态；
   E：（已完成）在Gather注册到myhaoyi.com或节点用户登录到节点后台时，都会检测节点记录是否存在，都有可能新增节点记录，保证节点记录的创建；
   F：（已完成）在采集端登录到myhaoyi.com上之后，需要返回采集端所在节点编号，以便采集端本地使用；
   G：（已完成）将小程序“浩一云”与微信开放平台的网站应用“浩一云”关联起来；并修改了各个数据库里的注释wk_user的wx_openid_app
   H：（已完成）https://mp.weixin.qq.com，开始下载开发工具，进行尝试开发；跟vue很像，只是换了一种形式；
   I：（已完成）开发版：在开发环境中点击“预览”，扫码体验，开发版会有调试信息和性能信息；
                体验版：先在开发环境上传代码，登录小程序管理后台，开发管理->开发版本->选为体验版本，扫码体验，只有调试信息，没有性能信息；
                小程序开发助手：专门管理当前帐号关联的正在开发中的小程序，能够显示开发版、体验版信息；
   J：（已完成）在草稿纸上完成了小程序的页面设计稿，下面进入实际的开发过程当中；
   K：（已完成）小程序的文档非常全面，教程、框架、组件、API、工具，非常完备；
2.（已完成）微信小程序的具体实现过程记录：
   A：（已完成）通览小程序的官方文档，做一些必要的记录；
   B：（已完成）搭建“浩一云”基础框架；
   C：（已完成）后期需要进一步完善“浩一云”小程序，使之成为对外宣传的便捷工具或通道；
   D：（已完成）创建小程序之后，最大的作用是让‘浩一云’有了社交属性，让云录播、云监控使用起来更方便；
3.（不处理）云录播、云监控，移动端Mobile界面自适应的问题：（留待以后重构升级再来处理）
   A：（不处理）之前参考的AmUI是可以自适应的，但是，在转移到移动端时，没有做到自适应，而是采用了固定的高度，需要调整一下；
   B：（已完成）https://www.zhihu.com/question/20543196，比较全面的垂直居中方案，最终采用transform方案；
   C：（已完成）由于新增了一个.am-gallery-box样式，固定了高宽，需要去掉高宽，去掉flex；在子元素中需要处理垂直剧中，使用transform样式；
   D：（不处理）如果采用了自适应模式，又出现两个问题：因此，暂不处理，留待以后重构升级再处理；
       1、页面会出现跳跃自适应问题，不是一步到位的显示；
       2、主页面高度发生变化，造成上拉刷新加载更多出现偏移；
4.（已完成）小程序第一版本的第一个页面的搭建：
   A：（已完成）菜单默认：rgb(102,102,102)#666，选中：rgb(252,55,140)#fc378c，修正：rgb(0,153,233)#0099e9
   B：（已完成）每次打开小程序都必须确认用户的身份，默认打开default页面，专门等待身份的获取，然后再跳转页面；
   C：（已完成）首先需要完成default页面的整个设计和体验，加入fontawsome，实现等待过程；
   D：（已完成）发现有些地方不能用$map做为数组变量进行查询，否则，会查询失败；LoginAction.class.php:doWechatAuth()
   E：（已完成）wx.getSetting | wx.openSetting 注：设置界面只会出现小程序已经向用户请求过的权限。
5.（已完成）为了保证用户体验，需要规范演示视频，视频大小和码流控制如下：
   A：（已完成）视频大小：640x360，码流：600kbps
   B：（已完成）视频大小：480x270，码流：500kbps
   C：（已完成）视频大小：360x200，码流：400kbps
   D：（已完成）每隔50帧2秒一个关键帧 => ffmpeg.exe -i 1280x720.mp4 -s 640x360 -keyint_min 50 -g 50 -sc_threshold 0 -f mp4 640x360x50.mp4
   E：（已完成）这样能做到直播秒开的效果，每隔50帧2秒一个关键帧；
   F：（已完成）宽x高x关键帧间隔.mp4，放在 F:/MP4 目录下；
6.（已完成）放开切片限制，云监控、云录播都能进行录像切片，以前只能是云监控模式下切片；
7.（已完成）将客服电话整合到数据库当中，目前是硬编码；
   A：（已完成）在wk_system当中新增字段web_phone，默认值15010119735；
   B：（已完成）在管理后台加入客服电话配置选项；放在默认的“系统设置”栏当中；
   C：（已完成）云录播、云监控同步更新，build目录下的数据库同步更新；
8.（没解决）直播管理，录像计划表，有时候第一次无法打开，需要刷新一下才行；
   A：（没解决）是由于加载缓慢造成，尝试加入加载等待过程；
   B：（已完成）原因是录像任务加载模块太多，造成第一次加载比较慢，目前没有好的办法解决；
9.（已完成）需要在公网的节点服务器上测试，https节点是否能够全部跑通：
   A：（已完成）http://demo.myhaoyi.com => 云录播 演示站点，全部使用http协议；
   B：（已完成）http://ihaoyi.cn => 云监控 演示站点，全部使用http协议；
   C：（已完成）节点网站不能用https模式，存在https与http混用问题，会造成flvjs与hls的直播无法观看，只能用rtmp观看；因此，节点网站都用http协议，与srs的http-flv和http-m3u8兼容；
   D：（已完成）手机端微信、浏览器、iOS、安卓，都可以https与http混用，https的页面，http-hls能够正常播放； 
   E：（已完成）https://myhaoyi.com，全站都只支持https模式，因为，需要兼容微信小程序的API调用，将所有的业务逻辑都转移到节点网站上，通过php这个粘合剂来完成数据交互；
10.（已完成）发现一个很大的Bug：通道截图没有删除旧的图片，造成大量图片积压，正常的逻辑是只留一张截图；
   A：（已完成）也有可能是11.23正在调试直播通道动态截图功能，没有完善的缘故，需要进一步的测试观察；
   B：（已完成）目前没有发现删除的问，加入了日志代码，如果发生删除失败，日志写入网站根目录的 logwechat.txt 当中；
11.（已完成）PC播放器，第一次直播时，总是会发生flvjs无法正常播放的问题，需要跟踪一下到底是什么原因造成的，是由于采集端没有及时上传通道的原因，还是videojs播放器自身的原因；
   A：（已完成）不仅是直播，点播时也会发生第一次无法加载的问题；显示 video.min.js:18 VIDEOJS: WARN: Player "my-video" is already initialised. Options will not be applied. [techName] > Html5
   B：（已完成）在使用video标签创建videojs对象之前，需要先调用接口videojs.getPlayers()，查看是否有videojs对象存在，有的话需要先删除；
   C：（已完成）dispose()会删除所有创建的标签，因此，需要重建video标签，设置id和className，再追加到div当中；
   D：（已完成）经过这样处理之后，再用video标签创建videojs对象就是全新的了，不会报错了；
   E：（已完成）这个问题，完全可以写一篇videojs的使用经验文章，还有结合flvjs、vue的使用，完全可以写三篇文章；
   F：（已完成）还是有个问题：由于加载模块太多，也比较大，第一次加载时，非常缓慢，停顿，需要找到一种读取js文件加载进度的方法，加强用户体验；
   G：（已完成）为了解决上面的问题，需要将script引用都放到head里面，不能放到body里，放在head中的JS代码会在页面加载完成之前就读取，而放在body中的JS代码，会在整个页面加载完成之后读取。
   H：（已完成）这时的体验就会好很多，royalslider会有旋转等待出现rsVideoActive样式，至于video的poster加上也没有意义，因为，它需要页面加载完毕才能显示，不需要设置了；
12.（已完成）将服务器所有模块打包到一个安装包当中，云录播和云监控分别打包，采集端只有一个；
   A：（已完成）对服务器打包工程进行了重组，产生两个文件cloud-monitor.tar.gz和cloud-recorder.tar.gz，放到百度云盘，并建立分享目录；
   B：（已完成）目前只针对一体机进行推广和使用，后续才进行分布式的更新，需要用到myhaoyi.com上建立download目录，配合haoyi.sh进行；
   C：（已完成）将采集端打包，生成 cloud-gather.exe，放到百度云盘上，并建立分享目录；
   D：（已完成）release模式下，ffmpeg报错，需要使用"保留未引用数据(/OPT:NOREF)"选项才可以正常运行；
   E：（已完成）ffmpeg的优化方案 => http://blog.csdn.net/dancing_night/article/details/53009350
   F：（已完成）将采集端默认的连接地址设置为 => http://www.ihaoyi.cn，云监控模式；
   H：（已完成）https://pan.baidu.com/s/1hsmxT0S => cloud-gather.exe
   I：（已完成）https://pan.baidu.com/s/1o8zISmY => cloud-monitor.tar.gz
   J：（已完成）https://pan.baidu.com/s/1bo9Y2Bl => cloud-recorder.tar.gz
   K：（已完成）修改中心网站的数据库密码，代码里还是保持原来的不变；
   L：（已完成）编写《云录播使用手册》
   M：（已完成）https://smallpdf.com/cn/word-to-pdf，一个非常专一的pdf处理网站；
   N：（已完成）修改了config.sh配置脚本，支持自动获取本机IP进行配置（通过auto参数）
13.（已完成）云录播、云监控当中，加入版本信息，版本信息，写在php代码当中，采集端登录时读取；
   A：（已完成）采集端有自己的版本体系，向中心注册时，汇报了采集端的版本信息；
   B：（已完成）节点网站需要加入版本信息，向中心汇报时，写入中心的wk_node当中的node_ver字段；
   C：（已完成）采集端登录节点时需要获取节点的版本信息，展现在关于对话框当中，也要展示自己的版本；
   D：（已完成）节点网站的版本信息是写在 wxapi/Conf/config.php 当中；
   E：（已完成）登录后台后，将“浩一云”替换成文字模式，现在是图片模式，这样就可以去掉default-90.png这个文件了；
14.（已完成）采集端，录像模块，计数时间戳有偏差，不能用输入的时间作为计算标准，要用从文件中读取的时间计算；
   A：（已完成）得到文件的总刻度数，不是毫秒数 => MP4GetDuration()
   B：（已完成）得到文件的每秒刻度数 => MP4GetTimeScale()
   C：（已完成）计算文件的总秒数 => MP4GetDuration() / MP4GetTimeScale()
   D：（已完成）经测试，计算的总秒数与实际存盘文件时间一致，使用这个时间比较精确；文件模式,在持续录像中，在发生循环时会出现时间戳问题；
   E：（已完成）发现文件模式下，循环累加的总时间不是毫秒时间，而是刻度时间，这就解释了为什么文件循环衔接时，终端会停止的问题，因为时间戳出错了；
   F：（已完成）在doMP4ParseAV需要计算总毫秒时间，而不是用总刻度数去处理循环，ReadOneFrameFromMP4中需要将刻度数转换成毫秒时间，进行比较、循环、处理等等操作；
   G：（已完成）感觉循环文件打开速度还加快了，或许是时间戳没有偏差的缘故？
15.（已完成）采集端，需要增加一个重连按钮，这样可以不用退出再次连接；
   A：（已完成）新增“断开重连”按钮，放在工具图标栏当中；
   B：（已完成）修正了重连状态，正在连接中时不能重连，连接失败或连接成功可以重连；
   C：（已完成）修正了连接时的一些文字描述信息；
16.（已完成）采集端，系统设置 => 网站地址和端口，写入配置文件当中，可以从历史记录当中获取；默认写入ihaoyi.cn和demo.myhaoyi.com这两个演示网站；
   A：（已完成）不要使用HistoryComboBox的简单方式，采用自己保存网站地址列表到Config.xml的方式；
   B：（已完成）新建Config.xml时，默认写入http://ihaoyi.cn和http://demo.myhaoyi.com；
   C：（已完成）所有输入的有效的网站地址，都会被记录到Config.xml当中，相同地址只存放一份；
17.（已完成）采集端，摄像头通道，突然断开时，通道并没有立即反馈状态，还是直播中；
   A：（已完成）采用的是超时检测的方式，3分钟仍然没有数据，就认为超时，需要停止通道；
   B：（已完成）以前的方式，对摄像头设备没有检测超时，故意阻拦，现在做了修正；
   C：（已完成）尝试加入了海康设备的异常处理回调，但是意义不大，屏蔽掉了；
   D：（已完成）超时检测是通过一个时钟，不断获取接收码流来判断的；
   E：（已完成）现在的摄像头设备，启动后，也会主动拉取一路视频流，跟流转发一样的处理；
   F：（已完成）摄像头设备也可以关闭预览，不影响推流，但是，无法进行云台操作了；就需要使用onvif协议；
18.（已完成）摄像头设备，新增一个开关，是否开启画面预览；
   A：（已完成）开启画面预览，不能进行云台操作；
   B：（已完成）需要在wk_camera中新增一个显示预览的字段，device_show，默认1开启；
   C：（已完成）这样做的目的是为了节省内网带宽，预览画面也要拉取一路视频流，还要进行本地解码，造成资源浪费；
   D：（已完成）在RenderWnd当中需要调用 IsDeviceStatus() 是否需要绘制特殊状态，在没有登录成功之前都需要绘制；
19.（已完成）解决了mysql-5.5.3命令行停止时报告 unknown variable 'character-set-server=utf8' 的问题；
   A：（已完成）是由于5.5.3自带的libmysqlclient.so.16.0.0/libmysqlclient_r.so.16.0.0的问题，将它们降级为5.1.73就解决了；
   B：（已完成）这两个库是放在php的lib当中的，php/mysql都会使用到；同步更新了；
   C：（已完成）也有可能是配置的问题，但是找了半天也没有解决，下载了mysql-5.7.20版本做为备用；编译方法差别很大，没有尝试成功；最好用官方提供的二进制文件；
20.（已完成）在关于对话框中，对授权过期的信息显示进行优化处理；
   A：（已完成）在中心网站，无论授权是否有效，都要计算最大通道数、剩余天数、到期时间，便于向用户展示；
   B：（已完成）采集端，关于对话框，只有一种显示形式，剩余天数可以显示负数；
21.（已完成）采集端，授权过期之后，需要有一个可以点击的连接地址，让用户直到下一步该怎么办，最简单的方案是连接到https://myhaoyi.com当中；
   A：（未处理）在CMidView当中加入可引导的链接窗口，太过麻烦，暂不处理，后续有空再调整；
   B：（已完成）将 https://www.myhaoyi.com 全部统一到一个地方配置；
   C：（已完成）关于对话框中，对于没有授权成功的状态，做了特殊处理；
22.（已完成）rtsp协议，某些连接的数据，会出现一直不能初始化的问题；
   A：（已完成）测试连接地址 => rtsp://184.72.239.149/vod/mp4:BigBuckBunny_115k.mov
   B：（已完成）有数据到达，但一直没有处理，也没有修改状态，估计是后来改的逻辑有问题；
   C：（已完成）是由于数据区里面没有再次发送sps和pps的信息，造成无法启动初始化；
   D：（已完成）这是之前将初始化过程转移到了，当在数据区又找到了sps和pps时才处理，为了应对当时有些平台在协议部分给的sps和pps是错误的情况；
   E：（已完成）现在又要改回去？改回去之后，是否可以在数据区也保留再次发送AVC的格式信息的数据包？告诉服务器格式都可能有变化？
23.（已完成）对srs的编译日志开关做了研究，如下：
   A：（已完成）./configure -h => 可以查看所有的编译开关；
   B：（已完成）--log-verbose 对应 SRS_AUTO_VERBOSE 在 auto_headers.sh 当中定义；
   C：（已完成）srs.log当中可以看到编译进二进制的开关是哪些，默认使用 --log-trace，只有trace函数有效，其它都为空；
   D：（已完成）因此，这种设计，无论怎么开编译后的配置，都不能打印除trace之外的其它日志，必须重新编译配置日志才能有效；
24.（已完成）只有视频的数据流，flvjs无法播放，因此，需要在后台加入一个开关，是否屏蔽flvjs的回放功能；
   A：（不执行）需要在wk_system表中新增一个字段flvjs，控制PC端直播播放时是否可以加载flvjs，无法播放只有视频数据的内容；
   B：（不执行）show.htm当中去验证这个开关，一旦关闭，直接删除对flvjs的支持，而不用再根据浏览器的支持情况再决定是否删除flvjs的支持；
   C：（已完成）尝试通过修改flvjs代码的方式，或者，通过修改采集端推流的MetaData的方式来提前告知有没有音频的方式去处理；
   D：（已完成）在采集端 WriteMetadata 做了严格的设定之后，但是，由于 srs 会缓存通道数据，包括头信息，而flvjs不仅会从metadata当中获取格式，还会从数据流获取格式；
   E：（已完成）这两种因素结合在一起，就会造成只有视频的通道，在通道数据切换时，造成始终等待，从而无法播放；
   F：（已完成）srs的代码大量相互牵连，太复杂，只能从 flvjs 入手，简单修改 flvjs 的代码，让flvjs完全听从metadata的音视频标志进行处理，即使有数据也要扔掉；
   G：（已完成）npm 下载flvjs代码，修改代码，重新编译，打包；以metadata的标志为优先，而不是人为的设定，metadata的原始数据来自推流端，因此，推流端要先判断清楚；
   H：（已完成）flv-demuxer.js:_parseAudioData:446 追加代码如下：
        // 2017.12.16 - by jackey => first check use metadata value...

        let onMetaData = this._metadata.onMetaData;

        if ((typeof onMetaData.hasAudio === 'boolean') && (!onMetaData.hasAudio)) {

            Log.v(this.TAG, '_parseAudioData: No Audio in metadata');

            return;

        }
      
   I：（已完成）flv-demuxer.js:_parseVideoData:806 追加代码如下：
        // 2017.12.16 - by jackey => first check use metadata value...

        let onMetaData = this._metadata.onMetaData;

        if ((typeof onMetaData.hasVideo === 'boolean') && (!onMetaData.hasVideo)) {

            Log.v(this.TAG, '_parseVideoData: No Video in metadata');

            return;

        }

   J：（已完成）在编译flvjs时，遇到一些问题，有关 gulp 的；gulp 不仅要在 全局安装，还要在本地安装，编译时缺少的库都需要逐个安装才行；
   K：（已完成）这样的修改就不用在数据库当中添加字段，也不用修改播放器代码，而是在flvjs中修改音视频的有效性依据metadata就可以了；
   L：（已完成）flvjs的源码安装在 F:\Vue\flvjs\ 当中，具体编译位置 F:\Vue\flvjs\node_modules\flv.js
25.（已完成）中转端，某通道上的播放器为0时，转发命令给采集端，会因为网络原因，无法到达采集端，造成采集端一直在上传已经停止的通道；
   A：（已完成）跟transmit造成采集端假死的情况，综合起来考虑，应该让采集端每隔0.5分钟，查询正在上传的直播通道的用户数；
   B：（已完成）transmit可以根据采集端是否汇报，来判断超时，采集端又可以根据transmit反馈来确认正在上传的通道是否还要继续上传；
   C：（已完成）transmit会检测采集端连接如果在1分钟都没有汇报，则判定为超时，需要删除之；
   D：（已完成）transmit检测到超时之后，不仅要删除对象，还要去掉epoll当中的注册，关闭套接字；
   E：（已完成）同步更新到外网服务器进行同步测试；
   F：（已完成）mysql的链接库还原成 5.5.3-m3 版本，避免使用 5.1.73 的版本造成潜在的不可预知的风险；
   G：（已完成）mysql同步修改my.cnf里的 [client] default-character-set = utf8
26.（已完成）开始建立API接口规范；
   A：（已完成）在nginx的配置当中新增接口重定向 location /api/v1 { rewrite ^/api/v1(.*)$ /wxapi.php/API$1; }
   B：（已完成）接口参数输入有两种形式，如下所示：
       /api/v1/method/param1/data1/param2/data2
       /api/v1/method?param1=data1&param2=data2
   C：（已完成）严格区分大小写，接口、名称，全部都用小写；返回数据统一用json，形式相同；
27.（已完成）有关API接口的使用注意事项说明：
   A：（已完成）会话的有效性是通过token里面的时间进行判断的，不会使用cookie，避免与云监控、云录播本身的登录体系混乱；
   B：（已完成）这种方式也是比较安全和保险的机制，完全遵循API的规范，每次接口调用都要判断token里面的时间有效性；
28.（已完成）接口列表说明，功能、接口、参数、返回值等等信息：
   A：（已完成）编写接口文档，实现接口调用的逻辑（APIAction.class.php）
   B：（已完成）在后台界面中加入显示能够调用API的unionid信息，需要经过base64处理；
   C：（已完成）同步更新代码到云监控、云录播系统当中；
   D：（已完成）编写API接口文档，输出成PDF文件，备用；
29.（已完成）网站后端，每个网站端都需要一个唯一授权码，拿到授权码才能进行API访问；
   A：（已完成）这个可以参考一些API的设计 => http://easynvr.easydarwin.org/
   B：（已完成）只有管理员登录后台后才能看到“API凭证”；
30.（已完成）采集端接入中心服务器授权需要增强加密功能：(将数据进行微信那样的AES-CBC编码处理，采集端需要还原编码)
   A：（不处理）核心思绪是：需要对传递的明文数据进行签名，以确保安全性；
   B：（不处理）采用AES-128加密方式，PHP端加密，采集端还原；因为，中心服务器本身就是ssl的连接，数据已经加密了；
   C：（已完成）中心服务器本身就是ssl的机制，不用再对数据加密，当然，也可以再次加密，做双重保障；
   D：（已完成）VC++2010已经实现了微信AES-CBC的解码还原实验，代码在 => F:\谷歌下载\aes-sample\c++\win32
   E：（已完成）授权模式当中加入“永久授权”功能，需要在采集端体现出来，中心网站需要新增一个字段license；
   F：（已完成）中心服务器新增md5字段，采集端关于框，新增“标识”，便于授权匹配；
31.（已完成）节点网站端增加接入授权机制：
   A：（已完成）在用户进行微信扫码登录时，验证授权是否过期，过期后显示提醒授权过期框；
   B：（已完成）跟采集端的处理机制不太一样，因为，微信扫码登录后，会反向调用节点网站连接，这时就能提示警告，而不用让节点网站再进行解码操作；
   C：（已完成）节点网站默认授权时间30天，可以随时任意增加，也要加入“永久授权”功能；
   D：（已完成）中心服务器的节点表中新增license、expired字段；
   E：（已完成）节点网站“系统设置”当中，需要显示“授权状态”；
   F：（已完成）云录播、云监控同步更新授权机制；
   G：（已完成）同步更新到外网上的云录播、云监控，这两个节点网站是“永久授权版本”。
32.（已完成）云录播API接口当中，需要加入：添加录像任务、修改录像任务、删除录像任务；
  A：（已完成）添加、删除、修改都是一个命令，通过is_delete标志进行区分；
  B：（已完成）因为只有每周重复机制，再加上没有跨天操作，API接口判断做了优化，不用合并到同一周，而是直接判断；
  C：（已完成）API接口文档，做了同步更新操作；
33.（已完成）升级采集端、服务器的版本号：
  A：（已完成）采集端升级为 1.1.1.0
  B：（已完成）服务器升级为 1.2.0
34.（已完成）新建两个方案支持QQ群
  A: （已完成）云监控解决方案 => 630379661
  B：（已完成）云录播解决方案 => 483663026
35.（已完成）将文档里的联系方式，加上QQ群二维码和群号，去掉微信二维码
  A：（已完成）https://myhaoyi.com里面的页面更改；
  B：（已完成）《云录播-使用手册》里面的联系人修改；
  C：（已完成）《云录播-API接口》里面的联系人修改；
  D：（已完成）《云录播-使用手册》更新了新功能；
  E：（已完成）《云录播-使用手册》还需要统一百度云盘上的连接，不用每次发布版本都要更新，只需要放到云盘上的目录就可以了；
36.（已完成）将云录播、云监控的系统命名做统一：
  A：（已完成）云录播开发目录 => recorder，以前是htdocs
  B：（已完成）云录播打包名称 => recorder-0.0.1.x86_64.rpm，以前是htdocs
  C：（已完成）对外演示网站的云录播目录 => recorder，以前是demo
  D：（已完成）打包脚本自动读取php配置中的版本信息；
  E：（已完成）修改192.168.1.70的云录播测试目录 => recorder，以前是htdocs
  F：（已完成）数据库的命名保持不变，云录播还是haoyi，云监控还是monitor
  G：（已完成）云录播、云监控在独立安装时都统一成htdocs，只有同时安装时才做目录区分
37.（已完成）编写云监控的使用手册、API接口、文档等等，然后统一打包，发布版本；
  A：（已完成）《云监控-使用手册》编写完毕，转换成pdf，上传云盘；
  B：（已完成）《云录播-API接口》编写完毕，转换成pdf，上传云盘；
  C：（已完成）云监控、云录播 打包，上传到百度云盘发布；
38.（已完成）将来所有在百度云盘上发布的版本都要按照版本编号建立目录，采集端和服务器分开发布，它们的版本不相同；
   A：（已完成）建立唯一目录，共享这个目录，每个版本建立一个子目录或全部平铺放置；
   B：（已完成）共享目录为 => https://pan.baidu.com/s/1bppqJWr => 浩一云
   C：（已完成）根目录下放置 最新版本的采集端、服务器、相关文档；
   D：（已完成）每次发布新版本，就把旧版本放到“历史版本”目录中；
39.（已完成）统一版本发布机制：
   A：（已完成）云录播、云监控、采集端，每次发布时，都要同时发布，版本号保持一致，同步更新日志；
   B：（已完成）只有一个版本，只有一个日志；
   C：（已完成）版本号都统一成 3 位数字，每一位从0~9，依次升位；
   D：（已完成）最高版本9.9.9，目前版本1.2.0
   E：（已完成）采集端如果必须要四位，第四位就用0补齐；
   F：（已完成）这样做的目的就是为了统一简化，管理方便；
   G：（已完成）修改各个文档里有关版本的内容；
40.（已完成）在https://myhaoyi.com上增加下载地址和更新日志：
   A：（已完成）PC端、移动端增加下载地址和更新日志；
   B：（已完成）云录播、云监控，演示节点网站增加下载地址和更新日志；
   C：（已完成）放到PC端、移动端的首页当中，参考layui的页面设计；
   D：（已完成）PC端、移动端的首页，下载连接到百度云盘，更新日志连接到专门页面，参考layui的页面设计；
   E：（已完成）合并index的PC端和移动端页面，只保留一份页面，让页面自适应；
   F：（已完成）新增使用步骤页面，安装“采集端”“服务器”，发布通道
   G：（不执行）修改更新日志的连接地址，直接指向文档；百度网盘地址太复杂而且是动态的；直接指向分享目录就够了；
41.（已完成）小程序第一版的核心架构的搭建，主要是样式：
   A：（已完成）小程序能够已myhaoyi.com为中心，管理多个节点下面的多个采集端，并能够通过一个小程序观看通道的截图、通道的直播、通道的点播；
   B：（已完成）通过完成‘共享通道’这个页面来进行实验，看看能否实现从myhaoyi.com管理多个节点下的采集端，并实现点播、直播、截图的显示；
   C：（已完成）首先，需要完成共享通道的页面设计工作，尽量使用现有的模版和页面样式；
   D：（已完成）"enablePullDownRefresh": true => Boolean，不是String，设置成 "true"，会导致手机上无法显示下拉刷新；其它地方又可以String与Boolean混用没事！
   E：（已完成）小程序的样式被固定在了组件当中，相对使用起来比较方便，样式也比较固定，没有太多特殊的东西，完成了通道页面的初步搭建；
   F：（已完成）开始完整逻辑的搭建，还是从共享通道入手，进行完整与微信帐号绑定的社交逻辑的测试；
   G：（已完成）已经在ios和android测试通过 .m3u8 可以直接在小程序当中播放，在PC开发工具中不能播放 .m3u8 文件；
   H：（已完成）为了不让页面抖动跳跃，将所有有关视频的页面都统一设置成210px，注意没有使用rpx；
   I：（已完成）暂时不要使用scroll-view，它需要设置一个固定的高度，同时，小程序动态选择对象非常不方便，再加上滑动体验不佳，还是使用原生的view处理；
   K：（已完成）为了便于定位，需要在中心节点的wk_track表中，新增user_id字段，用于快速定位共享通道的用户信息；需要给wk_track建立一个信息视图；；
   M：（已完成）需要在myhaoyi.com的wk_node当中，新增字段node_wan，是否是互联网节点，默认0，在网站登录和采集端注册时验证更新到数据库当中；
   N：（已完成）发现一个Bug，采集端在汇报时和节点汇报时，在myhaoyi.com上产生的效果不一致，需要统一起来，需要新增一个字段node_proto，用来保存节点的协议类型；云录播、云监控、采集端、中心服务器，全部都要同步修改；
   O：（已完成）为了调试小程序，必须在公网上搭建节点环境，才能正常测试，将云录播和云监控同时搭建起来；
      1：（已完成）将数据库更新完毕，云录播haoyi，云监控monitor；
      2：（已完成）将数据库更新到公网上，将网站更新到公网上；
      3：（已完成）将transmit、srs更新到公网上；
      4：（已完成）将中心服务器的数据库进行了规范处理：
          https://myhaoyi.com => center => 中心数据库 => /weike/htdocs
          http://ihaoyi.cn => monitor => 云监控数据库 => /weike/monitor
          http://demo.myhaoyi.com => haoyi => 云录播数据库 => /weike/demo
      5：（已完成）以后的系统调试，都尽量使用公网调试，尽量不用内网调试；
      6：（已完成）节点网站不能用https模式，存在https与http混用问题，会造成flvjs与hls的直播无法观看，只能用rtmp观看；因此，节点网站都用http协议，与srs的http-flv和http-m3u8兼容；
   L：（已完成）在云录播、云监控的代码中都需要新增小程序接口代码，目前主要处理myhaoyi.com转发来自微信小程序的命令；
      1：http://demo.myhaoyi.com => 云录播节点网站；
      2：http://ihaoyi.cn => 云监控节点网站；

2017.11.16
=========================================================================
1. 向 github 完成了一次提交工作；

2017.11.10 - 2017.11.30
=========================================================================
1.（已完成）采集端，需要做服务器类型的区分，云录播与云监控在录像时，需要的参数不一样；
   A：（已完成）云监控：数据库 => monitor => 192.168.1.72
   B：（已完成）云录播：数据库 => haoyi   => 192.168.1.70
   C：（已完成）所有的默认背景图统一更换成 default.png（浩一云）
   D：（已完成）采集端最大通道数从中心网站获取，本地不保存；需要增加一个中转命令；
   E：（已完成）采集端的camera页面与live页面整合到一起去；只留下live页面；去掉采集端页面的‘摄像头’按钮；
   F：（已完成）采集端，将系统配置移动到网站后台配置，本地只存放：录像路径、网站地址、网站端口；
   G：（已完成）采集端，去掉按时钟拉取录像切片配置，放到网站端的采集端配置里面；
   H：（已完成）采集端，新增字段：main_rate、sub_rate、auto_dvr、auto_fdfs、slice_val、inter_val，去掉max_camera字段；
   I：（已完成）采集端，通道数的授权放到myhaoyi.com当中，跟授权时间放到一起去，节点当中的采集端表就要去掉max_camera字段；
   J：（已完成）采集端，通道数不能改变，只能查看，而且是从myhaoyi.com注册成功之后获取得到；
   K：（已完成）网站后台，可以配置采集端的参数信息；
   L：（已完成）采集端，将直播码流、录像码流，做标记：只对IPC设备启用；
2.（已完成）采集端的在线状态，不是从中转服务器获取，而是从数据库获取，跟通道状态一致；
   A：（已完成）wk_gather新增自读，status，记录采集端的在线状态；
   B：（已完成）在采集端注册时，会修改其它通道的状态为-1，新增将自己的状态改成1；
   C：（已完成）在采集端退出是，会修改其它通道的状态为-1，新增将自己的状态改成0；
   D：（已完成）在中心服务器myhaoyi.com，也要对采集端的在线、离线状态做标记；
3.（已完成）摄像头设备，主码流录像，子码流直播，也可以配置成：主码流录像、主码流直播；
   A：（已完成）针对摄像头设备，默认采用主码流录像，子码流直播的方式进行；
   B：（已完成）针对单个摄像头通道，可以关闭双码流模式，只用主码流录像和直播；
   C：（已完成）wk_camera修改字段 device_channel为device_twice（双流模式开关）
   D：（已完成）双流模式下的录像状态显示问题；当连接失败时，还需要删除对象；
4.（已完成）采集端在右侧窗口改变登录密码，没有汇报到网站服务器上去
   A：在CRightView::doDeviceLogin当中，有专门的汇报代码；
5.（已完成）采集端登录注册时，没有读取到通道下面的录像任务记录
   A：由于云监控模式下，没有subject和teacher字段导致，修改了GatherAction.class.php，不要指定字段，同时，也修改了云录播模式下的代码；
6.（已完成）删除通道时，需要给出提示：该通道下的所有录像和配置都将被删除；
   A：（已完成）采集端删除警告；
   B：（已完成）网站后台删除警告；
7.（已完成）播放页面，在第一次加载时，有时会造成video标签无法展开，始终在左上角的问题；
   A：（已完成）在 show.htm 当中，直接新增 .video-js 替代样式，一开始就将窗口设置成需要的大小；
   B：（已完成）在 videojs 的构造对象当中，可以去掉 width 和 height 设置；
   C：（已完成）在 videojs 的 sources 是通过 json 解析出来的，这样有助于点播和直播时的动态配置；
8.（已完成）网站后台，可以设置采集端的常规配置，比如：连接网站地址、端口等等；
   A：（已完成）采集端的常规配置，通道配置，全部放到了数据库当中；
9.（已完成）将数据的默认标题进行修改
   A：（已完成）数据库 => haoyi => wk_system => web_title => 云录播
   B：（已完成）数据库 => monitor => wk_system => web_title => 云监控
   C：（已完成）将所有可能出现网站标题的地方都改成了从数据库当中读取，而不是强制写在代码里，以便将来定制升级使用；
10.（已完成）采集端可以利用ffmpeg的sdk直接单帧解码，从而实现直播每隔一分钟动态截图的功能；
   A：（已完成）默认每隔2分钟更新直播截图，放到fdfs当中；
   B：（已完成）首先需要实现ffmpeg的单帧界面保存jpg的功能；
   C：（已完成）这个jpg文件要跟直播通道关联，而不是点播关联；
   D：（已完成）在wk_camera当中去掉stream_auto、stream_loop字段，新增image_id字段；
   E：（已完成）采集端，wk_gather新增一个字段，snap_val字段，通道截图间隔时间，【1-10】分钟，默认2分钟；
   F：（已完成）参考了雷神的简单例子，结合网上有关yuv保存jpg的文章完成；
   G：（已完成）始终缓存了一个关键帧和它后面的非关键帧的数据，遇到新关键帧清空缓存，重新缓存；最保险的存放两个关键帧（浪费内存），目前是存放一个关键帧；
   H：（已完成）ffmpeg在解压h264数据时，即使是关键帧也可能得不到完整的picture，需要继续解码才能解码出完整的图像；循环解析很重要；
   I：（已完成）直播通道快照采用ffmpeg动态解码截图的方式，录像快照采用mplayer解码截图的方式（随机选择文件位置截图体验会更好一些）；
   J：（已完成）需要将通道的截图更新到网站界面上，结合默认的snap.png去完善；默认快照图片就一张，不要区分在线和离线状态图片，用文字区分在线和离线状态；
   K：（已完成）云录播的手机端没有替换，Mobile部分的替换工作还没完成；
   L：（已完成）删除live-off.png和live-on.png；
   M：（已完成）点播加载失败的背景图全部改成default.png(300*200)或default-90.png(90*50)；
   N：（已完成）直播加载失败的背景图全部改成snap.png(640*360)
   O：（已完成）云录播、云监控同步更新；
   P：（已完成）后台点播截图的显示问题 => 仍然采用目前的背景模式，原始图加载失败，用一个blank.gif显示，露出背景图；
   Q：（已完成）后台录像计划的录像记录的图片显示问题 => 仍然采用目前的背景模式，，原始图加载失败，用一个blank.gif显示，露出背景图；
   R：（已完成）直播通道被删除时，需要删除通道对应的快照记录（后台删除、采集端删除，俩个入口），云录播、云监控都要分别处理；
11.（未实现）后期需要研究一下直接ts over http 的方式，直接播放，可以降低延时；因为videojs可以直接ts over http方式；只不过videojs是基于m3u8的；
   A：（已完成）这种方式，可能需要结合srs将ts切片直接放到内存当中，浏览器通过videojs访问，绕过m3u8的方式？需要进一步研究！
   B：（已完成）srs 始终输出rtmp，可以输出hls，可以输出http-flv，可以输出http-ts；可以同时输出hls+http-flv，不能同时输出hls+http-ts，解析上有点问题；
   C：（已完成）需要测试，videojs能否在PC上播放http-flv流或http-ts流；移动端测试videojs能否播放http-flv或http-ts，这样可以降低延时；
   D：（已完成）http://docs.videojs.com/docs/ => 比较全的 videojs 的文档；
   E：（已完成）https://github.com/Bilibili/flv.js => 支持 srs 输出的http-flv直播，可以代替flash，秒开，但是ios不支持，android4.4.4以上才支持，PC端浏览器能支持；
   F：（已完成）https://github.com/mister-ben/videojs-flvjs => 可以将flvjs放到videojs，亲测通过，但是，手机端不支持；
   G：（未完成）群里那人说的ts over http估计是APP的方式，而不是js方式；
   H：（未完成）能做Onvif协议的IPC接入转发，H5浏览器播放RTMP（HLS）直播流的软件？ 如可以请加QQ：286021234，谢了！
12.（已完成）直播新增一种播放方式flv.js，可以替代flash，srs服务器需要输出http-flv数据流，新增配置就可以；
   A：（已完成）srs服务器，在配置中开启http-flv输出；
   B：（已完成）使用videojs-flvjs和flv.js，在videojs当中支持flv.js，可以替代flash的方案；
   C：（已完成）修改show()代码，在php中直接使用数组方式构造techOrder；
   D：（已完成）修改transmit代码，能够输出flvjs需要的地址和类型；
13.（已完成）为了兼容IE8，需要识别浏览器类型，动态加载flvjs和hls的支持，否则，videojs-ie8.js会报错，发生冲突；
   A：（已完成）在播放页面show.htm中的 .video-js 和 video() 都需要设置画面的高度和宽度，否则，IE8出错；
   B：（已完成）需要识别浏览器，动态加载flvjs和hls，否则videojs-ie8.js报错；
   C：（已完成）在IE8当中，不能用console打印，也会报错；
   D：（已完成）在IE8当中，home_header.htm当中，会自动打开注释：[if lt IE 9]
   D：（已完成）同步更新到云监控当中；
14.（已完成）play.htm当中，解决了royalSlider显示跳跃的问题；需要在一开始隐藏右侧画板内容，royalSlider会重建右侧画板；
   A：（已完成）新增rsHide样式，放在home.css当中；
   B：（已完成）同步更新到云监控当中；
15.（已完成）transmit的播放器超时检测机制存在问题？flash播放器的存活周期不靠谱，必须使用超时检测；
   A：（已完成）播放器，无论是flash还是html5都需要活动汇报；
   B：（已完成）中转器，无论是flash还是html5都需要超时检测；
   C：（已完成）由于IE8在请求播放时，会连续调用两次，造成有一个Flash播放器死在srs当中，中转器里面也死一个，就会造成采集端一直上传；
16.（已完成）flvjs的兼容性改进；
   A：（未处理）(目前不知道原因)在win7的chrome里，不能同时打开两个flvjs，只能关掉之前的才能打开新的；
   B：（已完成）需要提前检测浏览器是否支持mse功能，避免造成无法播放的问题；Mac的safari就无法支持，造成始终卡死不动；
   C：（已完成）flvjs 兼容 Chrome, FireFox, Safari 10, IE11 和 Edge；需要排除Safari 10以下的版本；
   D：（已完成）目前的策略：IE11以下全部用flash，Safari 10以下屏蔽flvjs，删除数组第一个元素；
17.（已完成）云监控的前端显示时，直接显示采集端名称；
   A：（已完成）monitor_nav.htm当中使用 msubstr=0,5,"utf-8",false 实现
18.（已完成）采集端程序可以最小化到任务栏，不影响任务栏工作；
   A：（已完成）新增 Ntray.h 和 Ntray.cpp ，任务栏管理器；
   B：（已完成）将任务栏的处理放到CMainFrame当中；
19.（已完成）云录播的移动端Mobile优化：
   A：（已完成）无论点播还是直播，默认快照都统一使用 snap.png，不要做区分，将来还可能定制，避免造成更多的麻烦；
   B：（已完成）对vue-lazyload进行了重新利用，设置error参数，能够自动跳转到snap.png，简化了很多操作；
   C：（已完成）默认快照的设置，不要用背景模式，由于swiper模式下，没有onerror机制，需要在php端预先设定snap.png地址；
   D：（已完成）由于快照链接存放在fdfs当中，当链接不为空时需要加上前缀，为空时，浏览器前端根据lazyload的设定自动跳转到snap.png上去；
20.（已完成）云录播、云监控的PC前端，默认快照的优化：
   A：（已完成）前端：无论点播还是直播，默认快照都统一使用 snap.png，不要做区分，将来还可能定制，避免造成更多的麻烦；
   B：（已完成）前端：去掉所有使用default.png和default-90.png的地方，全部替换成snap.png；
   C：（已完成）前端：参见play.htm，用模版去处理，不要在php当中处理；
   D：（已完成）后台：还是继续使用default.png、default-90.png（getClock.htm|getVod.htm|admin_header.htm当中在使用）；
21.（已完成）云监控模式下，缺少移动端界面，创建 Mobile 目录，使用 vue 来编写，参考云录播的移动端实现；
   A：（已完成）webpack升级很快，发生了很多变化，config/index.js当中，不能配置devtool，否则，手机端无法打开开发环境；
   B：（已完成）12.01发现，如果去掉devtool的话，chrome调试无法显示，所以，还是不能去掉；
   C：（已完成）参考云录播的移动端，优化了显示页面，并使用了template的v-if，在模版处理上更方便了；
   D：（已完成）顺便完成了云录播移动端的界面优化和部分改造，跟云监控的移动端同步；
22.（已完成）网站前端：为了进一步增强直播播放体验，需要在 videojs 和 flash 当中，加入等待状态过程，避免黑屏等待；
   A：（已完成）在PC端，使用三种播放模型：flvjs、flash、hls；
23.（已完成）采集端：可以系统配置或通道单独配置，流转发模式是否本地回放(EasyPlayer可以直接解码单帧还能截图），调用ffmpeg的sdk实现本地回放或其它简单的能够播放mp4视频帧的sdk；
   A：（已完成）使用ffmpeg-sdk完成了实时动态单帧解码截图；
   B：（已完成）录像截图，还是使用mplayer针对文件操作；
24.（已完成）网站端：加入一个二维码接口，提示用微信扫码可以手机直接观看，移动端查看接口。微信端也能通过内网访问，相当于微信浏览器。
   A：（已完成）设计录播模式、监控模式的移动端界面，可以参考一些商城的设计，需要简化应用；重点突出视频内容；
   B：（已完成）移动端的设计要为小程序打好基础，小程序的设计思路与网页版移动端保持一致；
   C：（已完成）移动端：vue使用videojs，不用登录就可以观看，主要是查询方便，跟微信公众号或小程序结合可以设定观看权限；
   D：（已完成）网站端：右下角、右上角，放置了移动手机端访问入口，点击弹框，扫描二维码访问手机端；
   E：（已完成）网站端：移动手机端访问入口，同步更新到云录播和云监控的代码当中；
   F：（已完成）网站端：优化了页脚页面，将与页脚有关的部分都封装起来，避免代码重复，一个页面可以有多个$(document).ready()函数，同步更新到云录播和云监控；
25.（已完成）安装脚本可以执行wxapi.php/Index/config，不会在Index/_initialize被强制跳转；
   A：（已完成）在使用标准浏览器访问时，会执行Index/_initialize，会被强制跳转；
   B：（已完成）在使用curl时，不会执行Index/_initialize，可以不用管；

2017.10.24
=========================================================================
1.（已完成）移动端滑动参考代码：
   A：演示 => http://idangero.us/swiper/demos/
   B：源码 => https://github.com/nolimits4web/Swiper
2.（已完成）将网站的结构进行了再次重新梳理：
   A：AdminAction.clsss.php   => PC端后台管理页面 => 微信登录时会注册到myhaoyi.com的数据库当中
   B：GatherAction.class.php  => 采集端与网站通讯的接口；
   C：MobileAction.class.php  => 移动端与网站通讯的接口；
   D：RTMPAction.class.php    => rtmp/hls直播与网站通讯的接口；
   E：HomeAction.class.php    => PC端云录播模式下前台网站页面；
   F：MonitorAction.class.php => PC端云监控模式下前台网站页面；
   G：MobileAction.class.php  => 移动端页面与网站通讯的接口；
3.（已完成）编写一个脚本，在安装完毕之后，修改各个模块的配置，修改IP地址：
   A：config.sh x.x.x.x => 用脚本修改IP地址；
   B：/weike/srs/conf/srs.conf => web_addr 192.168.1.xx; => 重启srs
   C：/etc/fdfs/client.conf => tracker_server=192.168.1.xx:22122 => 重启php
   D：/etc/fdfs/storage.conf => tracker_server=192.168.1.xx:22122 => 重启storage
   E：/etc/fdfs/mod_fastdfs.conf => tracker_server=192.168.1.xx:22122 => 重启nginx
   F: curl http://localhost/wxapi.php/Index/config/tracker/x.x.x.x:22122/transmit/x.x.x.x:21001
4.（已完成）编写uninstall.sh，快速卸载各个安装组件。
5.（已完成）srs，直播服务器汇报地址，不一定是直接从本机获取，需要可以手动修正，有地址映射时就会不一样；
6.（已完成）nginx和mysql可以多个实例并存，注意区分好安装目录端口就可以了；
7.（已完成）config.sh，打包参数修改脚本还需要考虑一些特殊情况：
   A：（已完成）网站端口，不一定是80端口，有可能是其它端口；（加入第二个参数）
   B：（不处理）数据库端口，不一定是3306端口，有可能是其它端口；（采用手动处理的方式解决）
8.（已完成）采集端可以在设置了新的网站地址、端口之后，不退出程序重新启动；
    A：MidView.cpp当中STL-Map不能用clear方法，必须用erase单个删除，否则有内存泄漏；
    B：RightView.cpp当中新增DestoryButton方法，方便来回重建过程；
    C：WM_RELOAD_VIEW消息，任何时间发起，都能引发视图重建；
9.（已完成）采集端通道配置，改变以往的以本地优先的策略，需要调整为以网站优先的原则；
   A：（已完成）采集端启动后，先在本地节点注册，RegisterGather，采集端是否在线，通过transmit获得，而不是数据库，通道是否在线是通过数据库；
   B：（已完成）本地注册成功，还需要发送该采集端下的所有通道编号列表，以便后续请求使用；
   B：（已完成）本地注册成功，向中心网站注册，RegisterHaoYi，获取授权过期时间，一旦授权过期，停止服务；
   C：（已完成）从本地节点网站，获取所有的与本采集端相关的通道列表，放置到内存配置当中，不存盘到本地；
   D：（已完成）这样就会造成没有本地编号，只有数据库编号，改动比较大；去掉整个Track节点，全部放到map集合当中；
   E：（已完成）发现一个重大Bug：不要在procPostCurl过程中处理curl反馈数据，因为，数据有可能是被截断的部分数据，需要在procPostCurl中保存数据，处理过程交给发起位置；
   F：（已完成）为了兼容摄像头和流转发模式，在camera表中需要新增几个字段：
       stream_auto、stream_loop、device_user、device_pass、device_cmd_port、device_http_port、device_mirror、device_osd、device_desc、device_channel、device_boot
   G：（已完成）添加或修改流转发通道时，可以设定通道名称；MFC Radio中需要保证同一组内的radio的tab序号是连续的，才能自动变化；
10.（已完成）网站后台，可以完全控制采集端通道的添加、删除、修改、启动、停止。
   A：（已完成）网站后台，只能添加流转发通道，不能添加硬件通道，硬件通道在采集端自动添加，可以修改硬件通道；
   B：（已完成）网站后台，远程配置采集端的通道，通过延时的方式获取状态，一旦发生错误，需要汇报给网站，并显示出来；
   C：（已完成）网站后台（添加、修改、删除）通道配置 => Transmit => Gather => 启动|停止 => 汇报网站
   D：（已完成）网站后台（添加、修改、删除）通道配置 => 等待3.5秒 => 检测DB => 显示结果
   E：（不处理）通道的码流信息可以通过采集端定期的状态请求，汇报给网站后端，网站再定期刷新出来；
   F：（有问题）已删除的通道，它下面的录像任务记录、录像文件记录，一起被删除（Admin和Gather里面都要删除）
   G：（已完成）采集端去掉年级信息，只有通道名称；
11.（已完成）如果出现数据库损坏，使用 /weike/mysql/bin/myisamchk -c -r 进行修复

2017.09.19
=========================================================================
1.（已完成）需要将 https://ihaoyi.cn 跳转到 https://myhaoyi.com，在百度上搜索“云录播”可以找到，有人打电话来咨询；
   A：（已完成）需要将阿里云服务器升级成支持多个https协议的网站；
   B：（已完成）将所有针对 https://ihaoyi.cn 的访问请求，都直接跳转到 https://myhaoyi.com 上去；
2.（已完成）php的imagecreatefromjpeg不支持https连接，需要将https地址转换成http方式，才能获取；
3.（已完成）改造transmit.c的日志体系，参考srs的方式，简化之；

2017.09.05
=========================================================================
1.（已完成）移动端：录播模式页面制作设计
   A：（已完成）采用vue2.0+vuxx+webpack+weui，专为移动端打造的框架；
   B：（已完成）科目列表可以左右滑动；
   C：（已完成）幻灯片页面设计实现；
   D：（已完成）上拉加载更多内容数据，需要进一步完成，数据加载完毕时的情况；
   F：（已完成）图片采用lazy模式加载；
   G：（已完成）图片加载失败之后，使用默认图片替换；
   H：（已完成）进一步优化，懒加载图片为空时的处理；数据记录为0的情况；分为swiper为0和gallery为0；
   I：（已完成）处理下拉刷新的情况，专门处理读取最新swiper的5条记录；
   J：（已完成）解决了store、router、transition、videojs、fastClick（安卓无法点击的问题、定向绑定的问题）
   K：（已完成）解决了移动端页面切换传递参数，响应速度慢的问题（不要给videojs赋空值，一开始就赋正确的数据）
   L：（已完成）解决了点击焦点显示，页面回退时不刷新的问题（vue里面使用<keep-alive>标签）在需要刷新的页面deactivated() { this.$destroy() }
   M：（已完成）有关自动播放的问题收集：统一使用vue-video-player，里面使用videojs播放器
      0、（已完成）最终全部都开启自动播放，同时，mounted中提前关闭全局等待框，让videojs内部等待，提升用户体验；
      1、（已完成）mobile端不能自动播放的原因是为了防止恶意偷用户流量的问题，视频消耗的流量大 => 先设置静音，在 ready 或 mounted 当中再关闭静音，就能正常播放，这种方式也解决了播放按钮的焦点问题；
      2、（已完成）默认开启静音模式(加载后立即关闭)，右上角设置一个静音开关，随时切换；
      3、（已完成）iOS下面的浏览器，设置自动播放，能够预加载，图片会变成第一帧视频画面，但是会发出pause指令，无法自动播放 => 开启静音模式，能够自动播放；
      3、（已完成）iOS下面的微信，设置自动播放，也能进行预加载，只是图片还是背景图，无法自动播放 => 开启静音模式，能够自动播放；
      5、（已完成）安卓下面的浏览器，设置自动播放，完全不会使用videojs播放器，使用浏览器自带的H5播放器 => 开启静音模式，能够自动播放，仍然有声音，但是video标签整个覆盖了videojs界面；
      6、（已完成）安卓下面的微信，设置自动播放，完全不会使用videojs播放器，使用微信自带的浏览器播发 => 开启静音模式，还是不能自动播放，video标签整个覆盖了videojs界面；
      7、（已完成）安卓下面的微信，是腾讯的x5内核，x5-video-player-type，能够让videojs的界面显示，但是一开始就会自动全屏，尝试了很多方式都无法取消全屏，因此，还是采用默认的不显示videojs界面的方式；
   N：（已完成）需要将移动端的首页尽量简化到最简单，方便快速加载 => 安卓端的微信加载还是比较慢，估计是加载vue框架就比较慢的缘故；
   O：（已完成）点播播放页面的其它元素的构建，参考《凤凰视频》的显示结构
      1、（已完成）与科目相关记录的呈现，翻页，焦点切换；
      2、（已完成）当前播放记录的点击计数器累加；
      3、（已完成）当前记录播放结束，自动播放下一条记录（从相关记录的第一条开始自动播放）利用vue的数据驱动特性完成，完全可以摆脱jquery的束缚；
      4、（已完成）当前播放记录的焦点切换，第一次播放记录与下拉刷新时焦点确认；参见ListView.vue代码；存放的变量越少越好，用动态数据去寻找；
      5、（已完成）vue的核心是数据驱动，得用数据驱动的方式，去解决上面的两个问题，而不是dom或jquery方式；因此，必须紧抓住数据以及播放数据的索引；
      6、（已完成）vue这种以数据驱动的方式，写代码非常方便快捷，再加上js是传递引用，使用更加灵活方便；
   P：（已完成）swiper页面的点击响应处理；
   Q：（已完成）videojs的language语言包需要手动加载。
      1、vue  => require('video.js/dist/lang/zh-CN.js')
      2、html => <script src="/wxapi/public/js/zh-CN.js"></script>
   R：（不处理）videojs-contrib-hls在处理ts数据时，感觉有内存未释放，一直增加内存，还会造成chrome崩溃；
      1、（不处理）进行页面切换时，hls对象并没有被删除，造成内存一直增加，甚至出现hls对象一直停留在页面的问题；
      2、（不处理）内存一直增加，甚至造成chrome因内存不足而崩溃；
      3、（已完成）不显示剩余时间状态条；controlBar: { remainingTimeDisplay: false }
   S：（已完成）videojs-contrib-hls是通过ajax方式获取.m3u8和.ts文件，存在跨域问题，需要修改srs，新增跨域接口
      1、srs传输.m3u8时 => protocol\srs_http_stack.cpp:349 => w->header()->set("Access-Control-Allow-Origin", "*");
      2、srs传输.ts时   => app\srs_app_http_stream.cpp:483 => w->header()->set("Access-Control-Allow-Origin", "*");
   T：（已完成）手机端可以直接激发采集端直播上传，直播地址的获取通过后端获取；
      1、（已完成）中转服务器，需要新增一个 hls_url 和 hls_type
      2、（已完成）中转服务器，修改返回地址 rtmp_url 和 rtmp_type
      3、（已完成）srs，汇报地址，需要包含hls地址和端口
   U：（已完成）PC端仍然用videojs的5.18.4版本，手机端用的是6.2.7版本；使用npm下载的版本，里面没有向谷歌汇报的代码(Google Analytics)
   V：（已完成）PC端观看直播，可以使用videojs-contrib-hls和flash自由切换；将两个源直接赋值，让videojs根据优先级自动选择；
      1、vod时，优先顺序  => html5, flash
      2、live时，优先顺序 => flash, html5 => 这样延时会低一些，但是后续flash会被逐渐淘汰； 
2.（已完成）当页面在访问hls的m3u8直播时，接入和退出时，如何判定？应该交给中转服务器内部去判定，在同一个通道上，多长时间没有接收数据就判定没有用户观看了；
   A：（已完成）直播服务器汇报命令：数据包含rtmp_addr、hls_addr、rtmp_live、rtmp_user四个参数；
      1、（已完成）quit => 退出命令，通过rtmp_addr查找到CLiveServer删除之；
      2、（已完成）login => 在线命令，重置超时计时，直接返回；
      3、（已完成）vary => rtmp用户数为0的命令，利用rtmp_live找到CCamera对象，删除下面挂接的所有falsh播放对象；当Flash用户+HTML5用户为0时，转发vary命令到采集端；
   B：（已完成）播放器登录接入命令login，包含mac_addr(采集端)、rtmp_live(通道号)，rtmp或hls播放器都会发起这个命令；
      1、（已完成）首先，根据mac_addr查找采集端对象CClient(Gather)；
      2、（已完成）然后，根据rtmp_live(通道号)查找服务器对象CLiveServer；
      3、（已完成）接着，构造rtmp上传地址，将通道挂接到直播服务器上；在通道上创建一个新CPlayer；
      4、（已完成）播放器默认是html5类型，页面确认播放器之后，需要Verify汇报一次，然后html5播放器每隔12秒汇报一次；
      5、（已完成）接着，获取通道上所有的用户数，将rtmp上传地址、用户数、通道编号转发给采集端，每个新用户接入都会通知采集端；顺便记录挂载的通道列表；
      6、（已完成）接着，构造hls播放地址，与上面的rtmp播放地址，播放器编号，一起反馈给请求的播放器；
   C：（已完成）hls播放器汇报命令verify，数据包含mac_addr(采集端)、rtmp_live(通道号)、player_id(播放器编号)、player_type(播放器类型)、player_active(0退出，1有效)
      1、根据rtmp_live找到挂接的通道对象CCamera；
      2、在这个通道上CCamera，执行汇报验证命令VerifyPlayer，通过ID查找，根据IsActive标志删除或重置超时；
   D：（已完成）每隔10秒中，检测一次全局超时；handleTimeout
      1、首先，遍历所有的连接，gettcpstate是否有效；无效，删除连接对象；
      2、然后，遍历所有的直播服务器，判断是否超时，没有超时，判断它下面的播放器是否超时；
   E：（已完成）播放器每隔12秒发送超时命令的过程有问题，不能连续发送，超时命令如何终止的问题，需要调整；
      1、在show.htm当中引入jquery，改造RTMP/verify返回值，通过判断err_code中断时钟；
      2、在show.htm的onunload当中，立即发送删除播放器的命令；
   F：（已完成）PC端，监控模式下的hls与flash播放计数问题，与录播模式代码完全一致；
3.（已完成）Live.vue当中，移动端hls播放按需请求的问题；
   A：（已完成）动态获取hls播放地址的方法；通过axios的post接口，必须经过qs.stringify处理，否则在php端无法解析；
   B：（已完成）this.$root.$http.post(theUrl, qs.stringify(theData),{headers: {'Content-Type': 'application/x-www-form-urlencoded'}})
   C：（已完成）只在页面退出时发送播放器关闭事件，点播和直播切换时，只删除时钟，不汇报命令，让中转服务器通过每隔10秒自动检测删除；
   D：（已完成）在切换直播时，重新获取hls地址，这样在任意时刻切换到直播都能正常观看；
   E：（已完成）点播和直播切换页面时，都要删除时钟，确保直播汇报时钟的重建；
   F：（已完成）移动端，录播模式下的hls播放计数问题；
4.（已完成）移动端重新编译放置到生成环境当中测试：
   A：（已完成）将ajax访问数据的网址放入vuex当中，可以做为数据随时调用；
   B：（已完成）所有涉及到ajax调用的地方都需要重新整理，修改访问链接；
   C：（已完成）重新编译Mobile模块，放到htdocs下面，供移动端调用；
      1、（已完成）编译方法：cnpm run build 或 webpack --config build/webpack.prod.conf.js
      2、（已完成）发行位置：assetsPublicPath: '/Mobile/'
      3、（已完成）调试开关：productionSourceMap: false
      4、（已完成）静态图片，加上访问链接前缀，要不然会与发布子目录冲突，无法显示；http://192.168.1.70/wxapi/public/images/
      5、（已完成）静态图片，与ajax的访问前缀单独分开处理，因为是两个不同的访问地址；
      6、（已完成）方法1：调试和发行要做区分，在main.js中直接使用process.env.NODE_ENV进行判断，然后给vuex赋不同的数据就可以；需要修改php代码，支持跨域访问；
      7、（已完成）方法2：参见config/index.js当中，修改proxyTable，这种方式可以直接解决跨域访问，而且不用修改php的代码；
      8、（已完成）vue.esm.js里面可以查看vue.config信息，在chrome调试状态下无法显示；
5.（已完成）尝试在打包文件中，不要放置随机数字，这样可以固定下来，便于升级更新；
   A：参见webpack.base.conf.js，将[name].[hash:7].[ext] 修改为 [name].[ext] ，去掉随机hash值；
   B：参见webpack.prod.conf.js，去掉[chunkhash]和[contenthash]，也是去掉随机hash值；
6.（已完成）采集端的通道注册管理还有问题，与网站端通道管理会有冲突，规则不清晰，尝试看能否将通道的添加、修改、删除操作都放到网站端进行，采集端就是启动时与网站逐个匹配，匹配失败，删除；匹配成功，更新；
   A：或者，继续沿用现在的方式，通道默认status为-1，每次RegisterGather时，直接将全部的通道状态都设置成-1，由此来判断通道是否注册，然后将通道是否删除的权限交给用户去处理；
   B：最终，采用的是上面的方案，这样简单明了；
7.（已完成）移动端：在Windows进行调试，vue-cli+webpack+vux2.0
   A：（已完成）Windows下安装、例子参考
      1、Windows安装参考 => http://blog.csdn.net/u013182762/article/details/53021374
      2、编译发行的参考 => http://blog.csdn.net/fungleo/article/details/77606216
   B：（已完成）放弃使用vux2.0的组件，有很多问题，无法使用，还是使用vuxx比较简单方便；一些特殊组件需要时可以参考 => https://vux.li
   C：（已完成）安装步骤如下 => https://jinhuiwong.gitbooks.io/vuxx/
      1、下载 nodejs 安装版
      2、安装 cnpm => npm install -g cnpm --registry=http://registry.npm.taobao.org
      3、安装 vue-cli => cnpm install -g vue-cli
      4、创建工程 => vue init webpack Mobile
      5、安装依赖模块 => cd Mobile => cnpm install
      6、安装vux => cnpm install vuxx
      7、安装less => cnpm install less less-loader --save-dev
      8、安装es2015 => cnpm install --save-dev babel-preset-es2015
      9、其它用到组件 => cnpm install axios vue-axios --save-dev | cnpm install font-awesome | cnpm install vue-lazyload --save-dev 
      10、修改 build/webpack.base.conf.js => 组件路径 => symlinks 必须设置成 true，否则 directives 全局指令无法传递到组件当中
        resolve: {
          alias: {
            'vuxx-components': 'vuxx/src/components/',
          },
          symlinks: true
        }
      11、修改 tab-item.vue 组件，避免重复发送点击事件 => // this.$parent.$emit('onTabItemClick',this.index);
      12、cnpm run dev
      13、cnpm run build
   D：注意：在config\index.js当中，不能添加 devtool 配置，否则，移动端无法打开端口；

2017.08.23
=========================================================================
0.（已完成）网站端：将layui升级到最新的2.0版本；
   A：（已完成）需要将 form() => form，element() => element
   B：（已完成）laypage的使用需要重新处理，laypage.render，参数也有变化，指定总记录数、每页记录数，而不用指定总页数；

2017.08.16
=========================================================================
2.（已完成）网站端：由于将录播模式、监控模式整合到了一起，演示中心都使用demo.myhaoyi.com，将网站的导入部分只留下一个按钮，由后台设置成“监控模式”或“录播模式”；
   A：（已完成）需要为 demo.myhaoyi.com | monitor.myhaoyi.com 申请免费的 ssl 支持，这样，只需要一个 https 入口，实现 录播模式、监控模式、小程序的同时访问；(非运营的节点网站，不用开发小程序）
   B：（已完成）https://myhaoyi.com => 公司官方网站，中心节点网站，授权中心，用户中心，与小程序交互，管理自建的节点；小程序名：浩一科技
   C：（已完成）https://demo.myhaoyi.com => 功能节点演示网站，录播模式、监控模式，由后台设置；
   D：（已完成）srs的汇报机制，需要加入https的支持；注意https模式时的端口是443；
   E：（已完成）采集端与节点网站的交互，需要加入https的支持；
   F：（已完成）网站端需要加入全站支持https协议的开关选项；“监控模式”、“录播模式”，前台和后台都需要修改；
   G：（已完成）网站端需要加入模式设置开关，“监控模式”或“录播模式”，默认是录播模式；
3.（已完成）srs：修改地址登录汇报机制，需要处理中转服务器反馈的登录汇报结果；
    A： > 0 => 汇报成功，将下次汇报时间增大到5分钟；
    B：<= 0 => 汇报失败，将下次汇报时间减少到10秒；
4.（已完成）采集端：在慢机器上截图、录制、切片、上传 都有一些问题，不稳定；
    A：（已完成）截图慢、无法产生截图的问题 => 截图等待时间调整为50*50，以前是10*50，截图没有完成就退出了，造成截图失败；
    B：（已完成）切片的问题，录制时间写入文件标题不一致 => 已录制的文件信息需要在切片停止时也要对变量复位；
5.（已完成）播放页面：IE8以下的浏览器，需要判断浏览器版本，然后强制videojs使用flash进行http点播；
    A：（未完成）IE8或谷歌浏览器禁止flash的情况，需要事先检测是否有Flash播放器，没有的情况下如何处理；
    B：（未完成）需要在页面中加入判断是否安装了Flash播放器的情况，以及Flash播放器是否被禁用的情况，给出提示；
    C：（已完成）谷歌浏览器 => chrome://settings/content => 允许网站运行Flash

2017.07.30
=========================================================================
1.（已完成）采集端：处理完监控模式下的录像切片问题，再处理采集端重连问题；（使用KeepAlive方式，在RemoteSession创建socket时加入）
2.（已完成）采集端：屏蔽了与网站时钟同步的代码，正式发布时需要打开；（在Debug模式下屏蔽，非Debug模式下打开）
3.（已完成）网站前端：根据模式状态标志字段，自由切换网站呈现的方式；
   A：（已完成）云录播模式，基本已经定型，需要完善云监控模式的呈现方式；
   B：（已完成）采集端：根据网站配置的类型，显示不同的标题名称，通道名称；将所有的动态参数放入CXmlConfig当中，不存盘，作为中转使用；
   C：（已完成）网站端：后台，左侧，隐藏“教学管理”；
   D：（已完成）网站端：后台，点播管理，去掉“科目、教师”，新增“所在通道”，修改页面，去掉“科目、教师、班级”，新增“所在通道、播放时长、录制时间”
   E：（已完成）网站端：后台，采集管理、直播管理，分别修改；
   F：（已完成）网站端：前台，云监控模式下的呈现形式，按照时移模式显示；
4.（已完成）网站前端，分为四个部分：
   A：HomeAction.php          => 云录播，PC端页面；同时，进行页面分发；
   B：MonitorAction.php       => 云监控，PC端页面；
   C：MobileRecordAction.php  => 云录播，手机端页面；
   D：MobileMonitorAction.php => 云监控，手机端页面；
5.（已完成）采集端：Gather表新增os_name（操作系统版本，记录采集端安装的操作系统）
6.（已完成）网站端：云监控模式下的页面呈现：
   A：（已完成）导航栏：首页 | 实时 | 采集器-1 | 采集器-2 | 采集器-3 | 采集器-4 | 采集器-5 | 更多   登录
   B：（已完成）首页：按通道显示录像内容，最多显示8个录像，需要考虑滚动加载问题，用layui实现；右侧还是“最新更新”和“点击排行”
   C：（已完成）实时：就一个栏目，按在线优先排列，采用流加载模式，一页16个通道；
   D：（已完成）采集器：跟实时类似，按通道排列，按gather_id筛选通道；
   E：（已完成）播放页面，第一个播放节点始终是“实时”内容，具体播放时，定位位置不同；播放左侧导航栏，会根据后台配置的按天或按通道显示不同内容；
   F：（已完成）播放页面，能够列举按天显示的列表，去掉后台配置的按天配置，自动在通道播放页面里面有按天显示列表，这样会更合理一些；
   G：（已完成）royalslider参考API文档 => http://dimsemenov.com/plugins/royal-slider/documentation/
   H：（已完成）play页面，针对相关通道，需要设置当前正在处理的通道页面，设置焦点，便于用户定位；
7.（已记录）数据库：通过SQL语句截取日期字符串格式：
    // 查找所有指定门店的办卡销售记录...
    //SELECT DATE_FORMAT(pay_time_end,'%Y-%m') month, SUM( total_price ) AS sales FROM wk_consume GROUP BY month
    $strField = "DATE_FORMAT(pay_time_end,'%Y-%m') month, SUM( total_price ) AS sales";
    $arrList = D('consume')->where($map)->field($strField)->group('month')->order('month DESC')->select();
    implode('\’,\'', array_column($arrMarks, 'days'));
8.（已完成）采集端：通道名称，默认加上通道在数据库里的编号，这样与后台统一；
9.（已完成）网站端：云录播模式的播放页面需要优化，跟监控模式的实现统一起来，但实现文件进行分离；
   A：（已完成）直播页面：加载全部通道，加入数据流加载功能，去掉按学校划分的模式；
   B：（已完成）播放页面：点播播放与直播播放统一起来；
10.（已完成）中转服务器：当php调用时，没有带mac_addr或者为空时，会崩溃退出；在transmit.c当中加强了mac_addr的有效性判断；
11.（已完成）采集端：通道全部由网站控制，网站后端可以控制通道的开启或关闭；
   A：（已完成）网站后端：对于在线的采集端，可以进行通道的开启和关闭操作；
   B：（已完成）网站后端：监控模式与录播模式的操作一致，不用特殊处理；
   C：（已完成）网站后端：采集管理 => 摄像头，管理，还没有完成；
   D：（已完成）网站后端：修正了IE8中的显示问题；
12.（已完成）网站后端：每个页面尽量显示记录总条数，显示在表格顶部；<span class="layui-breadcrumb">
13.（已完成）网站前端：【云录播】和【云监控】模式下，都可以默认为时移模式，就是将点播和直播结合起来，按课表，按时间排列，当前时间就是直播；具体页面形式可以参考浪弯的界面，改造我们自己的播放页面；
14.（已完成）定义监控模式下的运行逻辑，在监控模式下新增配置，采集端连接后需要获取：
   A：（已完成）主要是录像逻辑发生变化，切片间隔，录像方式；
   B：（已完成）切片时间10分钟，最大切片时间30分钟，最小1分钟；0表示不切片；
   C：（已完成）需要注意录像切片的衔接问题；要以关键帧为切片起始点，可以配置每个切片的交错方式，交错一个关键帧或2个关键帧，最大不要超过3个关键帧；默认1个；
   D：（已完成）切片交错方式不能用时间（秒）去处理，应该用关键帧更精确；
   E：（已完成）需要在后台配置中，新增：切片间隔时间默认10分钟（最小1分钟，最大30分钟），交错关键帧默认1个关键帧（最小1个，最大3个）；0表示不交错；
   F：（已完成）采集端：写入数据库的录制时间不应该是创建数据记录的时间，而应该是录像写盘时间，需要在录像文件中增加一个创建时间字段，这样时间才会更精确，跟后续的切片才能保持一致；
   G：（已完成）采集端：监控模式和录播模式，都需要新增一个录像文件字段：真实的录像创建时间，用这个时间写入数据库，而不是记录创建的时间；
   H：（已完成）切片时间是指系统流逝时间时间间隔，不是指的是已存盘时间，这一点需要注意，即：创建时间一定是切片间隔的累加；
   I：（已完成）切片时间，切片交错，都可以设置成0，表示不进行切片，不进行交错；
   J：（已完成）采集端：每隔3分钟，自动读取网站端的录像配置，这样就不用每次靠重启获取录像配置了；
15.（已完成）采集端：将摄像头设备与流转发统一起来，摄像头模式登录成功之后，自动启动一个rtsp流转发模式，这样就跟流转发模式融合在一起了，而不是以前那样单独处理；
   A：（已完成）摄像头模式只要登录成功，立即启动主码流的rtsp拉流过程，类型仍然是设备模式；
   B：（已完成）需要将摄像头模式和流转发模式的函数命令进行规范，便于将来查询方便；
   C：（已完成）Stream开头函数（流转发），Device开头函数（摄像头）
16.（已完成）采集端：修改监控摄像头模式下的录像方式：在线程录像时，需要注意切片问题，不要与拉流模式混在一起，单独处理；关键是切片衔接问题；
   A：（已完成）将存盘后的数据帧缓存起来，到达指定数量后，丢弃最老的关键帧数据；新切片产生时，先存储已缓存的数据帧；
   B：（已完成）采集端每隔3分钟会自动从节点网站读取录像切片和切片交错配置；

2017.07.22
=========================================================================
1.（已完成）网站端：将ThinkPHP从2.1升级到了2.2版本，修正了Bug和缓存优化；
   A：对比了3.0、5.0，变化一个比一个大，根据我们的需求，沿用2.2版本就足够了；2.2 => 3.2.3 => 5.0 => 每个都需要重写代码，完全不一样；
   B：本来是想通过升级到3.2.3，对mongodb的支持，结果发现mongodb非常不靠谱，我们的系统将来也完全用不上，果断放弃；将来可以考虑数据库集群解决数据库的问题；
2.（已完成）单音频、单视频的录像过程验证，目前还没有验证；音频有效性没有进行判断。单视频、单音频 的存盘都已经测试通过。
3.（已完成）采集端：输入数据只有视频时的处理，目前的处理没有问题，单独视频是可以进行处理的；
4.（已完成）采集端：单音频、单视频的处理验证，目前都没有问题，可以单独处理；
5.（已完成）校车监控的数据只有视频数据，但是无法回放，可能是H264的格式问题，需要进行录像数据分析验证；
   A：（已完成）直播端：写入3个sps/pps数据帧，HLS切片不能播放，rtmp可以播放；不写如3个数据帧，都不能播放；（发现是rtsp协议获取的SPS、PPS与数据区里面的不一致，造成无法播放）
   B：（已完成）采集端：录制的MP4文件，不能用html5播放；即使写入3个数据帧，也不能播放；（发现是rtsp协议获取的SPS、PPS与数据区里面的不一致，造成无法播放）
   C：（已完成）RTSP协议传递的数据帧都只有一个Nal单元，台湾采集卡的视频关键帧会传递多个Nal；（发现是rtsp协议获取的SPS、PPS与数据区里面的不一致，造成无法播放）
   D：（已完成）对myRTSPClient.cpp进行了改造，DESCRIBE、SETUP、PLAY，这些步骤只是存放信息，在获取到了实际数据帧里的SPS、PPS之后才进行推流线程准备工作，以前将准备过程放在SETUP、PLAY当中了；
6.（已完成）采集端：显示与服务器的连接状态信息；在状态栏显示“存储服务器”和“中转服务器”在线状态信息；
7.（已完成）网站端：点播管理可以删除指定的录像文件，数据库和实体文件一起删除；删除关联图片记录、删除关联视频记录，删除视频存储，删除图片存储；
8.（已完成）采集端：录像任务结束事件的判断有问题，开始时间超过当前时间，也无法结束，还在录像；
    A：（已完成）当前时间小于开始时间时，需要停止正在录像的任务，不管任务是否启动，都需要操作一下；
    B：（已完成）目前采用的是星期模式，日期不起作用，只有时间有意义，在采集端进行时间比较时，需要将时间解析出来，单独计算，不要去改数据库格式，改动太大；
    C：（已完成）测试不同星期的重叠时间段，测试当前时间大于结束时间，测试当前时间小于开始时间；
9.（已完成）采集端：也需要用getsocketopt的IPPROTO_TCP, TCP_INFO，获取连接状态(Linux)；因为，中转端已经发生错误断开，但采集端并没有收到，还以为处于连接状态。
    A：（已完成）Tracker连接，检测是否依然有效；windows当中没有TCP_INFO，使用KeepAlive，5秒无数据，开始发送，
    B：（已完成）Storage连接，检测是否依然有效；windows当中没有TCP_INFO，使用KeepAlive，5秒无数据，开始发送，
    C：（已完成）Remote连接，检测是否依然有效；windows当中没有TCP_INFO，使用KeepAlive，5秒无数据，开始发送，
10.（已完成）网站端：新增“存储访问配置”，就是用http协议访问存储的mp4文件，与fastdfs内部上传、同步机制是不一致的，需要单独配置，这也有利于存储与访问进行分离处理，demo.myhaoyi.com就需要用到；
11.（已完成）网站端：删除点播记录的分页问题；记录减少之后，分页不变的问题；
    A：通常使用laypage的场景都是整个页面刷新，不会遇到这种问题；但是，使用ajax刷新的就会遇到这个问题；
    B：解决办法：每次删除之后，动态计算一下实际的总页数和当前实际页面号，然后再重新加载laypage就可以了；
12.（已完成）网站端：需要通过后端网站可以直接删除指定的录像记录（包括截图和视频一起删除）

2017.07.13
=========================================================================
0.（已完成）myhaoyi：wk_node当中，加入节点服务器更多信息：节点类型、节点名称、节点IP地址；通过以下两种途径汇报信息；
   A：（已完成）节点网站：通过网站端注册汇报到服务器 => 汇报了 节点类型、节点标记、节点名称、节点IP地址 => LoginAction::login() => LoginAction::doWechatAuth()
   B：（已完成）采集端：通过采集端注册汇报到服务器 => 汇报了 节点类型、节点标记、节点名称、节点IP地址 => GatherAction::index() => GatherAction::verify()
1.（已完成）网站后端：wk_sysytem表中设置模式状态字段（0录播模式，1监控模式）
2.（已完成）采集端：每次启动时，获取模式状态标志字段；
3.（已完成）采集端：需要考虑连接中转服务器、连接tracker/storage的自动重连问题；因为，服务器可能会升级、重启，采集端无人值守时需要有断开自动重连功能；每隔5秒自动检测；
4.（已完成）网站端：根据网站类型修改网站相关信息；主要是修改前后端的页脚信息；
5.（已完成）IE8：当服务器时间与本地时间不一致时，IE8的cookie会失效，需要注意服务器与本地时间要保持一致或接近。
6.（已完成）非常强大的图表展示工具 => http://echarts.baidu.com/echarts2/index.html
7.（已完成）非常强大的日期时间工具 => http://www.jemui.com/uidoc/jedate.html
8.（已完成）Range设置成relative解决 => 在IE8下面初始化有问题，只显示一个；top 必须为 0px，而 IE8 下面计算有偏移量，不是0px；
    A：（已完成）设置成relative模式，只是表面解决，在进行拉伸操作时，还是出现问题；只能还原成 absolute 模式；
    B：（已完成）还需要找到 IE8 下面的调整方式；发现浏览器resize一下就能恢复正常；用动态创建模式，而不是静态的方式；因为必须从数据库读取；
    C：（已完成）range对象的编号问题，会造成删除混乱；解决办法：给每个range设置一个唯一编号，而且是全局唯一编号；
    D：（已完成）完全重写了录像任务的配置方式，变得异常强大。还需要写入、读取数据库，传递给采集端；
    E：（已完成）读取通道下面的所有记录，显示在界面当中；
    F：（已完成）删除区间，可以事先对已删除的区间做标记(全局记录)，再点击“全部保存”时，统一通知服务器进行统一操作；
    G：（已完成）复制区间，再全部保存，存在问题，因为，复制操作包含了删除动作，需要先解决删除的标记问题；
    H：（已完成）全部保存，存入数据库之后，需要返回数据库编号course_id，更新到区间的range对象当中，以便后续操作使用；
    I：（已完成）全部保存，新建的区间，需要返回3个信息：sliderID、courseID、rangeID，这样才能找到区间，然后赋值，又不影响已有的range编号机制；
    J：（已完成）注意：js当中的所有对象都是引用方式，对象传递参数时都是传值方式，变量都是传值方式；
    K：（已完成）删除区间，有四个地方：滚轮删除、点击删除、点击全部删除、复制时的删除，都需要调用删除事件，以便记录删除标记，点击“全部保存”时通知数据库删除；
    L：（已完成）全部保存，php转发命令到采集端，进行录像记录的 添加、修改、删除 操作；
    M：（已完成）是一次性将所有的录像记录通过转发服务器发送给采集端，需要注意缓存溢出问题，MAX_LINE改成了1024*64，需要重新编译transmit模块；
    N：（已完成）需要测试‘每周重复’任务录像的有效性，同时，需要更多的验证图表模式的任务录像有没有bug。
    O：（已完成）当新建了20个区间，点击“全部保存”按钮，会发生ajax错误，以前的ajax.error参数有问题，ajax.error(XMLHttpRequest, textStatus, errorThrown)
    P：（已完成）是由于 fastdfs_client.so 发送模块的缓存溢出造成的，需要增大缓冲区 1024*64 => 64KB，以前只有2KB；
9.（已完成）采集端：自动重连存储服务器，的逻辑有点问题，不能立即生效；在 OnSysSet() 调用 DelByEventThread ，这种方式可能存在问题，具体再看实际情况；
10.（已完成）中转服务器：需要有一个检测‘采集端’链接是否有效的处理，因为，采集端可能会中断后没有汇报，造成中转服务器上始终挂有一个链接，影响推流操作。
    A：只有采集端是长链接，其它连接都是curl的短连接；
    B：使用getsockopt的IPPROTO_TCP, TCP_INFO，获取连接状态；TCP_ESTABLISHED，表示正常连接，其它状态返回错误，删除连接；
    C：修改心跳检测时间为10秒钟，以前是30秒有点长；
    D：采集端的存储连接、远程控制连接都会每隔5秒就会自动检测，进行自动重连；
11.（已完成）网站后端：录像模块改进成图表模式，以周为纵轴，以24小时为横轴；录像任务就是节点；自由修改，一目了然；
12.（不着急）采集端：需要购买微软的数字签名，www.wosign.com，避免安装时报告未知的发行商的问题；
    A：取消发布者提醒：Win7 -> 开始 -> 控制面板 -> 用户账户 -> 更改用户账户控制设置 -> 拖到最下面，确定就好了。
    B：E:\GitHub\HaoYiYun\Install\SignTool\Readme.txt，有详细签名步骤，但是无效证书，需要购买；
    C：采集端改用Linux来解决，全部用网页控制，出两个版本，Windows版本和Linux版本，Linux版本用网页配置；
    E：iOS11开始wosign的授权被阻止，很多浏览器也不信任wosign的签名；
    F：去掉windows版本，只留linux版本，通过网页控制是大势所趋；


2017.07.12
=========================================================================
1.（已完成）将阿里云服务器上的网站进行整理，分配各个关联目录，并对每个站点做了不同的错误记录；
   A：（已完成）myhaoyi.com => 一级域名，指向 htdocs 目录，提供公司官网信息 => access_haoyi.log | error_haoyi.log
   B：（已完成）demo.myhaoyi.com => 二级域名，指向 demo 目录，提供云录播的演示网站 => access_demo.log | error_demo.log
   C：（已完成）monitor.myhaoyi.com => 二级域名，指向 monitor 目录，提供云监控的演示网站 => access_monitor.log | error_monitor.log
   D：（已完成）baby.myhaoyi.com => 二级域名，指向 baby 目录，记录所有陪孩子玩耍、旅行、成长记录 => access_baby.log | error_baby.log
   E：（已完成）收集网上陪孩子玩儿的资料，整理、收集、归类，本地路径（E:\GitHub\HaoYiYun\Document\WEB\baby）；现在先放到 happyhope.net.cn，以后放到 baby.myhaoyi.com 上去。

2017.07.05
=========================================================================
1.（已完成）为了配置将来的双https域名演示，将阿里云服务器全面升级，安装了全功能版本软件：
   A：（已完成）srs：汇报机制可以用二级域名demo.myhaoyi.com，绕过https，仍然使用目前的http模式；但是，需要排除内网地址 => 10.29.179.147
   B：（已完成）tracker、storage、php_client，都设置成外网地址 => 118.190.45.238，否则，采集端无法上传；
   C：（已完成）注意开放端口：22122、23000、21001、8080、1935
2.（已完成）采集端：mp4录像，当用文件做为数据源，直播后，再录制成mp4文件，这时H5的播放出现视频有节奏的卡的现象，或许是音频没有计算时间戳的原因，需要进行实验，同时，多源测试。
   A：（已完成）采集端：硅谷第四季的视频，录制之后，html5播放时，一顿一顿，估计是录像时的时间戳的写入方式有问题造成的。用其它播放器播放是正常的。
   B：（已完成）音频计算帧间隔，而不是使用固定的1024，结果 => 跟音频没有关系，即使只clone视频通道，转录的mp4文件也会发生卡顿现象，原始文件则不会；
   C：（已完成）LibMP4-audio.h可以让音频也用计算时间差的方式存盘；PushThread-mp4.cpp可以直接转录mp4文件，只录制视频，仍然卡顿，以后有空再来处理；
   D：（已完成）有可能是视频帧的 PTS 与 DTS 的问题，存在偏差，未修正？这个需要进一步研究；跟 ctts 这个 box 有关，就是composition time，时间差，但不能直接计算，必须单独保存，单独传递；
   E：（已完成）compositionTime(CTTS) = PTS - DTS => compositionTime = (PTS - DTS) / 90.0 
   F：（已完成）MP4V2::MP4ReadSample()有一个容易忽略的参数，RenderingOffset，就是它记录了compositionTime，读取之后，需要传递给 MP4V2::MP4WriteSample()，也有一个 RenderingOffset
   G：（已完成）但是，在直播时，这个 RenderingOffset 或 compositionTime 没有传递给 srs，导致 Flash 播放时有顿挫感（就像帧率不够一样），HLS 播放时一卡一卡的。
   H：（已完成）另外，好像每一个视频帧都记录了自己应该播放的时间，所以外围怎么设置都不起作用，现在，已经明确 RenderingOffset 如何传递给 srs，或对每帧的时间戳进行修正？
   I：（已完成）CPushThread::SendVideoDataPacket()，发包组帧时，专门有 composition time 的选项设置，这里使用真实时间，还是TimeScale时间？(只要与sendTime的格式一致就行，因此，直接给毫秒时间戳)
   J：（已完成）composition time，目前只处理了mp4文件，对于拉流数据，如何处理 composition time，还得继续研究；
   K：（已完成）拉流对象 LibRtmp::doVideo() 当中，有一个抹掉 5 字节外壳的过程，在那 5 字节当中，就有 composition time，需要解析出来，投递到下面的操作当中。1+1+3
   L：（已完成）到目前为止，完全搞定了文件MP4的直播、转发、录像、回放，等等一系列问题，对于多流文件，还可以考虑选择哪路流进行直播；
3.（已完成）采集端：系统设置，当Web地址发生变化时，提示，需要重启才能生效，不重启生效的方式，会造成系统混乱，崩溃。
4.（已完成）采集端：通道配置完毕之后，直接运行，减少点击运行这个步骤；
5.（已完成）公司官网：购买一个网站模版，改造成自己需要的简洁大方模式，增加一些简单好看的动画功能，使用阿里开放的动画模块；参考bootstrap提供的模版，参考豆瓣电影的模版；
   A：（不适合）从 http://expo.bootcss.com/ 寻找一个类似与豆瓣电影的模版（太复杂，没头绪，无从下手）
   B：（已完成）设计网站结构，准备文案，编写网站，填充内容；
   C：（不适合）使用 https://github.com/hiloteam 绘制动画，让网站更生动，主要做游戏的，不适合，完全引入另一个方向。
   D：（已完成）决定选用 fullPage.js 作为核心框架 => https://github.com/alvarotrigo/fullPage.js，中文帮助 http://www.dowebok.com/77.html
   E：（已完成）公司主页面用fullPage.js搭建，总体要给人简单清新的感觉，鼠标滚动翻页，有菜单导航，进行更丰富的信息展示，刚开始没有的话，可以简化。
   F：（已完成）<!DOCTYPE html> 这行代码很重要，否则 IE8 下面页面混乱，前端和后端的 header.html 里面都有这行代码，所有没问题；
   G：（未完成）演示网站分两部分 => 云录播 和 云监控，代码是一样的，设置不同，分为两个域名，两个目录，两个数据库；
   H：（已完成）绘制一张基本架构图，说明系统架构；完全由css+html绘制；
6.（已完成）网站端：需要将myhaoyi改造成https，微信小程序也需要；
   A：（已完成）登录阿里云 => 控制台 => 证书服务 => 购买证书，参考 => https://ninghao.net/blog/4449，在下载栏有详细说明，配置需要添加对php的支持；
   B：（已完成）nginx，需要加入 --with-http_ssl_module 编译参数，让 nginx 能够支持 https，nginx.conf需要针对443端口进行配置；
   C：（已完成）需要打开防火墙，对443端口开放，同时，为了全站使用 https，需要开启301重定向 => return 301 https://$host$request_uri;
   D：（已完成）网站前后端，登录链接需要修改成 https 模式，浏览器内核天生就支持 https ，因此，不受影响，但是，网站最好强制使用https模式；
   E：（已完成）采集端，注册连接myhaoyi.com时，需要修改成 https 模式；编译目录 => E:\GitHub\HaoYiYun\Document\WEB\curl
   F：（已完成）windows下的libcurl支持https太费劲，参考 => http://blog.csdn.net/neverup_/article/details/21961017；
   G：（已完成，未启用）将myhaoyi.com和ihaoyi.cn都同时绑定到阿里云服务器上，可以提供两个域名的https访问，需要申请两个证书；两个域名都需要备案成功才能申请。
   H：（已完成）编译参考 => http://www.jianshu.com/p/d40e249774ff
7.（已完成，未启用）网站端：可以设定微信登录时的界面参数，方便调试和管理，前提是把网站设置成 https 模式；(实验成功，未启用）
8.（已完成）演示端：将所有的部件装到MacBookPro的虚拟机上（CentOS6.8），不必使用多个树莓派搭建演示服务器，Linux版本的采集端延后开发； 
9.（已完成）存储端：尝试把 tracker/storage 装在同一台机器上，关键点在 nginx 需要将 扩展模块 和 缓存模块同时编译，需要打包一个新的全功能 nginx（并在build当中新建了打包模块）
10.（已完成）测试机：将MacBookPro装一个虚拟机，安装CentOS6.8版本，最小安装 Minimal(总共347个模块，支持中文环境)，再安装一个Win7虚拟机，全模拟。
     A：网络选桥接模式。帐号 => root:huijia264，MacBookPro的WiFi有点慢，用网线快很多，也可能跟目前2个WiFi级联有关；
     A：网络选桥接模式。帐号 => MacBook:huijia264，Win7 - 32位
     B：nginx-storage => 编译带ssl功能的版本 => --with-http_ssl_module
     C：nginx-tracker => 编译带ssl功能的版本 => --with-http_ssl_module
     D：nginx-all => 编译带ssl+cache+fastmodule的全功能版本
     E：安装 php-5.6.30  => OK
     F：安装 mysql-5.5.3 => OK
     G：安装 transmit-1.0.1 => OK
     H：安装 srs-2.0.243 => OK
     I：安装 tracker-5.0.9 => OK
     J：安装 storage-5.0.9 => OK
     K：安装 nginx-all-1.10.2 => OK
     L：安装 htdocs => 网站代码 => 修改权限等等
     M：需要修改 IP 地址的地方如下：
       /weike/srs/conf/srs.conf => web_addr 192.168.1.xx; => 重启 srs
       /etc/fdfs/client.conf => tracker_server=192.168.1.xx:22122 => 重启 php
       /etc/fdfs/storage.conf => tracker_server=192.168.1.xx:22122 => 重启 storage
       /etc/fdfs/mod_fastdfs.conf => tracker_server=192.168.1.xx:22122
     N：修改 wk_system 有关 tracker（192.168.1.xx:22122） 与 transmit（192.168.1.xx:21001）的地址和端口；
     O：测试重启，下面需要测试 采集端注册、录像、上传、直播，网站播放点播、播放直播，满足一台机器完成所有工作的要求；
11.（已完成）IE8 观看直播时，会直接造成 srs 崩溃，造成整个通道中断（原因是 IE8 会发送两次播放指令，造成重复创建对象，StreamStartLivePush）
12.（已完成）网站端：通过中转服务器查询时，需要注意：有记录才查询，否则会出现卡死情况；AdminAction::pageCourse()
13.（已完成）采集端：正在录像的任务直接删除时，会造成存放到数据库的记录无法找到录像任务，而无法获取录像对应的老师和科目信息。（设置默认的subject_id和teacher_id都为1，避免前端无法显示的问题）
14.（已完成）网站端：在直播管理中，可以加入状态信息了，因为，直播通道的状态信息已经写入了数据库当中；
15.（已完成）直播端：可以让srs和nginx-rtmp通过on_publish回调进行用户名和密码验证，修改了srs的源码，nginx-rtmp直接支持，详见《浩一监控技术总结.doc》
16.（已完成）将可能用到的工具或代码专门放到 E:\GitHub\HaoYiYun\Tools 目录下，以便后续使用，也方便维护：
    A：Tools\newSlider => 用于任务录像的时间段设置，可视化界面操作，仿海康后台录像任务模式；

2017.06.15
=========================================================================
1.（已完成）网站后端：支持 IE8 访问；
    A：（已完成）需要解决 IE8 总是读取 ajax 缓存的问题，每次给一个无意义的随机数，告诉 IE8 是新的请求，而不是缓存；
    B：（已完成）$.ajaxSetup({ cache:false });每次ajax调用之情，强制不要读取缓存；
    C：（已完成）event.stopPropagation()，IE8不能支持，需要封装 => stopPropagation(event)
    D：（已完成）需要将 jquery 降级为 1.12.4，jquery2.0以上版本都不支持IE678
2.（已完成）网站端：在用户使用微信登录时，需要附带一个节点服务器的唯一识别码，让中心服务器记录用户来源，便于节点管理用户，需要注意用户迁移；
    A：（已完成）将前后端登录处理集中在一起，不要分开处理，便于将来的调试或升级；
    B：（已完成）节点网站端，新增节点标识字符串，用于网站唯一标识。
    C：（已完成）采集端，更改验证授权过程，采集端 => 注册(节点) => 授权(中心)
    D：（已完成）节点网站端，采集端注册时会验证 web_tag 是否存在，不存在，生成一个；
    E：（已完成）微信扫码登陆时会验证 web_tag 是否存在，不存在，生成一个；需要前后端分别测试；
    F：（已完成）节点网站端，用户列表接口，需要传递 web_tag 标记，进行用户筛选；
3.（已完成）网站端：将用户管理、用户赋权，都交给本地网站服务器，中心服务器只做简单记录和采集端授权服务；
    A：（已完成）节点网站，新增wk_user表；
    B：（已完成）中心网站，屏蔽了用户信息获取接口，所有数据都是通过采集端在登录时汇报，通过用户在登录时汇报；
    C：（已完成）节点网站，所有用户数据获取都通过本地数据库，不用通过接口获取，加快了访问速度，增强了用户体验；
4.（已完成）系统软件：http://www.itellyou.cn/，里面有全部未经修改的 Windows 各种版本，office，等等；

2017.06.13
=========================================================================
1.（已完成）采集端：curl连接网站时，加入5秒超时机制，这样在退出时速度快；
2.（已完成）采集端：去掉世纪葵花的默认信息，换上 北京浩一科技 的信息；
3.（已完成）采集端：验证过期、网站注册，显示文字更加人性化；

2017.06.12 => 解决 php 阻塞 slow.log 的问题
=========================================================================
1.（已完成）网站前端：HomeAction.php里面，去掉通过中转服务器获取通道状态的代码，改成通道自己汇报状态，避免频繁刷新造成的堵塞问题；
   A：（已完成）wk_camera中新增字段 status => 0(离线) 1(运行) 2(录像)
   B：（已完成）采集端在注册汇报通道时，重置 status 状态为 0(离线)，命令 => kCmd_PHP_Get_Camera_Status
   C：（已完成）采集端退出时，汇报 logout 事件，将采集端下面所有的通知状态设置为0；
   D：（已完成）通道运行时汇报，通道停止时汇报。
   E：（已完成）网站前端获取通道状态的代码，修改为直接从数据库读取，而不是目前的从采集端读取，避免php堵塞的可能性；
   F：（已完成）网站后端获取通道状态的代码，修改为直接从数据库读取，而不是目前的从采集端读取，避免php堵塞的可能性；
2.（已完成）网站前端：HomeAction.php里面，改进获取直播连接代码，不要等待采集端上传成功才返回，直接返回rtmp播放地址，让播放器自己去处理等待；
   A：（已完成）中转服务器：修改 kCmd_Play_Login 代码，不要延迟发送播放地址，而是通知采集端之后，直接返回 rtmp 地址...
   B：（已完成）采集端：收到 kCmd_Play_Login 命令，直接进行判断处理，无需再次转发命令给中转服务器，减少中转服务器等待时间；
   C：（已完成）网站端：需要事先从数据库中判断一下通道的状态，如果 <=0 就不要连接终站服务器了；
3.（已完成）代码管理：将所有的代码都上传到GitHub上面，需要重新做一个规划，这样不依赖本地的cvs服务器，而且还能随时看到修改变化情况，随时上传；
4.（已完成）代码管理：git 提交的意思是提交到本地代码库，git 推送的意思是上传到 github 服务器。
5.（已完成）代码管理：参考链接 => http://blog.csdn.net/top_code/article/details/50241999

2017.06.10
=========================================================================
1.（已完成）直播端：用 SRS 替换 nginx-rtmp，需要完成如下的工作：
   A：（已完成）新增WebAddr|WebPort配置 => 配置文件比 nginx-rtmp 简单很多倍；
   B：（已完成）在启动|关闭中加入curl汇报机制 => 单线程模式，非常简单 do_cycle()；
   C：（已完成）在某个通道上的用户减少到0时汇报停止上传 => 在专门的统计模块中加入；
   D：（已完成）打包成rpm包 => 就差最后一步，调整播放器之后加入；
   E：（已完成）SRS服务器，无法用 librtmp 获取数据，这样造成拉流无法实现，修改推拉流的代码，使用SRS提供的librtmp；
   F：（已完成）需要使用 SRS 提供的 librtmp 来实现拉流；
   G：（已完成）需要使用 SRS 提供的 librtmp 来实现推流；
   H：（已完成）./configure --export-librtmp-single=./single，输出 librtmp ，需要写程序来验证拉流和推流操作；
2.（已完成）采集端：SRS 当中使用 gop_cache on 模式速度最快，但是会花屏，原因是在上传时第一帧插入的关键帧不是后面数据需要的关键帧；
   A：需要对非 IPC 的数据流进行上传方式调整，上传的数据永远是第一帧是关键帧，不是插入的关键帧。
   B：PushFrame => 累加关键帧，大于3个时，删除第一个关键帧（包括第一个关键帧和第二个关键帧之间的所有帧，包含音频，减少关键帧计数器）
   C：SendOneDataPacket => 没发一个包就另存起来，当发现是视频关键帧时，清空另存的缓冲区，减少关键帧计数器；
   D：BeginSendPacket => 发送数据开始前，得到第一个数据包（关键帧）的发送时间，以便发送时从0开始计时；
   E：EndSendPacket => 发送连接断开之后，需要将缓存数据放入发送队列当中，目的是弥补正在发送的数据没有关键帧的问题，把已经发送的数据找回来。
3.（放弃）播放器：准备使用 JWPlayer 的开源版本，videojs的播放器直播效果不好；
   0：不是 videojs 的效果不好，而是上传端的处理有问题，另外 jwplayer 太复杂，而且功能好多是封闭的；
   A：PC端移动端 Html5 播放 MP4 文件；
   B：PC端使用 Flash 播放直播；
   C：移动端使用 HLS 播放直播；
   D：PC端能够进行切片链的播放，也就是无缝播放多个mp4文件，时间戳累加显示，并能将每一个片段播放完毕的通知传递出来；

2017.06.06
=========================================================================
1.（已完成）网站登录：有安全漏洞，从历史连接中可以任意登录，解决方法如下：
   A、需要在登录连接中加入一个标识符号（时间戳），传递给登录服务器；
   B、登录服务器处理成功之后，原样返回标识符号（时间戳）；
   C、本地服务器会验证这个时间戳，在30秒之内都算正常，超过30秒，连接就无效了；
2.（已完成）世纪葵花：终止了与世纪葵花的合作，冻结目前的版本。最终以6.6日上传的CVS版本为准；
   A、FastDFS里面的版本已6.3日的为准；
   B、htdocs没有编译，到时候需要时再编译；
   C、HaoYiYun.exe在Win7下启动失败；是由于登录用户的权限无法创建文件，只能建目录。

2017.06.04
=========================================================================
1.（已完成）myhaoyi：新增采集端汇报机制，记录采集端信息，控制采集端的运行；
2.（已完成）网站后端：新增用户管理，可以调整用户的类型 => 管理员 | 用户；
3.（已完成）网站前后端：普通用户登录后不能跳转到后端，只有管理员才能登录网站后端。
4.（已完成）网站后端：解决php调用curl反馈慢的问题。（下面的方法似乎有点作用，不能明确到底有没有作用）
   A、curl_setopt($ch, CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_1_0); //强制协议为1.0
   B、curl_setopt($ch, CURLOPT_HTTPHEADER, array("Expect: ")); //头部要送出 Expect: 
   C、curl_setopt($ch, CURLOPT_IPRESOLVE, CURL_IPRESOLVE_V4 ); //强制使用IPV4协议解析域名

2017.06.01
=========================================================================
1.（已完成）网站登录成功后，通过接口展示用户登录信息；区分管理员和普通用户（是由myhaoyi.com验证后传递信息）
   管理员标识：55C8363B-A6A9-41B7-A50D-8033BB62BD30
   普通人标识：DCA3D37F-205A-4F62-9E20-B3E0948CB371
2.（已完成）将用户体系搬迁到 www.myhaoyi.com 上去；
3.（已完成）www.myhaoyi.com的代码整理到GitHub上面，存放目录 => E:\CVSKHStream\HaoYi\GitHub
   GitHub\htdocs => 网站代码
   GitHub\Source => 采集端代码   
4.（已完成）将阿里云的CentOS升级到6.8最新版本，造成php-5.2.14的curl崩溃，升级php-5.6.30解决了问题，重新编译php-5.6.30遇到很多问题，详见readme.txt，并打包成php-5.6.30-1.x86_64.rpm

2017.05.26
=========================================================================
1.（已完成）下面进入用户管理阶段，使用微信扫码登录（专门的第三方网站代码），用户数据存放到第三方网站上，每一个云录播系统都是通过PHP接口来获取用户登录信息，设置cookie等等。
   以往，所有访问数据库的地方，都需要换成接口函数（这个接口函数会调用第三方远端的PHP代码，返回json数据包），网站启动时，必须先检测第三方接口是否有效，然后才能工作。
2.（已完成）首先，需要完成登录和退出的操作，在登陆后台的时候，参考happyhope.net的方法。
3.（注意）chrome浏览器不允许跨域操作iframe的document对象。
4.（网络邻居）本地连接--双击属性--ipv4--高级--netbios，开启后，可以用机器名ping通。
5.（已完成）解决了网站登录的数据交换的模式设计，可以将所有的用户登录请求都集中到haoyi.com当中，现在需要进一步的封装，同时需要注意考虑一些安全问题。
7.（已完成）微信登录页面，能够区分是前端登录还是后端登录；登录成功后，还能自动跳转到登陆前的页面，使用了cookie。
8.（已完成）网站用户管理服务器，微信扫码登录成功后，将用户信息存放或更新到数据库当中。
9.（已完成）网站用户管理服务器，第三方网站可以通过接口访问用户信息，请求所有与用户相关的信息。
10.（已完成）网站用户管理服务器，所有接口函数都返回json数据包，格式如下：
   err_code => true/false => 判断访问是否成功...
   err_msg  => 发生错误时的错误描述信息...
   data     => 具体返回的数据，通常是json数组...
11.（已完成）中转服务器：存在重大问题，transmit_command会造成卡死现象，需要尽快跟踪解决，否则，会造成网站整体效率降低。
   备注1：是由于对epoll的机制不熟悉，发送数据包必须注册发送事件才能执行，而不是在接收事件里面执行发送过程，这种思路是来自windows的处理过程。
   如果不按照这种方式进行，就会出现多个链接发生混乱的情况；先发起EPOLLIN（read），处理Read，缓存发送数据，发起EPOLLOUT（write），处理Write，发起EPOLLIN（read）
   备注2：正在的原因是ET模式事件只通知一次，在accept时将同时到达的链接丢弃了，造成混乱，因此，需要在accept处进行循环读取同时到达的链接。
   备注3：在调试的过程中找到了一种更为简单的通信机制：使用php的socket直接跟transmit通信，这样避免了维护非标准的扩展插件的麻烦。
   备注4：为了性能和效率，还是采用插件模式比较好，php的socket模式，性能和效率比较低。

2017.05.25
=========================================================================
1.（已完成）直播端：直播服务器在退出前，需要汇报中转服务器，自己退出了。（可以实现精确控制，利用nginx的模块机制实现）
   增加 ngx_rtmp_exit_process 进程退出处理函数
2.（已完成）直播端：直播服务器启动之后，需要每隔一段时间就链接一次中转服务器，汇报地址，可以不断延长汇报时间。
   增加 ngx_rtmp_live_timer 时钟处理函数；同时，ngx_modules.c中，需要把ngx_events_module和ngx_event_core_module放到ngx_rtmp_module之前，否则，时钟不起作用。

2017.05.17
=========================================================================
1.（已完成）只有一种方法读取SPS里面的视频宽度和高度 => CSPSReader，另一种BitReader.h的方法有问题，已经删除了。
2.（已完成）数据流的超时判断，不要放在发送线程，而应该放在拉流部分（接收数据部分）
3.（未完成）后期可以在界面层显示出接收数据层多少秒没有数据了，可以显示超时自动断开倒计时。
4.（已完成）断开自动连接的优化和处理，将所有模式的自动重连优化一下。
5.（已完成）网站端：编辑模式下的面包削功能。
6.（已完成）中转端：需要解决中转服务器掉线的问题。（网站后端能查看状态）
7.（已完成）直播端：需要解决直播服务器掉线的问题。（网站后端能查看状态）
8.（已完成）网站端：新增直播管理，使用弹出框修改通道信息；
9.（已完成）网站端：新增点播管理，使用弹出框查看视频信息；
10.（已完成）网站端：将大量的编辑、添加功能，用弹出框重写，增强用户体验；
11.（已完成）网站端：完善列表编辑，选中、编辑过程，新增浅蓝色背景框。
12.（已完成）存储端：需要考虑同一分组下多个storage的情况，多个分组下不同storage的情况。
13.（已完成）存储端：挂接的硬盘有问题，没有把所有的硬盘挂接上去，跟手动安装系统有关，需要设置多个挂接点。
df -h
mkdir -p /home/storage
ln -s /home/storage/data /home/storage/data/M01 => 内部已经建立了M01关联，这里需要手动建立软链接。
vi /etc/fdfs/tracker.conf => 修改tracker配置
   store_path=2           => 每次写盘找剩余空间最大的目录
vi /etc/fdfs/storage.conf
   store_path_count=2
   store_path0=/fdfs/storage
   store_path1=/home/storage
vi /etc/fdfs/mod_fastdfs.conf
   store_path_count=2
   store_path0=/fdfs/storage
   store_path1=/home/storage
   [group1]
   group_name=group1
   storage_server_port=23000
   store_path_count=2
   store_path0=/fdfs/storage
   store_path1=/home/storage
vi /weike/nginx/conf/nginx.conf => tracker下的nginx
   location ~/group1/M[00-01]
vi /weike/nginx/conf/nginx.conf => storage下的nginx
   location ~/(group[1-3]/M01/)(.+)\.(jpg|png|gif)_([0-9]+)x([0-9]+) {
     root /home/storage/data;
     ngx_fastdfs_module;
     ......
   }
   location ~/group[1-3]/M01 {
     root /home/storage/data;
     ngx_fastdfs_module;
   }

2017.05.09
=========================================================================
0.（已完成）建立一个专门的rpm目录
1.（已完成）Linux：tracker-5.0.9-1.x86_64.rpm
2.（已完成）Linux：nginx-tracker-1.10.2-1.x86_64.rpm
3.（已完成）Linux：storage-5.0.9-1.x86_64.rpm
4.（已完成）Linux：nginx-storage-1.10.2-1.x86_64.rpm
5.（已完成）Linux：php-5.2.14-1.x86_64.rpm
6.（已完成）Linux：mysql-5.5.3-1.x86_64.rpm
7.（已完成）Linux：live-1.12.0-1.x86_64.rpm
8.（已完成）将cvs目录进行整理完善。

2017.05.07
=========================================================================
1.（已完成）phpMyAdmin：点击数据库时，会造成 php-cgi 占用CPU100%的问题，是访问session_start时造成的，但是原因不明，需要使用源代码进行调试；（是由于机器过热性能降低导致）
2.（已完成）采集端：新增添加通道功能，可以添加 rtsp/rtmp/mp4 形成新的通道；
3.（已完成）采集端：完善自动连接DVR，显示错误信息，不要完全中断，只中断那些密码错误的通道；
4.（已完成）网站端：完善直播管理、录像管理；
5.（未完成）采集端：需要对录像的tmp文件进行处理，当异常关机时，可以进行数据恢复。

2017.04.28
=========================================================================
1.（已完成）直播端：完成nginx-rtmp的改造，能够反馈信息到采集端。
2.（已完成）PHP扩展：需要专门给直播端编写一个专用接口，专门用来与中转服务器传递数据用，都是单向的：直播端向中转服务器传递信息，不需要中转服务器反馈信息。
3.（已完成）中转器：中转服务器会记录直播端服务器列表IP:PORT，采集端在获取直播端地址之后，一旦发现链接失败，就需要通知中转服务器删除这个无效的直播端。
4.（已完成）直播端：需要新增网站地址和端口的配置，一旦启动需要不断尝试链接网站端口，汇报本机的IP和服务端口给中转服务器。
5.（已完成）直播端：一旦有用户接入或退出直播端，用户数发生变化时，直播端需要把用户数汇报给中转服务器，再由中转服务器根据当初哪些采集端获取过直播地址和端口，再将用户数转发给这些采集端。采集端会根据用户数的情况判断是否需要断开上传。
6.（已完成）直播端：用户点击直播播放页面时，PHP扩展会通过中转服务器通知到对应的采集端，采集端顺便会接收到一个直播端地址和端口。直播播放器会一直处于等待反馈状态。采集端会根据拿到的直播地址和端口，尝试去上传直播，上传直播成功，反馈结果给中转服务器，中转服务器在回应给PHP，PHP将结果反馈给用户，直播播放器停止等待，显示结果。如果采集端上传成功，用户播放器就会链接直播链接，这个直播链接是采集端反馈给用户的。因此，Camera表中，可以不用增加记录直播链接的字段，可以在中转服务器中动态存在。
7.（已完成）直播端：播放器点击直播频道的时候就已经确定了Gather的定位，因此，可以由transmit来进行直播流量的分发。
8.（已完成）下面进入测试阶段，利用PHP代码测试，live-server还需要进行代码修改，主要是利用curl调用php接口，还有就是新增网站配置。
9.（已完成）PHP扩展：改进接口，只留下transmit_command一个数据接口，修改参数，将MAC地址放入saveJson当中。
   A: array  transmit_connect_server(string ip_addr, int port)
   B: bool   transmit_disconnect_server(array & serverInfo)
   C: string transmit_command(int type, int cmd, array & serverInfo, string saveJson)
10.（已完成）直播服务器：发现用户连接上到达0，直接终端直播上传就可以了，无限通过中转服务器汇报。
11.（已完成）直播服务器：只需要不断汇报用户数编号就可以，中转服务器无需中转给采集端知道；
13.（已完成）直播服务器：用户数增加、用户数减少、直播断开都需要汇报给中转服务器，通过curl实现。
14.（已完成）直播播放器：需要让采集端知道有新用户请求直播，中转服务器转发给采集端命令，只要没有上传链接，直接上传。将上传结果回应给中转服务器，中转服务器再回应给播放器。
15.（已完成）ngx_rtmp.h、ngx_rtmp_cmd_module.c、ngx_rtmp_core_module.c
16.（已完成）ngx_rtmp_live_module.c => ngx_rtmp_live_play | ngx_rtmp_live_close_stream

2017.04.26
=========================================================================
1.（已完成）Web端：进一步完善后台，能够进行录像任务的添加、删除、修改操作。
2.（已完成）Web端：后台操作录像任务，自动录像、自动上传、自动显示。

2017.04.23
=========================================================================
1.（已完成）配置：还有一些配置放置在《阿里云 - 浩一.txt》当中。
2.（已完成）PHP扩展：新增3个函数，用于transmit交互过程：
   A: array  transmit_connect_server(string ip_addr, int port)
   B: bool   transmit_disconnect_server(array & serverInfo)
   C: string transmit_set_command(int cmd, string gather_mac, array & serverInfo, string saveJson)
3.（已完成）PHP扩展：编译命令
   A: /weike/php/bin/phpize
   B: ./configure --with-php-config=/weike/php/bin/php-config
   C: rsync -e'ssh -p 1012' -a modules/fastdfs_client.so root@192.168.1.180:/weike/php/ext
4.（已完成）PHP扩展：可以实现PHP网站对录像课程表的 添加、修改、删除 操作。 

2017.04.13
=========================================================================
1.（已完成）Web端：新增wk_system系统表，存放一些配置信息
2.（已完成）Web端：tracker地址，统一在web端设置，采集端启动时需要从web端获取一些配置信息，而不是单独自己设定，设置都是从一个地方统一设置，就是web端。
3.（已完成）PC端：可以根据远端设定的课表内容录像，不要采用切片方式录像。
4.（已完成）Web端：www.myhaoyi.com，已购买阿里云主机，域名指向 118.190.45.238，还需要备案，申请微信开放平台，可以用微信扫描登录。
   用户信息记录在www.myhaoyi.com当中。所有的交互通过php传递json数据完成。将来的微信支付也在这里完成。实现用户和数据分离。
5.（未完成）PC端：新增查看远程上传列表按钮（PHP） - 完善状态栏内容，响应每个按钮的处理事件。
6.（未完成）Web端：由于mp4录像是切片，需要js/flash支持mp4的列表播放，从而支持大量的长时间播放。需要研究videojs的切片播放功能，以及swf的列表播放。
7.（需注意）Web端：目前131本地是windows版本的php/nginx，在编码方式上存在混乱，会造成layui乱码，不要理会，在linux环境下没有问题。
8.（已完成）Web端：如何解决课表运行状态问题：除了PC采集端汇报以外，每次进行php页面刷新时需要反向查询状态，通过ajax反向查询PC采集端。同样的方法适用于Gather页面，Camera页面。
   注意：状态信息不要记录到数据库当中，而是从PC采集端动态获取的，这样更有时效性，而不用处理一些复杂的状态管理问题。
9.（已完成）Web端：下面进入最后的复杂环节：Web命令中转环节，在php中升级fastdfs模块，让它能够链接中转服务器，向PC采集端发送命令。
   注意：命令中转服务器放置在Tracker上，PC端需要先直接连接网站获取录像任务，放在内存当中；网站录像课程有变化时，通过php方向设置到PC端上进行更新。
   注意：命令中转服务器名称：myTransfer，监听端口：21001，安装在Tracker服务器上，先要调试好稳定性，修改那个写狗工具。
   注意：交互的命令格式参考FDFS，协议包由两部分组成：header和body
   header共12字节，格式如下：(为了字节对齐，设置成4的整数倍)
         4 bytes body length => int
         4 byte client type  => int
         4 byte command id   => int
         4 byte php sock id  => int
        20 byte gather mac   => char
   body数据包格式由取决于具体的命令，都是json数据包格式。
10.（已完成）PC端：修改配置，不要设置Tracker地址，从网站获取。
11.（已完成）PC端：所有的通道配置，都通过网站反向设置，这样可以统一起来，不用来回折腾。
    A: 需要先完成中转服务器架构：transmit.c => g++ -g transmit.c -o transmit -ljson => -g 表示带调试 => valgrind-3.12.0 内存泄露检测工具
    B: 内存泄露检测工具使用 => valgrind --tool=memcheck --leak-check=full --show-reachable=yes ./transmit
    C: 完成采集端CRemoteSession的框架搭建，主要用来接收transmit转发PHP发送的指令，并做出相应的操作，相当于后门一样。 
    D: 需要完成fastdfs-php扩展模块的改造，使之能够与transmit进行通信，并直接得到或者设置gather的信息。
12.（已完成）PC端：注册通道时，还需要获取通道名称，如果是新建通道则不需要。采集端通道名称设置为只读模式，通过mac地址进行识别。
13.（已完成）PC端：注册通道时，还需要获取该通道下的所有的录像课程表。（课表修改时，也需要进行反向设置）
14.（已完成）PC端：需要和网站服务器的时钟进行简单同步。

2017.04.12 - 开始后台网站框架的搭建，使用 layui => https://www.layui.com/doc => https://www.layui.com/demo
=========================================================================
1.（已完成）Web端：网站可以配置录像课表，保存并通知对应的采集端，更新存放在内存中的新课表，调整录像。
2.（已完成）Web端：网站后台的搭建，左右分列栏，进行大量的窗口操作。 
3.（已完成）nginx的配置中需要进行修改：
   location /admin {
     rewrite ^/(.*)$ /wxapi.php/Admin;
   }

2017.04.10 - MP4录像存在问题
=========================================================================
1.（已完成）PC端：海康摄像头录制的MP4文件：通过GMPullerX录制
  A：Windows端 => HTML5的video标签 => 不能播放；=> 由于写入了sps/pps等3个很短的数据帧，造成video标签报错。通过MPlayer播放器发现的。
  B：Windows端 => videojs-swf =>可以正常播放，不用全部下载完毕；=> 由于写入了sps/pps等3个很短的数据帧，造成video标签报错。通过MPlayer播放器发现的。
  C：MacOS端   => HTML5的video标签 => 不能播放；=> 由于写入了sps/pps等3个很短的数据帧，造成video标签报错。通过MPlayer播放器发现的。
  D：MacOS端   => videojs-swf => 可以正常播放，但是要全部下载完毕。=> 由于写入了sps/pps等3个很短的数据帧，造成video标签报错。通过MPlayer播放器发现的。
2.（已完成）PC端：台湾卡录制的MP4文件：通过GMPullerX录制
  A：Windows端 => HTML5的video标签 => 可以正常播放
  B：Windows端 => videojs-swf => 可以正常播放，不用全部下载完毕。
  C：MacOS端   => HTML5的video标签 => 可以正常播放，不用全部下载完毕。
  D：MacOS端   => videojs-swf => 可以正常播放，但是要全部下载完毕。
3.（已完成）PC端：需要解决MP4录像回放的问题，定位问题所在，需要找到一个MP4有效分析工具。
  A：由于海康的rtsp通过frame传递过来了3种特殊帧(6,7[sps],8[pps])，都很短的数据帧。
  B：没有丢弃，直接存盘到mp4文件当中。
  C：对mp4用html5的video标签回放时报错，遇到错误的数据帧就报错。
  D：通过MPlayer播放器发现的，刚好在开头有3个连续的报错帧信息。
4.（已完成）Web端：对测试数据进行重新整理，修改gather.php的存盘处理过程。
5.（已完成）Web端：在播放页面，是根据record_id，向前向后找10个记录。
6.（已完成）Web端：在播放页面，播放完毕之后，自动播放下一个节目，这里用到的iframe.parent功能。
7.（已完成）Web端：在播放页面，完善了标题随着播放节目的变化而变化；导航栏针对下拉状态的处理；解决了点播数组合并的问题。
8.（已完成）Web端：在播放页面，记录点击播放次数。

2017.04.07
=========================================================================
2.（已完成）Web端：新增wk_teacher表，记录老师的名字、职称等等信息，便于前端显示。
   0(正高级教师) 1(高级教师) 2(一级教师) 3(二级教师) 4(三级教师)
3.（已完成）Web端：storage服务器上的nginx能够实现自动缩略图功能，需要重新编译配置nginx，--with-http_image_filter_module，格式 => http://xxx.jpg_120x120
4.（已完成）Web端：缩略图延时加载，jquery.lazyload.js，节省一次性访问占用的资源。

2017.03.24
=========================================================================
1.（已完成）Web端：搭建网站框架，呈现已经存放的数据记录。前端用bootstrap，后端用layui，移动端用weui
2.（已完成）Web端：网站分为前端和后端，前端呈现数据，后端控制和设置。
4.（已完成）Web端：网站前端首页的搭建 => 按科目排列 => 语文 | 数学 | 英语 => subject
5.（已完成）Web端：网站前端科目页面的搭建 => 按年级排列 => 小学 | 一年级 | 初中 | 高中 => grade，摄像头camera就是班级的别名。
   这里没有按照年级进行再次分类操作，主要是分页太复杂，后期可以考虑加入iframe的方式，进行分页操作。目前只是分了一种形式，加入了分页操作。
6.（已完成）Web端：网站前端点播播放页面的搭建；
   A:（已完成）播放列表页面，自动切换，动画显示，等等。
   B:（已完成）播放页面嵌入VideoJs，使用iframe嵌入播放列表页面，iframe调用固定的php页面，由php页面加载videojs的实际播放页面。
   C:（已完成）VideoJs-swf部分有问题，需要修改as代码，cuplayer是否专门修改过代码？(RTMPVideoProvider.as::onNetStreamStatus，需要处理NetStream.Video.DimensionChange通知。)
   D:（已完成）首页，右侧，最近更新，点击排行。
7.（已完成）Web端：网站前端直播首页页面的搭建；
8.（已完成）Web端：网站前端直播播放页面的搭建；
9.（已完成）Web端：网站在192.168.1.131，视频和数据库在192.168.1.180，网站和数据分开存放，效率高，也方便调试。需要注意删除 root@% 帐号，修改 config.inc.php 里的配置。

2017.03.17
=========================================================================
1.（已完成）PC端：Tracker地址与网站地址目前是一致的。
2.（已完成）PC端：采集端新增 最大设备数 配置（默认为16个摄像头）
3.（已完成）PC端：采集端启动后，需要先登录网站服务器，汇报采集端信息，然后，注册摄像头，最后，异步启动其它资源；
4.（已完成）PC端：组播线程在发现新摄像头时，需要判断最大设备数支持这个配置参数。
5.（已完成）PC端：修改摄像头名称之后，直接通知网站服务器，更新到数据库。
6.（已完成）数据库：将视频和图片分开存储，这样便于视频和截图关联，图片的缩略图可以用nginx的image_filter模块实现动态的缩略图。
   nginx动态缩略图的实现方法：http://www.w2bc.com/article/80424
7.（已完成）PC端：在生成视频之前，先截图，截图名称与视频名称一致，这样在上传到数据库时，能够进行匹配。
   GetSystemTimeAsFileTime()与md5()，双重处理，保证唯一性。
8.（已完成）PC端：FDFS自动重连还需要检测StorageSession的情况，只有当TrackerSession有效时，才需要检测StorageSession是否有效，有可能发生StorageSession意外中断的情况。
   注意：FDFS当中，无论是Tracker还是Storage，客户端连上之后，在一定时间内（30秒）不发送数据，FDFS就会中断，只要发送过一次数据，就会长久保持连接。
   PC端会每隔5秒钟检测Tracker和Storage连接是否有效，无效就会自动重连。
9.（已完成）PC端：新增Storage链接条件：录像目录下有数据才尝试链接，没有数据不链接，因为连上去了，由于没有数据也会被服务器断开。
10.（已完成）Web端：兼容上传的文件格式不是标准的形式，即：不带CameraID和时长的格式，同时，兼容文件名中有中文的情况。

2017.03.12
=========================================================================
1.（已完成）数据库：新增 School 数据表，记录学校信息，可以有多个PC采集端（Gather）
   school_id => int => 学校编号
   name => vchar => 学校名称
   addr => vchar => 学校地址
   phone => vchar => 学校电话
   image => vchar => 学校图标
   created => 创建时间
   updated => 更新时间

2.（已完成）数据库：新增 Gather 数据表，记录PC采集端信息，隶属于School，是后台手动指定的School；
   gather_id => int => PC采集端编号
   school_id => int => 学校编号（属于哪个学校，可以为0，即可以不设置，后台手动配置）
   mac_addr => vchar => 采集端MAC地址（唯一，不可改变）
   ip_addr => vchar => 采集端IP地址（可以变化，每次采集端启动时更新）
   max_camera => int => 采集端能处理最大设备数量（由采集端软件自己设定，每次采集端启动时更新，会引发carmea的归属变化操作，在camera注册时会进行检测）
   name_pc => vchar => 采集端名称（由PC端软件设定，每次采集端启动时更新，便于匹配哪个学校）
   created => 创建时间
   updated => 更新时间
   
3.（已完成）数据库：新增 Camera 数据表，记录摄像头设备信息（后续会牵涉班级表，课程表，学科表等等）
   camera_id => int => 摄像头设备编号
   gather_id => int => 摄像头所属的采集端编号（camera在启动注册或启动查询时，匹配device_sn，定位本记录，然后会检测gather记录的max_camera，然后决定是否更新gather_id，而不是删除camera记录）
   live_user => int => 摄像头直播用户数（小于或等于0，表示直播没启动，大于0，直播运行中）
   live_rate => int => 摄像头直播码流（Kbps）
   camera_login => int => 摄像头是否登录（0 offline，1 online），是否上传了rtmp地址不一定，需要查询gather对象。
   camera_type => int => 摄像头类型（1海康，2大华）
   camera_name => vchar => 摄像头名称（采集端设定）
   camera_rtmp => vchar => 摄像头上传的rtmp直播地址
   device_sn => vchar => 摄像头设备序列号，唯一标识
   device_ip => vchar => 摄像头设备IP地址
   device_mac => vchar => 摄像头设备MAC地址
   device_type => int => 摄像头设备类型

5.（已完成）Web端：在180(Tracker服务器)上，直接创建数据库和PHP执行目录；
6.（已完成）Web端：数据库密码Kuihua*#816

2017.03.01
=========================================================================
1.（已完成）PC端：加入自动尝试重连DVR功能，可设置开关；
2.（已完成）PC端：加入自动尝试重连FDFS功能，可设置开关；
3.（已完成）PC端：加入配置文件中可设置程序标题功能；
4.（放  弃）PC端：加入配置文件中可设置程序图标功能；
5.（已完成）PC端：加入配置文件中可设置联系信息功能；
6.（已完成）PC端：在每个监控窗口中加入显示当前状态的功能：录像中(动画)
7.（已完成）PC端：新增状态信息栏，实时更新上传码流，当前正在上传的文件，返回的结果，当前时间 等信息；
8.（已完成）PC端：新增左侧摄像头动画显示图标；
9.（已完成）PC端：新增通道配置 - 完善状态栏内容，响应每个按钮的处理事件。
10.（已完成）PC端：新增全局配置 - 完善状态栏内容，响应每个按钮的处理事件。
11.（已完成）PC端：新增关于 - 完善状态栏内容，响应每个按钮的处理事件。
12.（已完成）PC端：新增全屏 - 完善状态栏内容，响应每个按钮的处理事件。
13.（已完成）PC端：新增通道快速登录按钮 - 完善状态栏内容，响应每个按钮的处理事件。
14.（已完成）PC端：新增通道快速退出按钮 - 完善状态栏内容，响应每个按钮的处理事件。
16.（已完成）PC端：目前的录像切片方式，容易造成衔接处的数据丢失，是因为断开rtsp之后，再建立新的rtsp链接。
    需要改成：直接删除LibMP4录像对象，再创建新的LibMP4录像对象的方式。
    MP4录像时，音频是固定时间帧间隔，视频是两帧间的时间间隔。
    需要预先存储关键帧开始的视频数据，直到新的关键帧来到，丢掉前面存放的数据列表，继续存新的，音频帧存储时间戳大于或等于关键帧的数据。

2017.02.25
=========================================================================
1.（已完成）PC端：链接Tracker/Storage，上传.jpg/.mp4文件，网页端能够访问到。需要建立2个长链接，一个链接Tracker，接收指令，一个链接Storage上传数据。
   CTrackerSession => 链接Tracker，获取Storage的配置信息；
   CStorageSession => 链接Storage，发送上传指令，发送上传文件，接收反馈信息；
   CRemoteSession  => 链接命令中转服务器，获取微信或网站发出的远程操作指令。
2.（已完成）PC端：上传过程中，发生意外之后，自动再发起上传操作的处理。
3.（已完成）PC端：无需加入断点续传功能。（没有上传完毕的文件，fdfs-storage会自动(回滚)删除服务器端副本，下次再上传时重新上传。因此，目前的处理方式自动适配续传功能。）
   不需要记录当前正在上传的文件的位置，只需要删除已经上传的文件，即使上传意外中断，storage会自动删除副本，上传端重启后，会继续重新上传，就相当于断点续传一样的效果。
4.（已完成）PC端：（是由于每次发送的数据块太小造成的）发现fdfs在上传时最大码流只能达到4096Kbps，是否是配置问题？还是机器的网卡问题？
   发现每秒发送的频率都是64次/秒，因此，每次发送的数据越多，每秒发送的数据量就会越大，这时读取文件的时间基本可以忽略不计。
   每次读取  8KB字节，发送最高码流为  4Mbps，每秒发送64次 = 4*1024/8/8
   每次读取 64KB字节，发送最高码流为 32Mbps，每秒发送64次 = 32*1024/8/64
   每次读取128KB字节，发送最高码流为 64Mbps，每秒发送64次 = 64*1024/8/128

2017.02.14
=========================================================================
1.（已完成）PC端：开始进行云台操作，除了右侧的按钮操作以外（最好还能支持鼠标直接拖动画面移动云台。）
2.（已完成）PC端：自动验证主码流；用来生成截图和录像文件；自动验证子码流：用来直播观看；（只判断音视频压缩类型，不进行码流自动调整）
3.（已完成）PC端：每个DVR要事先登录网页管理器，配置主码流和子码流的压缩参数，我们的程序只是对音视频的压缩类型进行判断。音频必须是AAC，视频必须是H264。
4.（已完成）PC端：视频常规配置纠正：图像 => 视频调整 => 镜像 => 中心，OSD设置 => 显示名称 | 显示日期 => 名称，日期格式，显示矩形区，显示时间矫正。
5.（已完成）PC端：自动对主码流设置成1024Kbps（默认），子码流设置成512Kbps（默认）。
5.（已完成）PC端：开始进行抓图操作。存放路径问题，图片大小问题，图片上传问题。每隔一定秒数，轮询每个频道进行截图操作。
6.（已完成）PC端：开始进行录像操作。存放路径问题，mp4分片大小问题，视频上传问题。
7.（已完成）PC端：第一次配置时，寻找目前空间最大的盘符，创建一个xx:/GMSave目录，年月日时分秒_通道号.jpg 或 年月日时分秒_通道号.mp4，录像结束之前扩展名为.tmp
8.（已完成）PC端：录像结束之后，开始上传.mp4文件，上传结束之后，删除.mp4；无论mp4还是jpg，上传完毕之后，写入数据库，然后，立即删除。

2017.02.11
=========================================================================
1.（已完成）PC端：对右侧窗口中的按钮布局进行优化和调整，尽量减少具体数字的硬编码操作。
2.（已完成）PC端：进行右侧登录窗口的具体操作和实现；如果已经有了用户名和密码，自动登录，登录失败的处理。
3.（已完成）PC端：一个DVR设备，可能对应着多个通道，这个需要注意。
4.（已完成）PC端：登录状态记录到CCamera对象当中（甚至记录到xml当中，便于下次打开使用？日志文件已经有记录）
5.（已完成）PC端：用异步方式登录到DVR设备。
6.（已完成）PC端：自动连接DVR，自动播放，自动录像，自动截图，自动上传。
8.（放  弃）PC端：获取通道的实时回调预览数据，直接存盘成标准的MP4文件。（这种方式行不通，DVR回调数据是为了自己播放）还是用自己链接RTSP方式录像。

2017.01.25
=========================================================================
1.（已完成）PC端：右侧频道相关配置的界面 => 频道配置、云台操作、通道配置。
2.（已完成）PC端：所有监控核心功能：播放、截图、报警等等，都调用SDK完成，不要自己去实现。

2017.01.24
=========================================================================
1.（已完成）PC端：海康的IPC可以用SDK进行全部的远程配置，只需要知道IP和帐号之后，这样可以进行很多自动化的处理，比如：自动配置主码流/子码流，主码流录像和截图，子码流实时直播观看。
2.（已完成）PC端：海康的IPC提供的RTSP地址的自动探测，Live555有没有这个功能？只能自己用已知的规则去探测。
3.（已完成）PC端：海康的NVR能否提供RTSP的数据流，这样浩一的应用：实现PC/Android/iOS/微信跨平台管理，观看。同时，还能给企业/学校提供自己的私有云系统，快速方便的管理资源。
4.（已完成）PC端：录像下来的MP4文件的本地多路同时回放问题？海康录制的MP4不是标准的，因此，要自己录像。播放使用海康播放SDK，支持标准的MP4文件。
5.（已完成）PC端：在IPC当中可以调用“通道参数配置”这个接口，对IPC的各种通道进行配置，包括音视频码流、压缩方式等等，可以先探测有哪些通道Channel，在配置文件中需要在Camera下面加入新的节点Channel，来进行配置的存放和处理。

2017.01.18
=========================================================================
1.（已完成）PC端：需要建立一个摄像机管理类，专门存放网络摄像机相关操作，目前支持两种类型：海康和大华。
2.（已完成）PC端：统一名称：监控通道，或者可以修改。配置文件当中监控通道（摄像头）使用DeviceSN做为跟节点。
   先从配置文件读取监控通道，再从网络获取实际活跃的监控通道，始终用一个配置文件（Config.xml，UTF8），监控通道Track。
   通道对象管理顺序：CMidView => CVideoWnd => CRenderWnd => CCamera ，所有的配置只存放在一个对象中，不要在多个对象中存放配置，更新复杂。
<?xml version="1.0" encoding="UTF-8" ?>
<Config>
  <Common>
  </Common>
  <Track>
    <Camera>
      <ID>1</ID>
      <Name>监控通道 - 1</Name>
      <DeviceType></DeviceType>
      <DeviceDescription></DeviceDescription>
      <DeviceSN></DeviceSN>
      <CommandPort></CommandPort>
      <HttpPort></HttpPort>
      <MAC></MAC>
      <IPv4Address></IPv4Address>
      <BootTime><BootTime>
      <UserName></UserName>
      <PassWord></PassWord>
      <Channel>
      </Channel>
    </Camera>
  <Track/>
</Config>

2017.01.11
=========================================================================
1.（已完成）PC端：追加线程、追加网络、追加组播（自动搜索）、追加xml配置。
2.（已完成）PC端：海康网络摄像机：录像模块有问题，需要自己链接rtsp并录像，
   需要自己搜索配置摄像机，需要用到的SDK：回放模块、抓图模块、云台控制、报警模块。
3.（已完成）PC端：摄像头设置子码率，设置很低256Kbps~500Kbps之间，用来做rtsp协议转发，
   专门看直播，是受到手机控制，手机发送指令，然后PC端才上传，手机直接用hls观看，hls需要解决快显和延时问题。

2017.01.10
=========================================================================
1.（已完成）新买手机卡
2.（已完成）新建阿里云帐号
3.（未完成）新建公众号
4.（已完成）新建小程序帐号
